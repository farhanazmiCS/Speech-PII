{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejNwrT-YYssJ"
      },
      "source": [
        "# Forced Alignment with Vosk\n",
        "\n",
        "Originally, we evaluated the tagger's F1-score by simply using indices, which may be too penalising. In order to properly evaluate the performance of our PII identification pipeline, we would need to perform *forced alignment*, which aligns token-level transcripts into their corresponding timestamps in the audio files.\n",
        "\n",
        "For this, we shall be using *Vosk*, a toolkit which offers forced-alignment models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change Directory to Root Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "\n",
        "os.chdir('..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/farhan/Desktop/Research'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function for sorting the audio file names by ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_key(file: str) -> int:\n",
        "    try:\n",
        "        # 3 digit\n",
        "        key = int(file[2:5])\n",
        "    except ValueError:\n",
        "        # 1 digit\n",
        "        if file[3] == '.':\n",
        "            key = int(file[2])\n",
        "        else:\n",
        "            key = int(file[2:4])\n",
        "    return key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function to embed the entities within the transcripts (given a dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def insert_entity_tags_to_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Inserts entity boundary tags into the 'text' column based on 'entities',\n",
        "    and adds a new column 'tagged_text' with the result.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Must contain 'text' and 'entities' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Same DataFrame with an additional 'tagged_text' column.\n",
        "    \"\"\"\n",
        "    def insert_tags(row):\n",
        "        text = row[\"text\"]\n",
        "        entities = row[\"entities\"]\n",
        "\n",
        "        # Sort entities in reverse order of start index to avoid offset issues\n",
        "        entities_sorted = sorted(entities, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        for start, end, label in entities_sorted:\n",
        "            tag_start = f\"[{label}_START]\"\n",
        "            tag_end = f\"[{label}_END]\"\n",
        "            text = text[:end] + tag_end + text[end:]\n",
        "            text = text[:start] + tag_start + text[start:]\n",
        "        \n",
        "        return text\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"tagged_text\"] = df.apply(insert_tags, axis=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function to unify whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def unify_whitespace(s):\n",
        "    \"\"\"\n",
        "    - Unify multiple spaces into one space across the text.\n",
        "    - Ensure exactly one space after [XXX_START] and before [XXX_END].\n",
        "    - Ensure one space after [XXX_END] if missing.\n",
        "    - Ensure one space before [XXX_START] if missing.\n",
        "    \"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "\n",
        "    # Step 1: unify all whitespace\n",
        "    s = re.sub(r'\\s+', ' ', s.strip())\n",
        "\n",
        "    # Step 2: ensure space after [XXX_START] and before [XXX_END]\n",
        "    s = re.sub(r'(\\[\\w+_START\\])(\\S)', r'\\1 \\2', s)  # Add space after [START] if missing\n",
        "    s = re.sub(r'(\\S)(\\[\\w+_END\\])', r'\\1 \\2', s)    # Add space before [END] if missing\n",
        "\n",
        "    # Step 3: ensure space after [XXX_END] if missing\n",
        "    s = re.sub(r'(\\[\\w+_END\\])(\\S)', r'\\1 \\2', s)\n",
        "\n",
        "    # Step 4: ensure space before [XXX_START] if missing\n",
        "    s = re.sub(r'(\\S)(\\[\\w+_START\\])', r'\\1 \\2', s)  # Add space before [START] if missing\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function to run Vosk forced-alignment model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from vosk import Model, KaldiRecognizer\n",
        "import soundfile as sf\n",
        "import json\n",
        "\n",
        "def run_vosk(audio_path: str, vosk_model: Model) -> list:\n",
        "    # Load audio\n",
        "    audio_data, sample_rate = sf.read(audio_path)\n",
        "    \n",
        "    # Prepare recognizer\n",
        "    rec = KaldiRecognizer(vosk_model, sample_rate)\n",
        "    rec.SetWords(True)\n",
        "    \n",
        "    if audio_data.ndim > 1:\n",
        "        audio_data = audio_data.mean(axis=1)  # Stereo to mono\n",
        "\n",
        "    pcm_data = (audio_data * 32767).astype(\"int16\").tobytes()\n",
        "\n",
        "    rec.AcceptWaveform(pcm_data)\n",
        "    result = json.loads(rec.FinalResult())\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How `tokenize_reference()` works\n",
        "\n",
        "The `tokenize_reference()` function processes the input text and outputs a list of cleaned tokens for alignment. Here's how it works:\n",
        "\n",
        "1. **Scan the input text using regex (`TOKEN_RE`)**\n",
        "   - Splits the text into meaningful parts: tags, words, digits, and allowed symbols (`@`, `.`, `_`, `-`).\n",
        "   - Example: \n",
        "     ```\n",
        "     BEFORE: Farhan's credit card number is [CREDIT_CARD_START]1234-5678 [CREDIT_CARD_END]\"\n",
        "     AFTER: ['Farhan','s','credit','card','number','is','[CREDIT_CARD_START]','1234','-','5678','[CREDIT_CARD_END]']\n",
        "     ```\n",
        "\n",
        "2. **Go through each part and apply logic step by step:**\n",
        "\n",
        "   - **If it's a tag like `[CREDIT_CARD_START]` or `[PHONE_END]`:**\n",
        "     - Keep the tag as-is.\n",
        "     - If it's a START tag, remember the tag type (e.g., `CREDIT_CARD`) so we know we're inside a sensitive block.\n",
        "     - If it's an END tag, exit the sensitive block.\n",
        "\n",
        "   - **If it's a symbol (`@`, `.`, `_`):**\n",
        "     - Keep it as-is.\n",
        "\n",
        "   - **If it's inside a sensitive block (like credit card or phone), remove all hyphens from it.**\n",
        "     - Example: `\"1234-5678\"` -> `\"12345678\"`\n",
        "\n",
        "   - **If it contains digits:**\n",
        "     - Break it down character by character:\n",
        "       - Digits are converted to words using a map (`'1' -> 'one'`, etc.).\n",
        "       - Letters are converted to lowercase and kept.\n",
        "       - Example: `\"1234\"` -> `\"one two three four\"`\n",
        "\n",
        "   - **Otherwise, it's a normal word:**\n",
        "     - Lowercase it and strip punctuation.\n",
        "\n",
        "3. **Return the final list of processed tokens**\n",
        "\n",
        "This makes the reference string easier to compare to ASR outputs while handling PIIs and formatting consistently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 425,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re, string\n",
        "\n",
        "digit_map = {\n",
        "    '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n",
        "    '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'\n",
        "}\n",
        "\n",
        "# Match tags (case-insensitive, underscore tolerant)\n",
        "TAG_RE   = re.compile(r'\\[[A-Z0-9_]+?_(?:START|END)\\]', re.I)\n",
        "TOKEN_RE = re.compile(r'\\[[^\\]]+]|[\\w]+|[@._-]')\n",
        "\n",
        "# These PII types will have hyphens removed inside the block\n",
        "PII_TAGS_REMOVE_HYPHEN = {'CREDIT_CARD', 'PHONE', 'BANK_ACCOUNT'}\n",
        "\n",
        "def tokenize_reference(text: str):\n",
        "    tokens = []\n",
        "    inside_pii_block = None  # None or tag name (e.g., 'PHONE')\n",
        "\n",
        "    for part in TOKEN_RE.findall(text):\n",
        "        # ---------- 1. Exact tag: check START/END ----------\n",
        "        if TAG_RE.fullmatch(part):\n",
        "            tokens.append(part)\n",
        "            if part.upper().endswith('_START]'):\n",
        "                inside_pii_block = part[1:-1].replace('_START', '').upper()\n",
        "            elif part.upper().endswith('_END]'):\n",
        "                inside_pii_block = None\n",
        "            continue\n",
        "\n",
        "        # ---------- 2. Symbols ----------\n",
        "        if part in {'@', '.', '_'}:\n",
        "            tokens.append(part)\n",
        "            continue\n",
        "\n",
        "        # ---------- 3. Remove hyphens if inside selected PII ----------\n",
        "        if inside_pii_block in PII_TAGS_REMOVE_HYPHEN:\n",
        "            part = part.replace('-', '')\n",
        "\n",
        "        # ---------- 4. Contains digits: explode ----------\n",
        "        if re.search(r'\\d', part):\n",
        "            for ch in part:\n",
        "                if ch.isdigit():\n",
        "                    tokens.append(digit_map[ch])\n",
        "                elif ch.isalpha():\n",
        "                    tokens.append(ch.lower())\n",
        "            continue\n",
        "\n",
        "        # ---------- 5. Normal word ----------\n",
        "        clean = part.lower().strip(string.punctuation)\n",
        "        if clean:\n",
        "            tokens.append(clean)\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How `align_transcript_with_vosk()` works\n",
        "\n",
        "This function aligns a reference transcript (with tagged PII) to the word-level timestamps produced by Vosk ASR output. The final output is a list of words (including tags), each annotated with their `start` and `end` timestamps if they align to Vosk output.\n",
        "\n",
        "---\n",
        "\n",
        "### Step-by-step Breakdown\n",
        "\n",
        "1. **Tokenize the transcript**\n",
        "   - Uses the `tokenize_reference()` function to break the transcript into a list of tokens that include words and tags like `[CREDIT_CARD_START]`.\n",
        "   - Tags are preserved for reinsertion later.\n",
        "\n",
        "2. **Store tag positions**\n",
        "   - Extract the positions and values of all tags from the reference tokens.\n",
        "   - This allows tags to be reinserted later in the correct order after alignment.\n",
        "\n",
        "3. **Clean reference tokens**\n",
        "   - Remove tags from the reference.\n",
        "   - Normalize the remaining tokens by lowercasing and stripping punctuation (via `clean_token()`).\n",
        "\n",
        "4. **Clean Vosk tokens**\n",
        "   - Extract the `word` field from each Vosk word-level dict.\n",
        "   - Normalize them using the same `clean_token()` function for fair comparison.\n",
        "\n",
        "5. **Align using `SequenceMatcher`**\n",
        "   - Uses `difflib.SequenceMatcher` to find matching subsequences between cleaned reference tokens and Vosk tokens.\n",
        "   - This returns a sequence of **opcodes** describing alignment actions:\n",
        "     - `equal`: tokens match exactly\n",
        "     - `replace`: mismatch; estimate start/end for only outer tokens\n",
        "     - `delete`: token appears in reference but not in Vosk; mark with null timestamps\n",
        "     - `insert`: token appears in vosk output but not in reference; ignore\n",
        "\n",
        "6. **Build aligned list**\n",
        "   - For `equal`:\n",
        "     - Assign exact start and end timestamps from Vosk to the reference token.\n",
        "   - For `replace`:\n",
        "     - Only first and last tokens are assigned estimated timestamps based on available Vosk positions.\n",
        "     - Middle tokens (if any) have `None` timestamps.\n",
        "   - For `delete`:\n",
        "     - All tokens are kept with `start = None` and `end = None`.\n",
        "   - For `insert`:\n",
        "     - All tokens that are present in vosk but not present in the reference (Ignore)\n",
        "\n",
        "7. **Reinsert tags**\n",
        "   - Tags previously removed are reinserted into the `aligned` list at their original positions.\n",
        "   - Tags always have `start = None`, `end = None`.\n",
        "\n",
        "---\n",
        "\n",
        "### Output\n",
        "\n",
        "A list of dictionaries like:\n",
        "\n",
        "```python\n",
        "[\n",
        "  {'word': 'credit', 'start': 1.23, 'end': 1.56},\n",
        "  {'word': '[CREDIT_CARD_START]', 'start': None, 'end': None},\n",
        "  {'word': 'four', 'start': 1.57, 'end': 1.73},\n",
        "  ...\n",
        "]\n",
        "```\n",
        "\n",
        "This structure ensures tags are preserved and aligned words have timestamps if matched to the Vosk output.\n",
        "\n",
        "---\n",
        "\n",
        "### Helper Functions Used\n",
        "\n",
        "- `clean_token(token)`\n",
        "  - Lowercases and strips punctuation from a token unless it's a tag (enclosed in `[]`).\n",
        "- `is_tag(token)`\n",
        "  - Returns True if token is a tag (starts and ends with `[]`).\n",
        "- `is_start_tag(token)` / `is_end_tag(token)`\n",
        "  - Detect whether a tag is a START or END marker.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "import string\n",
        "import re\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Helpers\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def clean_token(token):\n",
        "    \"\"\"Lowercase and strip punctuation from token unless it's a tag.\"\"\"\n",
        "    if token.startswith(\"[\") and token.endswith(\"]\"):\n",
        "        return token\n",
        "    return token.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def is_tag(token):\n",
        "    return token.startswith(\"[\") and token.endswith(\"]\")\n",
        "\n",
        "def is_start_tag(token):\n",
        "    return token.endswith(\"_START]\")\n",
        "\n",
        "def is_end_tag(token):\n",
        "    return token.endswith(\"_END]\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Main Alignment Function\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def align_transcript_with_vosk(vosk_words, transcript):\n",
        "    # Tokenize reference into tokens including tags\n",
        "    ref_tokens = tokenize_reference(transcript)\n",
        "\n",
        "    # Save positions of tags\n",
        "    tag_positions = [(i, t) for i, t in enumerate(ref_tokens) if is_tag(t)]\n",
        "\n",
        "    # Remove tags from reference tokens\n",
        "    ref_tokens_clean = [t for t in ref_tokens if not is_tag(t)]\n",
        "    ref_tokens_clean_norm = [clean_token(t) for t in ref_tokens_clean]\n",
        "\n",
        "    vosk_tokens = [w['word'] for w in vosk_words]\n",
        "    vosk_tokens_clean = [clean_token(t) for t in vosk_tokens]\n",
        "\n",
        "    # Align with SequenceMatcher\n",
        "    matcher = SequenceMatcher(None, ref_tokens_clean_norm, vosk_tokens_clean, autojunk=False)\n",
        "\n",
        "    aligned = []\n",
        "    i_clean = 0  # index in cleaned ref tokens\n",
        "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
        "        for i_r, i_v in zip(range(i1, i2), range(j1, j2)) if tag == \"equal\" else []:\n",
        "            aligned.append({\n",
        "                'word': ref_tokens_clean[i_r],\n",
        "                'start': vosk_words[i_v]['start'],\n",
        "                'end': vosk_words[i_v]['end']\n",
        "            })\n",
        "        if tag == \"replace\":\n",
        "            for i_r in range(i1, i2):\n",
        "                start = vosk_words[j1]['start'] if j1 < len(vosk_words) else None\n",
        "                end = vosk_words[j2 - 1]['end'] if (j2 - 1) < len(vosk_words) else None\n",
        "                aligned.append({\n",
        "                    'word': ref_tokens_clean[i_r],\n",
        "                    'start': start if i_r == i1 else None,\n",
        "                    'end': end if i_r == i2 - 1 else None\n",
        "                })\n",
        "        if tag == \"delete\":\n",
        "            for i_r in range(i1, i2):\n",
        "                aligned.append({\n",
        "                    'word': ref_tokens_clean[i_r],\n",
        "                    'start': None,\n",
        "                    'end': None\n",
        "                })\n",
        "\n",
        "    # Reinsert tags with null times\n",
        "    for pos, tag in tag_positions:\n",
        "        aligned.insert(pos, {'word': tag, 'start': None, 'end': None})\n",
        "\n",
        "    return aligned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How `extract_pii_tuples()` works\n",
        "\n",
        "This function extracts time-stamped PII spans from an aligned transcript and stores them in a new column of a DataFrame. It detects pairs of `[TAG_START]` and `[TAG_END]` tokens, and assigns each a start and end time using fallback logic.\n",
        "\n",
        "---\n",
        "\n",
        "### Step-by-step Breakdown\n",
        "\n",
        "#### 1. **Regex Setup**\n",
        "- `_START_RE` and `_END_RE` are case-insensitive regex patterns to detect tags like `[CREDIT_CARD_START]` and `[PHONE_END]`, even if written with extra underscores or inconsistent casing.\n",
        "\n",
        "#### 2. **Helper Functions**\n",
        "\n",
        "- `_to_tokens(cell)`  \n",
        "  Converts the cell (either a list or a JSON-stringified list) into `list[dict]` form.\n",
        "  - Tries `json.loads()` first.\n",
        "  - Falls back to `ast.literal_eval()` if needed.\n",
        "\n",
        "- `_fwd_time(tokens, idx, field)`  \n",
        "  Finds the **first non-null** value of `field` (`start` or `end`) **after** the current index.\n",
        "\n",
        "- `_back_time(tokens, idx, field)`  \n",
        "  Finds the **last non-null** value of `field` **before** the current index.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Main Logic: `extract_pii_tuples()`**\n",
        "\n",
        "#### Parameters:\n",
        "- `df`: The input DataFrame.\n",
        "- `align_col`: Column name containing the aligned transcript (default: `\"aligned_transcript\"`).\n",
        "- `out_col`: Column to store the output tuples (default: `\"pii_tuples\"`).\n",
        "\n",
        "#### Output:\n",
        "- Populates each row of `df[out_col]` with a list of `(start_time, end_time, TAG)` tuples representing detected PII spans.\n",
        "\n",
        "#### Process:\n",
        "For each row in the DataFrame:\n",
        "\n",
        "1. **Parse the aligned tokens**\n",
        "   - Use `_to_tokens()` to ensure the token list is in `list[dict]` format.\n",
        "\n",
        "2. **Iterate over tokens**\n",
        "   - Maintain two variables:\n",
        "     - `open_tag`: The current open tag type (e.g., `\"CREDIT_CARD\"`).\n",
        "     - `open_start`: The estimated start time of the tag.\n",
        "\n",
        "3. **If `[TAG_START]` is found:**\n",
        "   - Extract the tag name from the token.\n",
        "   - Attempt to determine the start time using the following fallbacks:\n",
        "     1. The **next** token’s `start`. (As the tag token will always be `None`)\n",
        "     2. The **previous** token’s `start`.\n",
        "\n",
        "4. **If `[TAG_END]` is found and matches the last open tag:**\n",
        "   - Attempt to determine the end time using the following fallbacks:\n",
        "     1. The **previous** token’s `end`. (As the tag token will always be `None`)\n",
        "     2. The **next** token’s `end`.\n",
        "\n",
        "5. **Append a tuple**\n",
        "   - If both `[TAG_START]` and `[TAG_END]` are successfully paired, append:\n",
        "     ```python\n",
        "     (open_start, end_time, open_tag)\n",
        "     ```\n",
        "   - Reset `open_tag` and `open_start`.\n",
        "\n",
        "6. **Store result**\n",
        "   - After processing the row, append the list of extracted tuples to `out_rows`.\n",
        "\n",
        "---\n",
        "\n",
        "### Example Output\n",
        "\n",
        "After processing, each row in `df['pii_tuples']` will contain something like:\n",
        "\n",
        "```python\n",
        "[(1.23, 2.48, 'CREDIT_CARD'), (5.12, 6.01, 'PHONE')]\n",
        "```\n",
        "\n",
        "This means:\n",
        "- A credit card PII segment starts at `1.23s` and ends at `2.48s`\n",
        "- A phone PII segment starts at `5.12s` and ends at `6.01s`\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Fallback Order\n",
        "\n",
        "**For `[TAG_START]` timestamps:**\n",
        "1. Use the **next** token’s `start`\n",
        "2. Use the **previous** token’s `start`\n",
        "\n",
        "**For `[TAG_END]` timestamps:**\n",
        "1. Use the **previous** token’s `end`\n",
        "2. Use the **next** token’s `end`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, ast, re, pandas as pd\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# regexes: allow extra underscores, ignore case, tolerate punctuation\n",
        "_START_RE = re.compile(r'\\[([A-Z_]+?)_START\\]', re.I)\n",
        "_END_RE   = re.compile(r'\\[([A-Z_]+?)_END\\]'  , re.I)\n",
        "\n",
        "# ---------- helper: load cell → list[dict] ---------------------\n",
        "def _to_tokens(cell):\n",
        "    if isinstance(cell, list):\n",
        "        return cell\n",
        "    if isinstance(cell, str):\n",
        "        try:\n",
        "            return json.loads(cell)          # JSON\n",
        "        except json.JSONDecodeError:\n",
        "            return ast.literal_eval(cell)    # python literal\n",
        "    raise ValueError(\"aligned_transcript must be list or JSON-string\")\n",
        "\n",
        "# ---------- helper: first non-null AFTER idx -------------------\n",
        "def _fwd_time(tokens, idx, field):\n",
        "    for t in tokens[idx + 1:]:\n",
        "        if t[field] is not None:\n",
        "            return t[field]\n",
        "    return None\n",
        "\n",
        "# ---------- helper: last non-null BEFORE idx -------------------\n",
        "def _back_time(tokens, idx, field):\n",
        "    for t in reversed(tokens[:idx]):\n",
        "        if t[field] is not None:\n",
        "            return t[field]\n",
        "    return None\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# main extractor\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def extract_pii_tuples(df,\n",
        "                       align_col=\"aligned_transcript\",\n",
        "                       out_col=\"pii_tuples\"):\n",
        "    \"\"\"\n",
        "    Populate df[out_col] with [(start_time, end_time, TAG), …]\n",
        "\n",
        "    Fallback order\n",
        "    --------------\n",
        "    • [TAG_START] start-time\n",
        "        1. next token .start\n",
        "        2. previous token .start\n",
        "\n",
        "    • [TAG_END] end-time\n",
        "        1. previous token .end\n",
        "        2. next token .end\n",
        "    \"\"\"\n",
        "    out_rows = []\n",
        "\n",
        "    for cell in df[align_col]:\n",
        "        tokens      = _to_tokens(cell)\n",
        "        tuples_row  = []\n",
        "        open_tag    = None\n",
        "        open_start  = None\n",
        "\n",
        "        for idx, tok in enumerate(tokens):\n",
        "            word = tok[\"word\"]\n",
        "\n",
        "            # ---------- opening tag ----------\n",
        "            m_open = _START_RE.search(word)\n",
        "            if m_open:\n",
        "                open_tag   = m_open.group(1)\n",
        "\n",
        "                # 1. next token .start\n",
        "                if open_start is None:\n",
        "                    open_start = _fwd_time(tokens, idx, \"start\")\n",
        "                # 2. previous token .start\n",
        "                if open_start is None and idx > 0:\n",
        "                    open_start = tokens[idx - 1][\"start\"]\n",
        "                continue\n",
        "\n",
        "            # ---------- closing tag ----------\n",
        "            m_close = _END_RE.search(word)\n",
        "            if m_close and open_tag == m_close.group(1):\n",
        "                # 1. previous token .end\n",
        "                if end_t is None and idx > 0:\n",
        "                    end_t = tokens[idx - 1][\"end\"]\n",
        "                # 2. next token .end\n",
        "                if end_t is None:\n",
        "                    end_t = _fwd_time(tokens, idx, \"end\")\n",
        "\n",
        "                tuples_row.append((open_start, end_t, open_tag))\n",
        "                open_tag, open_start = None, None\n",
        "                continue\n",
        "\n",
        "        out_rows.append(tuples_row)\n",
        "\n",
        "    df[out_col] = out_rows\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract id from file name function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_id_number(filename):\n",
        "    match = re.search(r'id(\\d+)\\.wav', filename)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir('..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load batch 1 (150 samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 820,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The day before [DATE_START] yesterday, [DATE_E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>um my date of birth is uh second [DATE_START] ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>she handed over a crumpled piece of paper, the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aglio olio and err uh [CARDINAL_START] three t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[PERSON_START] Hong's [PERSON_END] email is [E...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  The day before [DATE_START] yesterday, [DATE_E...\n",
              "1  um my date of birth is uh second [DATE_START] ...\n",
              "2  she handed over a crumpled piece of paper, the...\n",
              "3  aglio olio and err uh [CARDINAL_START] three t...\n",
              "4  [PERSON_START] Hong's [PERSON_END] email is [E..."
            ]
          },
          "execution_count": 820,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "batch_one_ref = pd.read_json('data/true_data_150.jsonl', lines=True)\n",
        "batch_one_ref.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 821,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/Audio_Files_for_testing/id1.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/Audio_Files_for_testing/id2.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/Audio_Files_for_testing/id3.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/Audio_Files_for_testing/id4.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/Audio_Files_for_testing/id5.wav</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              file_name\n",
              "0  data/Audio_Files_for_testing/id1.wav\n",
              "1  data/Audio_Files_for_testing/id2.wav\n",
              "2  data/Audio_Files_for_testing/id3.wav\n",
              "3  data/Audio_Files_for_testing/id4.wav\n",
              "4  data/Audio_Files_for_testing/id5.wav"
            ]
          },
          "execution_count": 821,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "batch_one_files = sorted(os.listdir(\"data/Audio_Files_for_testing\"), key=retrieve_key)\n",
        "batch_one_files  = [f'data/Audio_Files_for_testing/{file}' for file in batch_one_files]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "batch_one_df = pd.DataFrame(data=batch_one_files, columns=['file_name'])\n",
        "batch_one_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load batch 2 (350 samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 822,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>entities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151</td>\n",
              "      <td>456 729103 8 is Kaifu Lee's DBS bank account, ...</td>\n",
              "      <td>[[0, 12, BANK_ACCOUNT]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>152</td>\n",
              "      <td>Jacob's OCBC bank account is 192-58462-3, and ...</td>\n",
              "      <td>[[29, 40, BANK_ACCOUNT]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>153</td>\n",
              "      <td>788 305194 2 is Zheng Qi's POSB bank account, ...</td>\n",
              "      <td>[[0, 12, BANK_ACCOUNT]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>154</td>\n",
              "      <td>Geetha's UOB bank account is 341-92741-9, and ...</td>\n",
              "      <td>[[29, 40, BANK_ACCOUNT]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>155</td>\n",
              "      <td>623 481057 6 is Ah Seng's Maybank account, and...</td>\n",
              "      <td>[[0, 12, BANK_ACCOUNT]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id                                               text  \\\n",
              "0  151  456 729103 8 is Kaifu Lee's DBS bank account, ...   \n",
              "1  152  Jacob's OCBC bank account is 192-58462-3, and ...   \n",
              "2  153  788 305194 2 is Zheng Qi's POSB bank account, ...   \n",
              "3  154  Geetha's UOB bank account is 341-92741-9, and ...   \n",
              "4  155  623 481057 6 is Ah Seng's Maybank account, and...   \n",
              "\n",
              "                   entities  \n",
              "0   [[0, 12, BANK_ACCOUNT]]  \n",
              "1  [[29, 40, BANK_ACCOUNT]]  \n",
              "2   [[0, 12, BANK_ACCOUNT]]  \n",
              "3  [[29, 40, BANK_ACCOUNT]]  \n",
              "4   [[0, 12, BANK_ACCOUNT]]  "
            ]
          },
          "execution_count": 822,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "batch_two_ref = pd.read_json('data/newtest_151_500_updated_TTS.jsonl', lines=True)\n",
        "batch_two_ref.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 823,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id151.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id152.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id153.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id154.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id155.wav</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    file_name\n",
              "0  data/newtest_151_500_updated_TTS/id151.wav\n",
              "1  data/newtest_151_500_updated_TTS/id152.wav\n",
              "2  data/newtest_151_500_updated_TTS/id153.wav\n",
              "3  data/newtest_151_500_updated_TTS/id154.wav\n",
              "4  data/newtest_151_500_updated_TTS/id155.wav"
            ]
          },
          "execution_count": 823,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "batch_two_files = sorted(os.listdir(\"data/newtest_151_500_updated_TTS\"), key=retrieve_key)\n",
        "batch_two_files  = [f'data/newtest_151_500_updated_TTS/{file}' for file in batch_two_files]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "batch_two_df = pd.DataFrame(data=batch_two_files, columns=['file_name'])\n",
        "batch_two_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, for batch 2, the entities enclosed are not in the reference transcripts. As we are given the indices, we can write a helper function to include them within the transcripts. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocess transcripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 824,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_two_ref = insert_entity_tags_to_df(batch_two_ref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 825,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>entities</th>\n",
              "      <th>tagged_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151</td>\n",
              "      <td>456 729103 8 is Kaifu Lee's DBS bank account, ...</td>\n",
              "      <td>[[0, 12, BANK_ACCOUNT]]</td>\n",
              "      <td>[BANK_ACCOUNT_START]456 729103 8[BANK_ACCOUNT_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>152</td>\n",
              "      <td>Jacob's OCBC bank account is 192-58462-3, and ...</td>\n",
              "      <td>[[29, 40, BANK_ACCOUNT]]</td>\n",
              "      <td>Jacob's OCBC bank account is [BANK_ACCOUNT_STA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>153</td>\n",
              "      <td>788 305194 2 is Zheng Qi's POSB bank account, ...</td>\n",
              "      <td>[[0, 12, BANK_ACCOUNT]]</td>\n",
              "      <td>[BANK_ACCOUNT_START]788 305194 2[BANK_ACCOUNT_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>154</td>\n",
              "      <td>Geetha's UOB bank account is 341-92741-9, and ...</td>\n",
              "      <td>[[29, 40, BANK_ACCOUNT]]</td>\n",
              "      <td>Geetha's UOB bank account is [BANK_ACCOUNT_STA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>155</td>\n",
              "      <td>623 481057 6 is Ah Seng's Maybank account, and...</td>\n",
              "      <td>[[0, 12, BANK_ACCOUNT]]</td>\n",
              "      <td>[BANK_ACCOUNT_START]623 481057 6[BANK_ACCOUNT_...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id                                               text  \\\n",
              "0  151  456 729103 8 is Kaifu Lee's DBS bank account, ...   \n",
              "1  152  Jacob's OCBC bank account is 192-58462-3, and ...   \n",
              "2  153  788 305194 2 is Zheng Qi's POSB bank account, ...   \n",
              "3  154  Geetha's UOB bank account is 341-92741-9, and ...   \n",
              "4  155  623 481057 6 is Ah Seng's Maybank account, and...   \n",
              "\n",
              "                   entities                                        tagged_text  \n",
              "0   [[0, 12, BANK_ACCOUNT]]  [BANK_ACCOUNT_START]456 729103 8[BANK_ACCOUNT_...  \n",
              "1  [[29, 40, BANK_ACCOUNT]]  Jacob's OCBC bank account is [BANK_ACCOUNT_STA...  \n",
              "2   [[0, 12, BANK_ACCOUNT]]  [BANK_ACCOUNT_START]788 305194 2[BANK_ACCOUNT_...  \n",
              "3  [[29, 40, BANK_ACCOUNT]]  Geetha's UOB bank account is [BANK_ACCOUNT_STA...  \n",
              "4   [[0, 12, BANK_ACCOUNT]]  [BANK_ACCOUNT_START]623 481057 6[BANK_ACCOUNT_...  "
            ]
          },
          "execution_count": 825,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_two_ref.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 826,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>entities</th>\n",
              "      <th>tagged_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>496</td>\n",
              "      <td>Patrick Loh boasting about his email patrick.l...</td>\n",
              "      <td>[[37, 60, EMAIL]]</td>\n",
              "      <td>Patrick Loh boasting about his email [EMAIL_ST...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>497</td>\n",
              "      <td>Jasmine Yeo got sian when someone spell her em...</td>\n",
              "      <td>[[50, 69, EMAIL]]</td>\n",
              "      <td>Jasmine Yeo got sian when someone spell her em...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>498</td>\n",
              "      <td>Bobby Tan write his email bobby.tan@gmail.com ...</td>\n",
              "      <td>[[26, 45, EMAIL]]</td>\n",
              "      <td>Bobby Tan write his email [EMAIL_START]bobby.t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>499</td>\n",
              "      <td>Kamala Singh telling the IT guy her email kama...</td>\n",
              "      <td>[[42, 60, EMAIL]]</td>\n",
              "      <td>Kamala Singh telling the IT guy her email [EMA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>500</td>\n",
              "      <td>Raymond Koh say his email raymond.k@singnet.co...</td>\n",
              "      <td>[[26, 50, EMAIL]]</td>\n",
              "      <td>Raymond Koh say his email [EMAIL_START]raymond...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                               text  \\\n",
              "346  496  Patrick Loh boasting about his email patrick.l...   \n",
              "347  497  Jasmine Yeo got sian when someone spell her em...   \n",
              "348  498  Bobby Tan write his email bobby.tan@gmail.com ...   \n",
              "349  499  Kamala Singh telling the IT guy her email kama...   \n",
              "350  500  Raymond Koh say his email raymond.k@singnet.co...   \n",
              "\n",
              "              entities                                        tagged_text  \n",
              "346  [[37, 60, EMAIL]]  Patrick Loh boasting about his email [EMAIL_ST...  \n",
              "347  [[50, 69, EMAIL]]  Jasmine Yeo got sian when someone spell her em...  \n",
              "348  [[26, 45, EMAIL]]  Bobby Tan write his email [EMAIL_START]bobby.t...  \n",
              "349  [[42, 60, EMAIL]]  Kamala Singh telling the IT guy her email [EMA...  \n",
              "350  [[26, 50, EMAIL]]  Raymond Koh say his email [EMAIL_START]raymond...  "
            ]
          },
          "execution_count": 826,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_two_ref.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 827,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[BANK_ACCOUNT_START]456 729103 8[BANK_ACCOUNT_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jacob's OCBC bank account is [BANK_ACCOUNT_STA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[BANK_ACCOUNT_START]788 305194 2[BANK_ACCOUNT_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Geetha's UOB bank account is [BANK_ACCOUNT_STA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[BANK_ACCOUNT_START]623 481057 6[BANK_ACCOUNT_...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  [BANK_ACCOUNT_START]456 729103 8[BANK_ACCOUNT_...\n",
              "1  Jacob's OCBC bank account is [BANK_ACCOUNT_STA...\n",
              "2  [BANK_ACCOUNT_START]788 305194 2[BANK_ACCOUNT_...\n",
              "3  Geetha's UOB bank account is [BANK_ACCOUNT_STA...\n",
              "4  [BANK_ACCOUNT_START]623 481057 6[BANK_ACCOUNT_..."
            ]
          },
          "execution_count": 827,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_two_ref = batch_two_ref.drop(columns=['entities', 'text', 'id'], axis=1)\n",
        "batch_two_ref.rename(columns={'tagged_text': 'text'}, inplace=True)\n",
        "batch_two_ref.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Combine the datasets [Run this when dataset not combined yet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 828,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_ref = pd.concat([batch_one_ref, batch_two_ref], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 829,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The day before [DATE_START] yesterday, [DATE_E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>um my date of birth is uh second [DATE_START] ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>she handed over a crumpled piece of paper, the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aglio olio and err uh [CARDINAL_START] three t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[PERSON_START] Hong's [PERSON_END] email is [E...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  The day before [DATE_START] yesterday, [DATE_E...\n",
              "1  um my date of birth is uh second [DATE_START] ...\n",
              "2  she handed over a crumpled piece of paper, the...\n",
              "3  aglio olio and err uh [CARDINAL_START] three t...\n",
              "4  [PERSON_START] Hong's [PERSON_END] email is [E..."
            ]
          },
          "execution_count": 829,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 830,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>Patrick Loh boasting about his email [EMAIL_ST...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>Jasmine Yeo got sian when someone spell her em...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>Bobby Tan write his email [EMAIL_START]bobby.t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>Kamala Singh telling the IT guy her email [EMA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>Raymond Koh say his email [EMAIL_START]raymond...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text\n",
              "496  Patrick Loh boasting about his email [EMAIL_ST...\n",
              "497  Jasmine Yeo got sian when someone spell her em...\n",
              "498  Bobby Tan write his email [EMAIL_START]bobby.t...\n",
              "499  Kamala Singh telling the IT guy her email [EMA...\n",
              "500  Raymond Koh say his email [EMAIL_START]raymond..."
            ]
          },
          "execution_count": 830,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 831,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_ref['text'] = test_set_ref['text'].apply(unify_whitespace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_ref.to_json('data/test_set_ref_combined.jsonl', lines=True, orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the combined processed dataset [When already combined]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_set_ref = pd.read_json('data/test_set_ref_all.jsonl', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The day before [DATE_START] yesterday, [DATE_E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>um my date of birth is uh second [DATE_START] ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>she handed over a crumpled piece of paper, the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aglio olio and err uh [CARDINAL_START] three t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[PERSON_START] Hong's [PERSON_END] email is [E...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  The day before [DATE_START] yesterday, [DATE_E...\n",
              "1  um my date of birth is uh second [DATE_START] ...\n",
              "2  she handed over a crumpled piece of paper, the...\n",
              "3  aglio olio and err uh [CARDINAL_START] three t...\n",
              "4  [PERSON_START] Hong's [PERSON_END] email is [E..."
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>Patrick Loh boasting about his email [EMAIL_ST...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>Jasmine Yeo got sian when someone spell her em...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>Bobby Tan write his email [EMAIL_START] bobby....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>Kamala Singh telling the IT guy her email [EMA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>Raymond Koh say his email [EMAIL_START] raymon...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text\n",
              "495  Patrick Loh boasting about his email [EMAIL_ST...\n",
              "496  Jasmine Yeo got sian when someone spell her em...\n",
              "497  Bobby Tan write his email [EMAIL_START] bobby....\n",
              "498  Kamala Singh telling the IT guy her email [EMA...\n",
              "499  Raymond Koh say his email [EMAIL_START] raymon..."
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the model (Vosk)\n",
        "\n",
        "Unfortunately, there are no current models that are tuned for Singaporean English (Singlish). As such, we shall use the `vosk-model-en-us-0.42-gigaspeech` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=8\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
            "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
            "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from models/vosk-model-en-us-0.42-gigaspeech/ivector/final.ie\n",
            "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
            "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from models/vosk-model-en-us-0.42-gigaspeech/graph/HCLG.fst\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from models/vosk-model-en-us-0.42-gigaspeech/graph/words.txt\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo models/vosk-model-en-us-0.42-gigaspeech/graph/phones/word_boundary.int\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from models/vosk-model-en-us-0.42-gigaspeech/rescore/G.fst\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from models/vosk-model-en-us-0.42-gigaspeech/rescore/G.carpa\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:318) Loading RNNLM model from models/vosk-model-en-us-0.42-gigaspeech/rnnlm/final.raw\n"
          ]
        }
      ],
      "source": [
        "from vosk import Model, KaldiRecognizer\n",
        "import soundfile as sf\n",
        "import json\n",
        "\n",
        "# Load model (replace path with your model directory)\n",
        "model_path = \"models/vosk-model-en-us-0.42-gigaspeech\"\n",
        "model = Model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Read Audio (With just one sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So here, several things are happening:\n",
        "\n",
        "1. We create a `KaldiRecognizer` instance and set `.SetWords()` to `True`, which means that we will get word-level timestamps.\n",
        "2. The `.AcceptWaveform()` method is used to process the waveform\n",
        "3. The `.FinalResult()` method is finally called to retrieve the word-level timestamps (transcribed from the Vosk Model - although with some innacurracies, as Vosk is not a full-fledged ASR model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test on one audio sample (Simple example with Name and Email PIIs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {},
      "outputs": [],
      "source": [
        "audio_path = \"data/Audio_Files_for_testing/id85.wav\"\n",
        "audio_data, sample_rate = sf.read(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = run_vosk(audio_path, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'result': [{'conf': 1.0, 'end': 0.63, 'start': 0.45, 'word': 'i'},\n",
              "  {'conf': 1.0, 'end': 1.02, 'start': 0.63, 'word': 'lost'},\n",
              "  {'conf': 1.0, 'end': 1.2, 'start': 1.02, 'word': 'my'},\n",
              "  {'conf': 1.0, 'end': 1.77, 'start': 1.23, 'word': 'passport'},\n",
              "  {'conf': 1.0, 'end': 2.25, 'start': 1.98, 'word': 'and'},\n",
              "  {'conf': 1.0, 'end': 2.49, 'start': 2.31, 'word': 'it'},\n",
              "  {'conf': 1.0, 'end': 2.728719, 'start': 2.49, 'word': 'is'},\n",
              "  {'conf': 1.0, 'end': 3.09, 'start': 2.76, 'word': 'new'},\n",
              "  {'conf': 1.0, 'end': 3.39, 'start': 3.09, 'word': 'one'},\n",
              "  {'conf': 1.0, 'end': 5.07, 'start': 4.59, 'word': 'number'},\n",
              "  {'conf': 1.0, 'end': 5.46, 'start': 5.1, 'word': 'is'},\n",
              "  {'conf': 0.70072, 'end': 6.15, 'start': 5.79, 'word': 'gay'},\n",
              "  {'conf': 1.0, 'end': 6.75, 'start': 6.18, 'word': 'seven'},\n",
              "  {'conf': 1.0, 'end': 7.29, 'start': 6.84, 'word': 'six'},\n",
              "  {'conf': 1.0, 'end': 7.83, 'start': 7.29, 'word': 'zero'},\n",
              "  {'conf': 1.0, 'end': 8.227815, 'start': 7.98, 'word': 'one'},\n",
              "  {'conf': 0.630251, 'end': 8.49, 'start': 8.25, 'word': 'eight'},\n",
              "  {'conf': 0.811326, 'end': 9.21, 'start': 8.85, 'word': 'four'},\n",
              "  {'conf': 1.0, 'end': 9.57, 'start': 9.27, 'word': 'two'},\n",
              "  {'conf': 0.971156, 'end': 10.02, 'start': 9.72, 'word': 't'},\n",
              "  {'conf': 1.0, 'end': 10.8, 'start': 10.5, 'word': 'and'},\n",
              "  {'conf': 1.0, 'end': 11.55, 'start': 11.31, 'word': 'k'},\n",
              "  {'conf': 1.0, 'end': 12.03, 'start': 11.55, 'word': 'seven'},\n",
              "  {'conf': 0.694935, 'end': 12.66, 'start': 12.27, 'word': 'six'},\n",
              "  {'conf': 1.0, 'end': 13.11, 'start': 12.699152, 'word': 'zero'},\n",
              "  {'conf': 1.0, 'end': 13.62, 'start': 13.32, 'word': 'one'},\n",
              "  {'conf': 1.0, 'end': 13.95, 'start': 13.68, 'word': 'eight'},\n",
              "  {'conf': 1.0, 'end': 14.64, 'start': 14.25, 'word': 'four'},\n",
              "  {'conf': 1.0, 'end': 15.06, 'start': 14.73, 'word': 'two'},\n",
              "  {'conf': 0.524812, 'end': 16.02, 'start': 15.72, 'word': 'd'}],\n",
              " 'text': 'i lost my passport and it is new one number is gay seven six zero one eight four two t and k seven six zero one eight four two d'}"
            ]
          },
          "execution_count": 389,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 390,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sample['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'lost', 'my', 'passport', 'and', 'it', 'is', 'new', 'one', 'number', 'is', 'gay', 'seven', 'six', 'zero', 'one', 'eight', 'four', 'two', 't', 'and', 'k', 'seven', 'six', 'zero', 'one', 'eight', 'four', 'two', 'd']\n"
          ]
        }
      ],
      "source": [
        "vosk_output = []\n",
        "\n",
        "for item in sample['result']:\n",
        "    vosk_output.append(item['word'])\n",
        "\n",
        "print(vosk_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reference Transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I lost my passport and it new [CARDINAL_START] one [CARDINAL_END] number is [PASSPORT_NUM_START] K 7601842 T [PASSPORT_NUM_END] and [PASSPORT_NUM_START] K 7601842 T [PASSPORT_NUM_END]'"
            ]
          },
          "execution_count": 392,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref['text'].iloc[84]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens: 35\n",
            "['i', 'lost', 'my', 'passport', 'and', 'it', 'new', '[CARDINAL_START]', 'one', '[CARDINAL_END]', 'number', 'is', '[PASSPORT_NUM_START]', 'k', 'seven', 'six', 'zero', 'one', 'eight', 'four', 'two', 't', '[PASSPORT_NUM_END]', 'and', '[PASSPORT_NUM_START]', 'k', 'seven', 'six', 'zero', 'one', 'eight', 'four', 'two', 't', '[PASSPORT_NUM_END]']\n"
          ]
        }
      ],
      "source": [
        "tokenized_reference = tokenize_reference(test_set_ref['text'].iloc[84])\n",
        "\n",
        "print(\"Number of tokens:\", len(tokenized_reference))\n",
        "print(tokenized_reference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using `SequenceMatcher` by `difflib` to perform edit-distance based heuristics\n",
        "\n",
        "**Limitations of Greedy-alignment**\n",
        "\n",
        "As we observed, greedy-based approaches are not feasible due to the variability of the dataset plus the difference in output format between the Kaldi alignment model and the reference text. For example, some words in the reference text include fillers like \"uh\" whereas the Vosk model seem to omit it. Another example include the handling of contractions - with the Vosk model outputting contractions as-is (considered as one token) whereas the reference text separates the words (e.g., they're -> they are, considered as two tokens). Lastly, Kaldi is designed foremost as a forced-alignment model and not a speech-recognition model - which means that it often outputs mistranscriptions (e.g., seventy vs seven three). This can have severe impact when it comes to alignment quality. Therefore, it has since been decided that greedy-alignment is not sufficient for our use case.\n",
        "\n",
        "**Edit Distance Heuristics with `SequenceMatcher`**\n",
        "\n",
        "Edit-distance alignment approaches, such as those based on `SequenceMatcher`, offer a more robust alternative to greedy heuristics in aligning reference transcripts with forced alignment outputs. Unlike greedy methods that proceed sequentially and can easily desynchronize when encountering mismatches, edit-distance techniques globally evaluate the sequences and can better tolerate insertions, deletions, or substitutions. For example, in the reference transcript, the phrase \"they are\" may appear as two tokens, while Vosk outputs \"they're\" as a single token—this would break a greedy matcher, but an edit-distance algorithm would register this as a \"replace\" operation and continue aligning the rest correctly. Similarly, reference text may contain filler words like \"uh\" or structured tag blocks like `[EMAIL_START] foo bar [EMAIL_END]` that do not exist in the Vosk output. Edit-distance allows these to be treated as deletions while maintaining alignment integrity. Lastly, in numerical sequences, Vosk may transcribe \"seven three\" as \"seventy\", which would otherwise cause greedy approaches to misalign all following tokens. With edit-distance, such errors are localized and do not compromise the entire alignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 398,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "equal      ref[0:6] -> vosk[0:6]\n",
            "  REF : ['i', 'lost', 'my', 'passport', 'and', 'it']\n",
            "  VOSK: ['i', 'lost', 'my', 'passport', 'and', 'it']\n",
            "insert     ref[6:6] -> vosk[6:7]\n",
            "  REF : []\n",
            "  VOSK: ['is']\n",
            "equal      ref[6:10] -> vosk[7:11]\n",
            "  REF : ['new', 'one', 'number', 'is']\n",
            "  VOSK: ['new', 'one', 'number', 'is']\n",
            "replace    ref[10:11] -> vosk[11:12]\n",
            "  REF : ['k']\n",
            "  VOSK: ['gay']\n",
            "equal      ref[11:28] -> vosk[12:29]\n",
            "  REF : ['seven', 'six', 'zero', 'one', 'eight', 'four', 'two', 't', 'and', 'k', 'seven', 'six', 'zero', 'one', 'eight', 'four', 'two']\n",
            "  VOSK: ['seven', 'six', 'zero', 'one', 'eight', 'four', 'two', 't', 'and', 'k', 'seven', 'six', 'zero', 'one', 'eight', 'four', 'two']\n",
            "replace    ref[28:29] -> vosk[29:30]\n",
            "  REF : ['t']\n",
            "  VOSK: ['d']\n",
            "['i', 'lost', 'my', 'passport', 'and', 'it', 'new', '[CARDINAL_START]', 'one', '[CARDINAL_END]', 'number', 'is', '[PASSPORT_NUM_START]', 'k', 'seven', 'six', 'zero', 'one', 'eight', 'four', 'two', 't', '[PASSPORT_NUM_END]', 'and', '[PASSPORT_NUM_START]', 'k', 'seven', 'six', 'zero', 'one', 'eight', 'four', 'two', 't', '[PASSPORT_NUM_END]']\n"
          ]
        }
      ],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def is_tag(tok):\n",
        "    return tok.startswith('[') and tok.endswith(']')\n",
        "\n",
        "def prepare_alignment_tokens(ref_tokens):\n",
        "    \"\"\"Strip out tags but record their positions for reinsertion later.\"\"\"\n",
        "    ref_clean = []\n",
        "    tag_insertions = []  # list of (index, tag)\n",
        "    for tok in ref_tokens:\n",
        "        if is_tag(tok):\n",
        "            tag_insertions.append((len(ref_clean), tok))\n",
        "        else:\n",
        "            ref_clean.append(tok)\n",
        "    return ref_clean, tag_insertions\n",
        "\n",
        "# 1. Original reference tokens\n",
        "ref_tokens = tokenize_reference(test_set_ref['text'].iloc[84])\n",
        "\n",
        "# 2. Cleaned tokens for alignment (no tags)\n",
        "ref_clean, tag_insertions = prepare_alignment_tokens(ref_tokens)\n",
        "\n",
        "# 3. Vosk tokens\n",
        "vosk = ['i', 'lost', 'my', 'passport', 'and', 'it', 'is', 'new', 'one', 'number', 'is',\n",
        "        'gay', 'seven', 'six', 'zero', 'one', 'eight', 'four', 'two', 't', 'and',\n",
        "        'k', 'seven', 'six', 'zero', 'one', 'eight', 'four', 'two', 'd']\n",
        "\n",
        "# 4. Alignment\n",
        "matcher = SequenceMatcher(None, ref_clean, vosk, autojunk=False)\n",
        "\n",
        "# 5. Print match result\n",
        "for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
        "    print(f\"{tag:10} ref[{i1}:{i2}] -> vosk[{j1}:{j2}]\")\n",
        "    print(f\"  REF : {ref_clean[i1:i2]}\")\n",
        "    print(f\"  VOSK: {vosk[j1:j2]}\")\n",
        "\n",
        "# 6. (Optional) If you want to reconstruct the full aligned output with tags:\n",
        "# Reinsert tags into ref_clean if needed:\n",
        "for idx, tag in reversed(tag_insertions):\n",
        "    ref_clean.insert(idx, tag)\n",
        "\n",
        "print(ref_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Align vosk output with reference (edit-distance based approach)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 400,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'word': 'i', 'start': 0.45, 'end': 0.63},\n",
              " {'word': 'lost', 'start': 0.63, 'end': 1.02},\n",
              " {'word': 'my', 'start': 1.02, 'end': 1.2},\n",
              " {'word': 'passport', 'start': 1.23, 'end': 1.77},\n",
              " {'word': 'and', 'start': 1.98, 'end': 2.25},\n",
              " {'word': 'it', 'start': 2.31, 'end': 2.49},\n",
              " {'word': 'new', 'start': 2.76, 'end': 3.09},\n",
              " {'word': '[CARDINAL_START]', 'start': None, 'end': None},\n",
              " {'word': 'one', 'start': 3.09, 'end': 3.39},\n",
              " {'word': '[CARDINAL_END]', 'start': None, 'end': None},\n",
              " {'word': 'number', 'start': 4.59, 'end': 5.07},\n",
              " {'word': 'is', 'start': 5.1, 'end': 5.46},\n",
              " {'word': '[PASSPORT_NUM_START]', 'start': None, 'end': None},\n",
              " {'word': 'k', 'start': 5.79, 'end': 6.15},\n",
              " {'word': 'seven', 'start': 6.18, 'end': 6.75},\n",
              " {'word': 'six', 'start': 6.84, 'end': 7.29},\n",
              " {'word': 'zero', 'start': 7.29, 'end': 7.83},\n",
              " {'word': 'one', 'start': 7.98, 'end': 8.227815},\n",
              " {'word': 'eight', 'start': 8.25, 'end': 8.49},\n",
              " {'word': 'four', 'start': 8.85, 'end': 9.21},\n",
              " {'word': 'two', 'start': 9.27, 'end': 9.57},\n",
              " {'word': 't', 'start': 9.72, 'end': 10.02},\n",
              " {'word': '[PASSPORT_NUM_END]', 'start': None, 'end': None},\n",
              " {'word': 'and', 'start': 10.5, 'end': 10.8},\n",
              " {'word': '[PASSPORT_NUM_START]', 'start': None, 'end': None},\n",
              " {'word': 'k', 'start': 11.31, 'end': 11.55},\n",
              " {'word': 'seven', 'start': 11.55, 'end': 12.03},\n",
              " {'word': 'six', 'start': 12.27, 'end': 12.66},\n",
              " {'word': 'zero', 'start': 12.699152, 'end': 13.11},\n",
              " {'word': 'one', 'start': 13.32, 'end': 13.62},\n",
              " {'word': 'eight', 'start': 13.68, 'end': 13.95},\n",
              " {'word': 'four', 'start': 14.25, 'end': 14.64},\n",
              " {'word': 'two', 'start': 14.73, 'end': 15.06},\n",
              " {'word': 't', 'start': 15.72, 'end': 16.02},\n",
              " {'word': '[PASSPORT_NUM_END]', 'start': None, 'end': None}]"
            ]
          },
          "execution_count": 400,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "align_transcript_with_vosk(sample['result'], test_set_ref['text'].iloc[84])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test on one audio sample (Numerical PIIs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 432,
      "metadata": {},
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "audio_path = \"data/Audio_Files_for_testing/id20.wav\"\n",
        "audio_data, sample_rate = sf.read(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 433,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample2 = run_vosk(audio_path, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'result': [{'conf': 0.248183,\n",
              "   'end': 1.35,\n",
              "   'start': 0.87,\n",
              "   'word': 'mortality'},\n",
              "  {'conf': 0.573568, 'end': 2.13, 'start': 1.3512, 'word': 'qualities'},\n",
              "  {'conf': 1.0, 'end': 2.43, 'start': 2.22, 'word': 'and'},\n",
              "  {'conf': 1.0, 'end': 2.94, 'start': 2.43, 'word': 'transferred'},\n",
              "  {'conf': 1.0, 'end': 3.09, 'start': 2.94, 'word': 'to'},\n",
              "  {'conf': 1.0, 'end': 3.57, 'start': 3.12, 'word': 'dvds'},\n",
              "  {'conf': 1.0, 'end': 3.81, 'start': 3.57, 'word': 'bank'},\n",
              "  {'conf': 1.0, 'end': 4.14, 'start': 3.81, 'word': 'account'},\n",
              "  {'conf': 1.0, 'end': 4.41, 'start': 4.23, 'word': 'and'},\n",
              "  {'conf': 1.0, 'end': 4.89, 'start': 4.41, 'word': 'contact'},\n",
              "  {'conf': 1.0, 'end': 4.98, 'start': 4.89, 'word': 'the'},\n",
              "  {'conf': 1.0, 'end': 5.4, 'start': 4.98, 'word': 'same'},\n",
              "  {'conf': 1.0, 'end': 5.94, 'start': 5.43, 'word': 'six'},\n",
              "  {'conf': 1.0, 'end': 6.24, 'start': 5.97, 'word': 'eight'},\n",
              "  {'conf': 1.0, 'end': 6.54, 'start': 6.27, 'word': 'eight'},\n",
              "  {'conf': 1.0, 'end': 6.84, 'start': 6.54, 'word': 'nine'},\n",
              "  {'conf': 1.0, 'end': 7.35, 'start': 7.05, 'word': 'five'},\n",
              "  {'conf': 1.0, 'end': 7.68, 'start': 7.35, 'word': 'seven'},\n",
              "  {'conf': 0.95307, 'end': 7.98, 'start': 7.68, 'word': 'four'},\n",
              "  {'conf': 1.0, 'end': 8.25, 'start': 7.98, 'word': 'eight'},\n",
              "  {'conf': 1.0, 'end': 8.73, 'start': 8.61, 'word': 'or'},\n",
              "  {'conf': 1.0, 'end': 9.09, 'start': 8.73, 'word': 'six'},\n",
              "  {'conf': 1.0, 'end': 9.33, 'start': 9.12, 'word': 'eight'},\n",
              "  {'conf': 1.0, 'end': 9.6, 'start': 9.39, 'word': 'eight'},\n",
              "  {'conf': 1.0, 'end': 9.84, 'start': 9.6, 'word': 'nine'},\n",
              "  {'conf': 1.0, 'end': 10.11, 'start': 9.84, 'word': 'five'},\n",
              "  {'conf': 1.0, 'end': 10.41, 'start': 10.11, 'word': 'seven'},\n",
              "  {'conf': 0.639573, 'end': 10.65, 'start': 10.41, 'word': 'four'}],\n",
              " 'text': 'mortality qualities and transferred to dvds bank account and contact the same six eight eight nine five seven four eight or six eight eight nine five seven four'}"
            ]
          },
          "execution_count": 434,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ppo hotel holidays ppl and transfer to [ORG_START] DBS [ORG_END] bank account and contact is same [PHONE_START] 6889-5748 [PHONE_END] or [PHONE_START] 6889-5748 [PHONE_END]'"
            ]
          },
          "execution_count": 435,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref['text'].iloc[19]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Force align"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "credit_card, car_plate, bank_account, nric, phone, passport_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'word': 'ppo', 'start': 0.87, 'end': None},\n",
              " {'word': 'hotel', 'start': None, 'end': None},\n",
              " {'word': 'holidays', 'start': None, 'end': None},\n",
              " {'word': 'ppl', 'start': None, 'end': 2.13},\n",
              " {'word': 'and', 'start': 2.22, 'end': 2.43},\n",
              " {'word': 'transfer', 'start': 2.43, 'end': 2.94},\n",
              " {'word': 'to', 'start': 2.94, 'end': 3.09},\n",
              " {'word': '[ORG_START]', 'start': None, 'end': None},\n",
              " {'word': 'dbs', 'start': 3.12, 'end': 3.57},\n",
              " {'word': '[ORG_END]', 'start': None, 'end': None},\n",
              " {'word': 'bank', 'start': 3.57, 'end': 3.81},\n",
              " {'word': 'account', 'start': 3.81, 'end': 4.14},\n",
              " {'word': 'and', 'start': 4.23, 'end': 4.41},\n",
              " {'word': 'contact', 'start': 4.41, 'end': 4.89},\n",
              " {'word': 'is', 'start': 4.89, 'end': 4.98},\n",
              " {'word': 'same', 'start': 4.98, 'end': 5.4},\n",
              " {'word': '[PHONE_START]', 'start': None, 'end': None},\n",
              " {'word': 'six', 'start': 5.43, 'end': 5.94},\n",
              " {'word': 'eight', 'start': 5.97, 'end': 6.24},\n",
              " {'word': 'eight', 'start': 6.27, 'end': 6.54},\n",
              " {'word': 'nine', 'start': 6.54, 'end': 6.84},\n",
              " {'word': 'five', 'start': 7.05, 'end': 7.35},\n",
              " {'word': 'seven', 'start': 7.35, 'end': 7.68},\n",
              " {'word': 'four', 'start': 7.68, 'end': 7.98},\n",
              " {'word': 'eight', 'start': 7.98, 'end': 8.25},\n",
              " {'word': '[PHONE_END]', 'start': None, 'end': None},\n",
              " {'word': 'or', 'start': 8.61, 'end': 8.73},\n",
              " {'word': '[PHONE_START]', 'start': None, 'end': None},\n",
              " {'word': 'six', 'start': 8.73, 'end': 9.09},\n",
              " {'word': 'eight', 'start': 9.12, 'end': 9.33},\n",
              " {'word': 'eight', 'start': 9.39, 'end': 9.6},\n",
              " {'word': 'nine', 'start': 9.6, 'end': 9.84},\n",
              " {'word': 'five', 'start': 9.84, 'end': 10.11},\n",
              " {'word': 'seven', 'start': 10.11, 'end': 10.41},\n",
              " {'word': 'four', 'start': 10.41, 'end': 10.65},\n",
              " {'word': 'eight', 'start': None, 'end': None},\n",
              " {'word': '[PHONE_END]', 'start': None, 'end': None}]"
            ]
          },
          "execution_count": 436,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "align_transcript_with_vosk(sample2['result'], test_set_ref['text'].iloc[19])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample3 = run_vosk(\"data/newtest_151_500_updated_TTS/id500.wav\", model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'result': [{'conf': 1.0, 'end': 0.48, 'start': 0.09, 'word': 'raymond'},\n",
              "  {'conf': 0.717032, 'end': 0.81, 'start': 0.48, 'word': 'co'},\n",
              "  {'conf': 1.0, 'end': 1.08, 'start': 0.81, 'word': 'say'},\n",
              "  {'conf': 1.0, 'end': 1.32, 'start': 1.08, 'word': 'his'},\n",
              "  {'conf': 1.0, 'end': 1.83, 'start': 1.35, 'word': 'email'},\n",
              "  {'conf': 1.0, 'end': 2.37, 'start': 1.86, 'word': 'raymond'},\n",
              "  {'conf': 1.0, 'end': 2.64, 'start': 2.37, 'word': 'dot'},\n",
              "  {'conf': 0.886485, 'end': 3.0, 'start': 2.64, 'word': 'k'},\n",
              "  {'conf': 1.0, 'end': 3.18, 'start': 3.03, 'word': 'at'},\n",
              "  {'conf': 0.414042, 'end': 3.51, 'start': 3.18, 'word': 'sing'},\n",
              "  {'conf': 1.0, 'end': 3.72, 'start': 3.51, 'word': 'that'},\n",
              "  {'conf': 1.0, 'end': 4.02, 'start': 3.72, 'word': 'dot'},\n",
              "  {'conf': 1.0, 'end': 4.41, 'start': 4.02, 'word': 'com'},\n",
              "  {'conf': 1.0, 'end': 4.68, 'start': 4.41, 'word': 'dot'},\n",
              "  {'conf': 0.913401, 'end': 4.95, 'start': 4.71, 'word': 's'},\n",
              "  {'conf': 0.936325, 'end': 5.31, 'start': 4.95, 'word': 'g'},\n",
              "  {'conf': 1.0, 'end': 5.79, 'start': 5.55, 'word': 'thought'},\n",
              "  {'conf': 1.0, 'end': 6.06, 'start': 5.79, 'word': 'so'},\n",
              "  {'conf': 1.0, 'end': 6.33, 'start': 6.06, 'word': 'many'},\n",
              "  {'conf': 1.0, 'end': 6.81, 'start': 6.33, 'word': 'spam'},\n",
              "  {'conf': 1.0, 'end': 7.32, 'start': 7.05, 'word': 'like'},\n",
              "  {'conf': 1.0, 'end': 7.77, 'start': 7.35, 'word': 'free'},\n",
              "  {'conf': 0.983461, 'end': 8.01, 'start': 7.77, 'word': 'low'},\n",
              "  {'conf': 1.0, 'end': 8.28, 'start': 8.01, 'word': 'bang'},\n",
              "  {'conf': 1.0, 'end': 8.46, 'start': 8.28, 'word': 'for'},\n",
              "  {'conf': 1.0, 'end': 9.21, 'start': 8.46, 'word': 'advertisers'},\n",
              "  {'conf': 0.806378, 'end': 9.36, 'start': 9.21, 'word': 'see'},\n",
              "  {'conf': 0.806378, 'end': 9.57, 'start': 9.36, 'word': 'ya'}],\n",
              " 'text': 'raymond co say his email raymond dot k at sing that dot com dot s g thought so many spam like free low bang for advertisers see ya'}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Raymond Koh say his email [EMAIL_START] raymond.k@singnet.com.s g [EMAIL_END] got so many spam, like free lobang for advertisers sia'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref['text'].iloc[499]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'word': 'raymond', 'start': 0.09, 'end': 0.48},\n",
              " {'word': 'koh', 'start': 0.48, 'end': 0.81},\n",
              " {'word': 'say', 'start': 0.81, 'end': 1.08},\n",
              " {'word': 'his', 'start': 1.08, 'end': 1.32},\n",
              " {'word': 'email', 'start': 1.35, 'end': 1.83},\n",
              " {'word': '[EMAIL_START]', 'start': None, 'end': None},\n",
              " {'word': 'raymond', 'start': 1.86, 'end': 2.37},\n",
              " {'word': '.', 'start': 2.37, 'end': 2.64},\n",
              " {'word': 'k', 'start': 2.64, 'end': 3.0},\n",
              " {'word': '@', 'start': 3.03, 'end': None},\n",
              " {'word': 'singnet', 'start': None, 'end': None},\n",
              " {'word': '.', 'start': None, 'end': 4.02},\n",
              " {'word': 'com', 'start': 4.02, 'end': 4.41},\n",
              " {'word': '.', 'start': 4.41, 'end': 4.68},\n",
              " {'word': 's', 'start': 4.71, 'end': 4.95},\n",
              " {'word': 'g', 'start': 4.95, 'end': 5.31},\n",
              " {'word': '[EMAIL_END]', 'start': 5.55, 'end': None},\n",
              " {'word': 'got', 'start': None, 'end': 5.79},\n",
              " {'word': 'so', 'start': 5.79, 'end': 6.06},\n",
              " {'word': 'many', 'start': 6.06, 'end': 6.33},\n",
              " {'word': 'spam', 'start': 6.33, 'end': 6.81},\n",
              " {'word': 'like', 'start': 7.05, 'end': 7.32},\n",
              " {'word': 'free', 'start': 7.35, 'end': 7.77},\n",
              " {'word': 'lobang', 'start': 7.77, 'end': 8.28},\n",
              " {'word': 'for', 'start': 8.28, 'end': 8.46},\n",
              " {'word': 'advertisers', 'start': 8.46, 'end': 9.21},\n",
              " {'word': 'sia', 'start': 9.21, 'end': 9.57}]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "align_transcript_with_vosk(sample3['result'], test_set_ref['text'].iloc[499])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Read Audio (All 500 samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 437,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "audio_paths = sorted(os.listdir(\"data/Audio_Files_for_testing\") + os.listdir(\"data/newtest_151_500_updated_TTS\"), key=retrieve_key)\n",
        "audio_paths_with_parent = [f'data/Audio_Files_for_testing/{file}' for file in audio_paths if file.endswith('.wav') and retrieve_key(file) < 151]\n",
        "audio_paths_with_parent += [f'data/newtest_151_500_updated_TTS/{file}' for file in audio_paths if file.endswith('.wav') and retrieve_key(file) >= 151]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 438,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['data/Audio_Files_for_testing/id1.wav',\n",
              " 'data/Audio_Files_for_testing/id2.wav',\n",
              " 'data/Audio_Files_for_testing/id3.wav',\n",
              " 'data/Audio_Files_for_testing/id4.wav',\n",
              " 'data/Audio_Files_for_testing/id5.wav',\n",
              " 'data/Audio_Files_for_testing/id6.wav',\n",
              " 'data/Audio_Files_for_testing/id7.wav',\n",
              " 'data/Audio_Files_for_testing/id8.wav',\n",
              " 'data/Audio_Files_for_testing/id9.wav',\n",
              " 'data/Audio_Files_for_testing/id10.wav',\n",
              " 'data/Audio_Files_for_testing/id11.wav',\n",
              " 'data/Audio_Files_for_testing/id12.wav',\n",
              " 'data/Audio_Files_for_testing/id13.wav',\n",
              " 'data/Audio_Files_for_testing/id14.wav',\n",
              " 'data/Audio_Files_for_testing/id15.wav',\n",
              " 'data/Audio_Files_for_testing/id16.wav',\n",
              " 'data/Audio_Files_for_testing/id17.wav',\n",
              " 'data/Audio_Files_for_testing/id18.wav',\n",
              " 'data/Audio_Files_for_testing/id19.wav',\n",
              " 'data/Audio_Files_for_testing/id20.wav',\n",
              " 'data/Audio_Files_for_testing/id21.wav',\n",
              " 'data/Audio_Files_for_testing/id22.wav',\n",
              " 'data/Audio_Files_for_testing/id23.wav',\n",
              " 'data/Audio_Files_for_testing/id24.wav',\n",
              " 'data/Audio_Files_for_testing/id25.wav',\n",
              " 'data/Audio_Files_for_testing/id26.wav',\n",
              " 'data/Audio_Files_for_testing/id27.wav',\n",
              " 'data/Audio_Files_for_testing/id28.wav',\n",
              " 'data/Audio_Files_for_testing/id29.wav',\n",
              " 'data/Audio_Files_for_testing/id30.wav',\n",
              " 'data/Audio_Files_for_testing/id31.wav',\n",
              " 'data/Audio_Files_for_testing/id32.wav',\n",
              " 'data/Audio_Files_for_testing/id33.wav',\n",
              " 'data/Audio_Files_for_testing/id34.wav',\n",
              " 'data/Audio_Files_for_testing/id35.wav',\n",
              " 'data/Audio_Files_for_testing/id36.wav',\n",
              " 'data/Audio_Files_for_testing/id37.wav',\n",
              " 'data/Audio_Files_for_testing/id38.wav',\n",
              " 'data/Audio_Files_for_testing/id39.wav',\n",
              " 'data/Audio_Files_for_testing/id40.wav',\n",
              " 'data/Audio_Files_for_testing/id41.wav',\n",
              " 'data/Audio_Files_for_testing/id42.wav',\n",
              " 'data/Audio_Files_for_testing/id43.wav',\n",
              " 'data/Audio_Files_for_testing/id44.wav',\n",
              " 'data/Audio_Files_for_testing/id45.wav',\n",
              " 'data/Audio_Files_for_testing/id46.wav',\n",
              " 'data/Audio_Files_for_testing/id47.wav',\n",
              " 'data/Audio_Files_for_testing/id48.wav',\n",
              " 'data/Audio_Files_for_testing/id49.wav',\n",
              " 'data/Audio_Files_for_testing/id50.wav',\n",
              " 'data/Audio_Files_for_testing/id51.wav',\n",
              " 'data/Audio_Files_for_testing/id52.wav',\n",
              " 'data/Audio_Files_for_testing/id53.wav',\n",
              " 'data/Audio_Files_for_testing/id54.wav',\n",
              " 'data/Audio_Files_for_testing/id55.wav',\n",
              " 'data/Audio_Files_for_testing/id56.wav',\n",
              " 'data/Audio_Files_for_testing/id57.wav',\n",
              " 'data/Audio_Files_for_testing/id58.wav',\n",
              " 'data/Audio_Files_for_testing/id59.wav',\n",
              " 'data/Audio_Files_for_testing/id60.wav',\n",
              " 'data/Audio_Files_for_testing/id61.wav',\n",
              " 'data/Audio_Files_for_testing/id62.wav',\n",
              " 'data/Audio_Files_for_testing/id63.wav',\n",
              " 'data/Audio_Files_for_testing/id64.wav',\n",
              " 'data/Audio_Files_for_testing/id65.wav',\n",
              " 'data/Audio_Files_for_testing/id66.wav',\n",
              " 'data/Audio_Files_for_testing/id67.wav',\n",
              " 'data/Audio_Files_for_testing/id68.wav',\n",
              " 'data/Audio_Files_for_testing/id69.wav',\n",
              " 'data/Audio_Files_for_testing/id70.wav',\n",
              " 'data/Audio_Files_for_testing/id71.wav',\n",
              " 'data/Audio_Files_for_testing/id72.wav',\n",
              " 'data/Audio_Files_for_testing/id73.wav',\n",
              " 'data/Audio_Files_for_testing/id74.wav',\n",
              " 'data/Audio_Files_for_testing/id75.wav',\n",
              " 'data/Audio_Files_for_testing/id76.wav',\n",
              " 'data/Audio_Files_for_testing/id77.wav',\n",
              " 'data/Audio_Files_for_testing/id78.wav',\n",
              " 'data/Audio_Files_for_testing/id79.wav',\n",
              " 'data/Audio_Files_for_testing/id80.wav',\n",
              " 'data/Audio_Files_for_testing/id81.wav',\n",
              " 'data/Audio_Files_for_testing/id82.wav',\n",
              " 'data/Audio_Files_for_testing/id83.wav',\n",
              " 'data/Audio_Files_for_testing/id84.wav',\n",
              " 'data/Audio_Files_for_testing/id85.wav',\n",
              " 'data/Audio_Files_for_testing/id86.wav',\n",
              " 'data/Audio_Files_for_testing/id87.wav',\n",
              " 'data/Audio_Files_for_testing/id88.wav',\n",
              " 'data/Audio_Files_for_testing/id89.wav',\n",
              " 'data/Audio_Files_for_testing/id90.wav',\n",
              " 'data/Audio_Files_for_testing/id91.wav',\n",
              " 'data/Audio_Files_for_testing/id92.wav',\n",
              " 'data/Audio_Files_for_testing/id93.wav',\n",
              " 'data/Audio_Files_for_testing/id94.wav',\n",
              " 'data/Audio_Files_for_testing/id95.wav',\n",
              " 'data/Audio_Files_for_testing/id96.wav',\n",
              " 'data/Audio_Files_for_testing/id97.wav',\n",
              " 'data/Audio_Files_for_testing/id98.wav',\n",
              " 'data/Audio_Files_for_testing/id99.wav',\n",
              " 'data/Audio_Files_for_testing/id100.wav',\n",
              " 'data/Audio_Files_for_testing/id101.wav',\n",
              " 'data/Audio_Files_for_testing/id102.wav',\n",
              " 'data/Audio_Files_for_testing/id103.wav',\n",
              " 'data/Audio_Files_for_testing/id104.wav',\n",
              " 'data/Audio_Files_for_testing/id105.wav',\n",
              " 'data/Audio_Files_for_testing/id106.wav',\n",
              " 'data/Audio_Files_for_testing/id107.wav',\n",
              " 'data/Audio_Files_for_testing/id108.wav',\n",
              " 'data/Audio_Files_for_testing/id109.wav',\n",
              " 'data/Audio_Files_for_testing/id110.wav',\n",
              " 'data/Audio_Files_for_testing/id111.wav',\n",
              " 'data/Audio_Files_for_testing/id112.wav',\n",
              " 'data/Audio_Files_for_testing/id113.wav',\n",
              " 'data/Audio_Files_for_testing/id114.wav',\n",
              " 'data/Audio_Files_for_testing/id115.wav',\n",
              " 'data/Audio_Files_for_testing/id116.wav',\n",
              " 'data/Audio_Files_for_testing/id117.wav',\n",
              " 'data/Audio_Files_for_testing/id118.wav',\n",
              " 'data/Audio_Files_for_testing/id119.wav',\n",
              " 'data/Audio_Files_for_testing/id120.wav',\n",
              " 'data/Audio_Files_for_testing/id121.wav',\n",
              " 'data/Audio_Files_for_testing/id122.wav',\n",
              " 'data/Audio_Files_for_testing/id123.wav',\n",
              " 'data/Audio_Files_for_testing/id124.wav',\n",
              " 'data/Audio_Files_for_testing/id125.wav',\n",
              " 'data/Audio_Files_for_testing/id126.wav',\n",
              " 'data/Audio_Files_for_testing/id127.wav',\n",
              " 'data/Audio_Files_for_testing/id128.wav',\n",
              " 'data/Audio_Files_for_testing/id129.wav',\n",
              " 'data/Audio_Files_for_testing/id130.wav',\n",
              " 'data/Audio_Files_for_testing/id131.wav',\n",
              " 'data/Audio_Files_for_testing/id132.wav',\n",
              " 'data/Audio_Files_for_testing/id133.wav',\n",
              " 'data/Audio_Files_for_testing/id134.wav',\n",
              " 'data/Audio_Files_for_testing/id135.wav',\n",
              " 'data/Audio_Files_for_testing/id136.wav',\n",
              " 'data/Audio_Files_for_testing/id137.wav',\n",
              " 'data/Audio_Files_for_testing/id138.wav',\n",
              " 'data/Audio_Files_for_testing/id139.wav',\n",
              " 'data/Audio_Files_for_testing/id140.wav',\n",
              " 'data/Audio_Files_for_testing/id141.wav',\n",
              " 'data/Audio_Files_for_testing/id142.wav',\n",
              " 'data/Audio_Files_for_testing/id143.wav',\n",
              " 'data/Audio_Files_for_testing/id144.wav',\n",
              " 'data/Audio_Files_for_testing/id145.wav',\n",
              " 'data/Audio_Files_for_testing/id146.wav',\n",
              " 'data/Audio_Files_for_testing/id147.wav',\n",
              " 'data/Audio_Files_for_testing/id148.wav',\n",
              " 'data/Audio_Files_for_testing/id149.wav',\n",
              " 'data/Audio_Files_for_testing/id150.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id151.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id152.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id153.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id154.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id155.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id156.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id157.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id158.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id159.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id160.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id161.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id162.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id163.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id164.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id165.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id166.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id167.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id168.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id169.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id170.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id171.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id172.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id173.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id174.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id175.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id176.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id177.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id178.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id179.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id180.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id181.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id182.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id183.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id184.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id185.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id186.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id187.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id188.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id189.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id190.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id191.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id192.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id193.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id194.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id195.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id196.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id197.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id198.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id199.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id200.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id201.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id202.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id203.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id204.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id205.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id206.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id207.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id208.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id209.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id210.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id211.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id212.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id213.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id214.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id215.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id216.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id217.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id218.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id219.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id220.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id221.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id222.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id223.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id224.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id225.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id226.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id227.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id228.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id229.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id230.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id231.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id232.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id233.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id234.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id235.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id236.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id237.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id238.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id239.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id240.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id241.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id242.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id243.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id244.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id245.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id246.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id247.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id248.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id249.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id250.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id251.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id252.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id253.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id254.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id255.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id256.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id257.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id258.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id259.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id260.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id261.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id262.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id263.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id264.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id265.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id266.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id267.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id268.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id269.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id270.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id271.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id272.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id273.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id274.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id275.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id276.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id277.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id278.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id279.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id280.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id281.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id282.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id283.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id284.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id285.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id286.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id287.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id288.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id289.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id290.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id291.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id292.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id293.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id294.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id295.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id296.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id297.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id298.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id299.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id300.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id301.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id302.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id303.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id304.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id305.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id306.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id307.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id308.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id309.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id310.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id311.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id312.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id313.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id314.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id315.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id316.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id317.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id318.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id319.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id320.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id321.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id322.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id323.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id324.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id325.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id326.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id327.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id328.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id329.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id330.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id331.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id332.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id333.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id334.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id335.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id336.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id337.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id338.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id339.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id340.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id341.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id342.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id343.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id344.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id345.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id346.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id347.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id348.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id349.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id350.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id351.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id352.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id353.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id354.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id355.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id356.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id357.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id358.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id359.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id360.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id361.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id362.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id363.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id364.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id365.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id366.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id367.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id368.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id369.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id370.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id371.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id372.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id373.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id374.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id375.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id376.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id377.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id378.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id379.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id380.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id381.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id382.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id383.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id384.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id385.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id386.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id387.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id388.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id389.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id390.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id391.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id392.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id393.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id394.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id395.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id396.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id397.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id398.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id399.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id400.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id401.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id402.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id403.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id404.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id405.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id406.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id407.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id408.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id409.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id410.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id411.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id412.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id413.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id414.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id415.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id416.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id417.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id418.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id419.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id420.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id421.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id422.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id423.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id424.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id425.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id426.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id427.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id428.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id429.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id430.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id431.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id432.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id433.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id434.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id435.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id436.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id437.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id438.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id439.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id440.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id441.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id442.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id443.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id444.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id445.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id446.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id447.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id448.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id449.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id450.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id451.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id452.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id453.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id454.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id455.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id456.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id457.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id458.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id459.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id460.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id461.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id462.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id463.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id464.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id465.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id466.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id467.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id468.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id469.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id470.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id471.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id472.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id473.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id474.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id475.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id476.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id477.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id478.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id479.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id480.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id481.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id482.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id483.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id484.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id485.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id486.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id487.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id488.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id489.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id490.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id491.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id492.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id493.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id494.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id495.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id496.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id497.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id498.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id499.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id500.wav']"
            ]
          },
          "execution_count": 438,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "audio_paths_with_parent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 439,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "execution_count": 439,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(audio_paths_with_parent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 440,
      "metadata": {},
      "outputs": [],
      "source": [
        "aligned_transcripts = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 441,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running forced alignment algorithm:  48%|████▊     | 242/500 [06:35<02:36,  1.65files/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing data/newtest_151_500_updated_TTS/id243.wav: 'result'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running forced alignment algorithm: 100%|██████████| 500/500 [12:39<00:00,  1.52s/files]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for audio in tqdm(audio_paths_with_parent, desc=\"Running forced alignment algorithm\", unit=\"files\", total=len(audio_paths_with_parent)):\n",
        "    try:\n",
        "        sample = run_vosk(audio, model)\n",
        "        vosk_output = sample['result']\n",
        "        # Ref text just get the number e.g., id48\n",
        "        idx = int(extract_id_number(audio))\n",
        "        ref_text = test_set_ref['text'].iloc[idx - 1]\n",
        "        ref_text_aligned = align_transcript_with_vosk(vosk_output, ref_text)\n",
        "        aligned_transcripts.append({\n",
        "            'file_name': audio,\n",
        "            'vosk_output': vosk_output,\n",
        "            'ref_text': ref_text,\n",
        "            'aligned_transcript': ref_text_aligned\n",
        "        })\n",
        "        # print(ref_text_aligned)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio}: {e}\")\n",
        "        aligned_transcripts.append({\n",
        "            'file_name': audio,\n",
        "            'vosk_output': None,\n",
        "            'ref_text': None,\n",
        "            'aligned_transcript': None\n",
        "        })\n",
        "        continue        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 459,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>vosk_output</th>\n",
              "      <th>ref_text</th>\n",
              "      <th>aligned_transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/Audio_Files_for_testing/id1.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.69, 'start': 0.51, 'wo...</td>\n",
              "      <td>The day before [DATE_START] yesterday, [DATE_E...</td>\n",
              "      <td>[{'word': 'the', 'start': 0.51, 'end': 0.69}, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/Audio_Files_for_testing/id2.wav</td>\n",
              "      <td>[{'conf': 0.681104, 'end': 1.14, 'start': 0.69...</td>\n",
              "      <td>um my date of birth is uh second [DATE_START] ...</td>\n",
              "      <td>[{'word': 'um', 'start': 0.69, 'end': 1.14}, {...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/Audio_Files_for_testing/id3.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.84, 'start': 0.48, 'wo...</td>\n",
              "      <td>she handed over a crumpled piece of paper, the...</td>\n",
              "      <td>[{'word': 'she', 'start': 0.48, 'end': 0.84}, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/Audio_Files_for_testing/id4.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 1.5, 'start': 1.11, 'wor...</td>\n",
              "      <td>aglio olio and err uh [CARDINAL_START] three t...</td>\n",
              "      <td>[{'word': 'aglio', 'start': None, 'end': None}...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/Audio_Files_for_testing/id5.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 1.65, 'start': 1.14, 'wo...</td>\n",
              "      <td>[PERSON_START] Hong's [PERSON_END] email is [E...</td>\n",
              "      <td>[{'word': '[PERSON_START]', 'start': None, 'en...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              file_name  \\\n",
              "0  data/Audio_Files_for_testing/id1.wav   \n",
              "1  data/Audio_Files_for_testing/id2.wav   \n",
              "2  data/Audio_Files_for_testing/id3.wav   \n",
              "3  data/Audio_Files_for_testing/id4.wav   \n",
              "4  data/Audio_Files_for_testing/id5.wav   \n",
              "\n",
              "                                         vosk_output  \\\n",
              "0  [{'conf': 1.0, 'end': 0.69, 'start': 0.51, 'wo...   \n",
              "1  [{'conf': 0.681104, 'end': 1.14, 'start': 0.69...   \n",
              "2  [{'conf': 1.0, 'end': 0.84, 'start': 0.48, 'wo...   \n",
              "3  [{'conf': 1.0, 'end': 1.5, 'start': 1.11, 'wor...   \n",
              "4  [{'conf': 1.0, 'end': 1.65, 'start': 1.14, 'wo...   \n",
              "\n",
              "                                            ref_text  \\\n",
              "0  The day before [DATE_START] yesterday, [DATE_E...   \n",
              "1  um my date of birth is uh second [DATE_START] ...   \n",
              "2  she handed over a crumpled piece of paper, the...   \n",
              "3  aglio olio and err uh [CARDINAL_START] three t...   \n",
              "4  [PERSON_START] Hong's [PERSON_END] email is [E...   \n",
              "\n",
              "                                  aligned_transcript  \n",
              "0  [{'word': 'the', 'start': 0.51, 'end': 0.69}, ...  \n",
              "1  [{'word': 'um', 'start': 0.69, 'end': 1.14}, {...  \n",
              "2  [{'word': 'she', 'start': 0.48, 'end': 0.84}, ...  \n",
              "3  [{'word': 'aglio', 'start': None, 'end': None}...  \n",
              "4  [{'word': '[PERSON_START]', 'start': None, 'en...  "
            ]
          },
          "execution_count": 459,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df = pd.DataFrame(aligned_transcripts)\n",
        "processed_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 443,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'conf': 1.0, 'end': 1.5, 'start': 1.11, 'word': 'and'},\n",
              " {'conf': 0.855331, 'end': 2.22, 'start': 1.62, 'word': 'ah'},\n",
              " {'conf': 1.0, 'end': 2.94, 'start': 2.261498, 'word': 'ah'},\n",
              " {'conf': 1.0, 'end': 3.39, 'start': 3.09, 'word': 'three'},\n",
              " {'conf': 1.0, 'end': 3.78, 'start': 3.39, 'word': 'three'},\n",
              " {'conf': 1.0, 'end': 3.99, 'start': 3.81, 'word': 'of'},\n",
              " {'conf': 1.0, 'end': 4.11, 'start': 3.99, 'word': 'the'},\n",
              " {'conf': 1.0, 'end': 4.5, 'start': 4.23, 'word': 'other'},\n",
              " {'conf': 1.0, 'end': 4.8, 'start': 4.5, 'word': 'one'},\n",
              " {'conf': 1.0, 'end': 5.13, 'start': 4.83, 'word': 'yeah'}]"
            ]
          },
          "execution_count": 443,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df['vosk_output'].iloc[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 444,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>vosk_output</th>\n",
              "      <th>ref_text</th>\n",
              "      <th>aligned_transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id496.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.45, 'start': 0.06, 'wo...</td>\n",
              "      <td>Patrick Loh boasting about his email [EMAIL_ST...</td>\n",
              "      <td>[{'word': 'patrick', 'start': 0.06, 'end': 0.4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id497.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.45, 'start': 0.03, 'wo...</td>\n",
              "      <td>Jasmine Yeo got sian when someone spell her em...</td>\n",
              "      <td>[{'word': 'jasmine', 'start': 0.03, 'end': 0.4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id498.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.45, 'start': 0.03, 'wo...</td>\n",
              "      <td>Bobby Tan write his email [EMAIL_START] bobby....</td>\n",
              "      <td>[{'word': 'bobby', 'start': 0.03, 'end': 0.45}...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id499.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.48, 'start': 0.06, 'wo...</td>\n",
              "      <td>Kamala Singh telling the IT guy her email [EMA...</td>\n",
              "      <td>[{'word': 'kamala', 'start': 0.06, 'end': 0.48...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id500.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.51, 'start': 0.09, 'wo...</td>\n",
              "      <td>Raymond Koh say his email [EMAIL_START] raymon...</td>\n",
              "      <td>[{'word': 'raymond', 'start': 0.09, 'end': 0.5...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      file_name  \\\n",
              "495  data/newtest_151_500_updated_TTS/id496.wav   \n",
              "496  data/newtest_151_500_updated_TTS/id497.wav   \n",
              "497  data/newtest_151_500_updated_TTS/id498.wav   \n",
              "498  data/newtest_151_500_updated_TTS/id499.wav   \n",
              "499  data/newtest_151_500_updated_TTS/id500.wav   \n",
              "\n",
              "                                           vosk_output  \\\n",
              "495  [{'conf': 1.0, 'end': 0.45, 'start': 0.06, 'wo...   \n",
              "496  [{'conf': 1.0, 'end': 0.45, 'start': 0.03, 'wo...   \n",
              "497  [{'conf': 1.0, 'end': 0.45, 'start': 0.03, 'wo...   \n",
              "498  [{'conf': 1.0, 'end': 0.48, 'start': 0.06, 'wo...   \n",
              "499  [{'conf': 1.0, 'end': 0.51, 'start': 0.09, 'wo...   \n",
              "\n",
              "                                              ref_text  \\\n",
              "495  Patrick Loh boasting about his email [EMAIL_ST...   \n",
              "496  Jasmine Yeo got sian when someone spell her em...   \n",
              "497  Bobby Tan write his email [EMAIL_START] bobby....   \n",
              "498  Kamala Singh telling the IT guy her email [EMA...   \n",
              "499  Raymond Koh say his email [EMAIL_START] raymon...   \n",
              "\n",
              "                                    aligned_transcript  \n",
              "495  [{'word': 'patrick', 'start': 0.06, 'end': 0.4...  \n",
              "496  [{'word': 'jasmine', 'start': 0.03, 'end': 0.4...  \n",
              "497  [{'word': 'bobby', 'start': 0.03, 'end': 0.45}...  \n",
              "498  [{'word': 'kamala', 'start': 0.06, 'end': 0.48...  \n",
              "499  [{'word': 'raymond', 'start': 0.09, 'end': 0.5...  "
            ]
          },
          "execution_count": 444,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 445,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_df.to_json('data/processed_test_set_aligned.json', lines=True, orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Triplets\n",
        "\n",
        "The triplets contain the `start_time`, `end_time`, and `entity`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 446,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_df = pd.read_json('data/processed_test_set_aligned.json', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 447,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>vosk_output</th>\n",
              "      <th>ref_text</th>\n",
              "      <th>aligned_transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/Audio_Files_for_testing/id1.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.6900000000000001, 'sta...</td>\n",
              "      <td>The day before [DATE_START] yesterday, [DATE_E...</td>\n",
              "      <td>[{'word': 'the', 'start': 0.51, 'end': 0.69000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/Audio_Files_for_testing/id2.wav</td>\n",
              "      <td>[{'conf': 0.6811039999999999, 'end': 1.14, 'st...</td>\n",
              "      <td>um my date of birth is uh second [DATE_START] ...</td>\n",
              "      <td>[{'word': 'um', 'start': 0.6900000000000001, '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/Audio_Files_for_testing/id3.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.84, 'start': 0.48, 'wo...</td>\n",
              "      <td>she handed over a crumpled piece of paper, the...</td>\n",
              "      <td>[{'word': 'she', 'start': 0.48, 'end': 0.84}, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/Audio_Files_for_testing/id4.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 1.5, 'start': 1.11, 'wor...</td>\n",
              "      <td>aglio olio and err uh [CARDINAL_START] three t...</td>\n",
              "      <td>[{'word': 'aglio', 'start': None, 'end': None}...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/Audio_Files_for_testing/id5.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 1.65, 'start': 1.14, 'wo...</td>\n",
              "      <td>[PERSON_START] Hong's [PERSON_END] email is [E...</td>\n",
              "      <td>[{'word': '[PERSON_START]', 'start': None, 'en...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              file_name  \\\n",
              "0  data/Audio_Files_for_testing/id1.wav   \n",
              "1  data/Audio_Files_for_testing/id2.wav   \n",
              "2  data/Audio_Files_for_testing/id3.wav   \n",
              "3  data/Audio_Files_for_testing/id4.wav   \n",
              "4  data/Audio_Files_for_testing/id5.wav   \n",
              "\n",
              "                                         vosk_output  \\\n",
              "0  [{'conf': 1.0, 'end': 0.6900000000000001, 'sta...   \n",
              "1  [{'conf': 0.6811039999999999, 'end': 1.14, 'st...   \n",
              "2  [{'conf': 1.0, 'end': 0.84, 'start': 0.48, 'wo...   \n",
              "3  [{'conf': 1.0, 'end': 1.5, 'start': 1.11, 'wor...   \n",
              "4  [{'conf': 1.0, 'end': 1.65, 'start': 1.14, 'wo...   \n",
              "\n",
              "                                            ref_text  \\\n",
              "0  The day before [DATE_START] yesterday, [DATE_E...   \n",
              "1  um my date of birth is uh second [DATE_START] ...   \n",
              "2  she handed over a crumpled piece of paper, the...   \n",
              "3  aglio olio and err uh [CARDINAL_START] three t...   \n",
              "4  [PERSON_START] Hong's [PERSON_END] email is [E...   \n",
              "\n",
              "                                  aligned_transcript  \n",
              "0  [{'word': 'the', 'start': 0.51, 'end': 0.69000...  \n",
              "1  [{'word': 'um', 'start': 0.6900000000000001, '...  \n",
              "2  [{'word': 'she', 'start': 0.48, 'end': 0.84}, ...  \n",
              "3  [{'word': 'aglio', 'start': None, 'end': None}...  \n",
              "4  [{'word': '[PERSON_START]', 'start': None, 'en...  "
            ]
          },
          "execution_count": 447,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 456,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'word': 'desmond', 'start': 0.03, 'end': 0.6000000000000001},\n",
              " {'word': 'tan', 'start': 0.6000000000000001, 'end': 1.35},\n",
              " {'word': 'his', 'start': None, 'end': None},\n",
              " {'word': 'i', 'start': None, 'end': None},\n",
              " {'word': 'c', 'start': None, 'end': None},\n",
              " {'word': '[NRIC_START]', 'start': None, 'end': None},\n",
              " {'word': 's', 'start': None, 'end': None},\n",
              " {'word': 'six', 'start': None, 'end': None},\n",
              " {'word': 'seven', 'start': None, 'end': None},\n",
              " {'word': 'eight', 'start': None, 'end': None},\n",
              " {'word': 'nine', 'start': None, 'end': None},\n",
              " {'word': 'zero', 'start': None, 'end': None},\n",
              " {'word': 'one', 'start': None, 'end': None},\n",
              " {'word': 'two', 'start': None, 'end': None},\n",
              " {'word': 'f', 'start': None, 'end': None},\n",
              " {'word': '[NRIC_END]', 'start': None, 'end': None},\n",
              " {'word': 'he', 'start': None, 'end': None},\n",
              " {'word': 'say', 'start': None, 'end': None},\n",
              " {'word': 'the', 'start': None, 'end': None},\n",
              " {'word': 'last', 'start': None, 'end': None},\n",
              " {'word': 'letter', 'start': None, 'end': None},\n",
              " {'word': 'f', 'start': None, 'end': None},\n",
              " {'word': 'stands', 'start': None, 'end': None},\n",
              " {'word': 'for', 'start': None, 'end': None},\n",
              " {'word': 'fantastic', 'start': None, 'end': None},\n",
              " {'word': 'sia', 'start': None, 'end': None},\n",
              " {'word': '.', 'start': None, 'end': None}]"
            ]
          },
          "execution_count": 456,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df['aligned_transcript'].iloc[305]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Desmond Tan, his I C [NRIC_START] S6789012F [NRIC_END], he say the last letter F stands for 'fantastic' sia.\""
            ]
          },
          "execution_count": 457,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df['ref_text'].iloc[305]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 458,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'conf': 1.0, 'end': 0.6000000000000001, 'start': 0.03, 'word': 'desmond'},\n",
              " {'conf': 1.0, 'end': 1.35, 'start': 0.6000000000000001, 'word': 'tan'}]"
            ]
          },
          "execution_count": 458,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df['vosk_output'].iloc[305]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 451,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'word': 'i', 'start': 0.45, 'end': 0.63},\n",
              " {'word': 'lost', 'start': 0.63, 'end': 1.02},\n",
              " {'word': 'my', 'start': 1.02, 'end': 1.2},\n",
              " {'word': 'passport', 'start': 1.23, 'end': 1.77},\n",
              " {'word': 'and', 'start': 1.98, 'end': 2.25},\n",
              " {'word': 'it', 'start': 2.31, 'end': 2.49},\n",
              " {'word': 'new', 'start': 2.7600000000000002, 'end': 3.09},\n",
              " {'word': '[CARDINAL_START]', 'start': None, 'end': None},\n",
              " {'word': 'one', 'start': 3.09, 'end': 3.39},\n",
              " {'word': '[CARDINAL_END]', 'start': None, 'end': None},\n",
              " {'word': 'number', 'start': 4.59, 'end': 5.07},\n",
              " {'word': 'is', 'start': 5.1, 'end': 5.46},\n",
              " {'word': '[PASSPORT_NUM_START]', 'start': None, 'end': None},\n",
              " {'word': 'k', 'start': 5.79, 'end': 6.15},\n",
              " {'word': 'seven', 'start': 6.18, 'end': 6.75},\n",
              " {'word': 'six', 'start': 6.84, 'end': 7.29},\n",
              " {'word': 'zero', 'start': 7.29, 'end': 7.83},\n",
              " {'word': 'one', 'start': 7.98, 'end': 8.227933},\n",
              " {'word': 'eight', 'start': 8.25, 'end': 8.49},\n",
              " {'word': 'four', 'start': 8.85, 'end': 9.21},\n",
              " {'word': 'two', 'start': 9.27, 'end': 9.57},\n",
              " {'word': 't', 'start': 9.72, 'end': 10.02},\n",
              " {'word': '[PASSPORT_NUM_END]', 'start': None, 'end': None},\n",
              " {'word': 'and', 'start': 10.5, 'end': 10.8},\n",
              " {'word': '[PASSPORT_NUM_START]', 'start': None, 'end': None},\n",
              " {'word': 'k', 'start': 11.31, 'end': 11.55},\n",
              " {'word': 'seven', 'start': 11.55, 'end': 12.03},\n",
              " {'word': 'six', 'start': 12.27, 'end': 12.66},\n",
              " {'word': 'zero', 'start': 12.699364, 'end': 13.11},\n",
              " {'word': 'one', 'start': 13.32, 'end': 13.62},\n",
              " {'word': 'eight', 'start': 13.68, 'end': 13.95},\n",
              " {'word': 'four', 'start': 14.25, 'end': 14.64},\n",
              " {'word': 'two', 'start': 14.73, 'end': 15.06},\n",
              " {'word': 't', 'start': 15.72, 'end': 16.02},\n",
              " {'word': '[PASSPORT_NUM_END]', 'start': None, 'end': None}]"
            ]
          },
          "execution_count": 451,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df['aligned_transcript'].iloc[84]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 452,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_with_triplets = extract_pii_tuples(processed_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 453,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0     [(1.2, 1.68, DATE), (1.77, 2.04, PERSON), (3.6...\n",
              "1                                  [(3.36, 3.93, DATE)]\n",
              "2                             [(4.56, 6.39, CAR_PLATE)]\n",
              "3        [(3.09, 3.78, CARDINAL), (4.5, 4.8, CARDINAL)]\n",
              "4           [(1.14, 1.65, PERSON), (2.49, 6.63, EMAIL)]\n",
              "5          [(2.01, 2.67, ORG), (3.39, 5.73, CAR_PLATE)]\n",
              "6     [(0.99, 3.93, EMAIL), (4.32, 5.22, ORG), (5.49...\n",
              "7     [(0.96, 5.04, EMAIL), (5.43, 5.82, ORG), (7.14...\n",
              "8     [(5.1, 5.34, CARDINAL), (6.39, 6.571295, CARDI...\n",
              "9           [(0.75, 1.17, PERSON), (2.55, 4.83, PHONE)]\n",
              "10       [(1.8, 2.1, CARDINAL), (3.66, 3.84, CARDINAL)]\n",
              "11              [(2.28, 2.73, DATE), (2.73, 3.9, DATE)]\n",
              "12                             [(0.96, 3.3, CAR_PLATE)]\n",
              "13    [(0.5700000000000001, 5.52, BANK_ACCOUNT), (6....\n",
              "14        [(0.99, 2.1, ORG), (3.45, 6.3, BANK_ACCOUNT)]\n",
              "15                         [(6.78, 9.81, BANK_ACCOUNT)]\n",
              "16    [(0.75, 5.1, CREDIT_CARD), (5.34, 6.21, ORG), ...\n",
              "17             [(3.84, 4.65, DATE), (5.64, 6.12, DATE)]\n",
              "18                                [(5.16, 8.43, PHONE)]\n",
              "19    [(3.12, 3.57, ORG), (5.43, 8.25, PHONE), (8.73...\n",
              "Name: pii_tuples, dtype: object"
            ]
          },
          "execution_count": 453,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_with_triplets['pii_tuples'].head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 454,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "480    [(2.22, 5.31, EMAIL), (5.82, 7.74, EMAIL)]\n",
              "481                         [(3.66, 5.94, EMAIL)]\n",
              "482                         [(1.47, 3.69, EMAIL)]\n",
              "483                         [(2.16, 4.74, EMAIL)]\n",
              "484                         [(2.01, 5.07, EMAIL)]\n",
              "485                           [(2.1, 3.9, EMAIL)]\n",
              "486                         [(2.13, 4.05, EMAIL)]\n",
              "487                         [(2.19, 4.32, EMAIL)]\n",
              "488                         [(2.82, 5.73, EMAIL)]\n",
              "489                         [(2.61, 5.88, EMAIL)]\n",
              "490                         [(1.68, 3.87, EMAIL)]\n",
              "491                         [(2.19, 4.17, EMAIL)]\n",
              "492                         [(1.98, 4.11, EMAIL)]\n",
              "493                         [(1.98, 3.99, EMAIL)]\n",
              "494                     [(2.410002, 5.67, EMAIL)]\n",
              "495                         [(2.22, 4.14, EMAIL)]\n",
              "496                         [(2.61, 4.17, EMAIL)]\n",
              "497                         [(2.16, 4.47, EMAIL)]\n",
              "498                         [(2.79, 5.13, EMAIL)]\n",
              "499                         [(1.86, 5.31, EMAIL)]\n",
              "Name: pii_tuples, dtype: object"
            ]
          },
          "execution_count": 454,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_with_triplets['pii_tuples'].tail(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 455,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_with_triplets['pii_tuples'].to_csv('data/ref_triplets_500.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [Archived]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heuristics Description (Greedy-based Alignment):\n",
        "\n",
        "Forced-alignment heuristics are necessary because:\n",
        "- The **Vosk model** may tokenize words differently compared to the reference transcript, especially for structured data like emails, phone numbers, and other PIIs (Personally Identifiable Information).\n",
        "- **PII structures vary greatly** (e.g., \"rendy.tan@hotmail.com\" vs \"rendy . tan at hotmail dot com\"), and simple word-to-word alignment would fail.\n",
        "- To achieve robust alignment and accurate timestamp mapping, **manual control** over token splitting and flattening is required based on the entity type.\n",
        "\n",
        "These heuristics ensure that:\n",
        "- Common free-text is aligned naturally,\n",
        "- Structured PII is broken down appropriately for correct timestamp boundary matching.\n",
        "\n",
        "#### 1. Outside of Entity Boundaries (General Case)\n",
        "\n",
        "- Tokens are aligned **as-is** with Vosk words.\n",
        "- No special splitting is done.\n",
        "- Regular cleaning (punctuation removal except for `\".\"`) is applied when matching.\n",
        "- **Example**:\n",
        "  - Input Transcript: `\"reach me at\"`\n",
        "  - Tokens: `[\"reach\", \"me\", \"at\"]`\n",
        "  - Aligned directly without splitting.\n",
        "\n",
        "#### 2. Inside Entity Boundaries (e.g., [EMAIL_START], [PHONE_START], etc.)\n",
        "\n",
        "- Special handling is done based on the entity type.\n",
        "\n",
        "##### (A) EMAIL Entity (`current_entity == 'EMAIL'`)\n",
        "\n",
        "- Split tokens based on `\".\"` and `\"@\"` separators.\n",
        "- Words like `\"at\"` are **left intact**.\n",
        "- **Example**:\n",
        "  - Input: `\"rendy.tan@hotmail.com\"`\n",
        "  - Split into: `[\"rendy\", \".\", \"tan\", \"@\", \"hotmail\", \".\", \"com\"]`\n",
        "\n",
        "##### (B) Other Entity Types (`CREDIT_CARD`, `CAR_PLATE`, `BANK_ACCOUNT`, `NRIC`, `PHONE`, `PASSPORT_NUM`)\n",
        "\n",
        "- **If the token is a spelled-out number** (checked against a dictionary):\n",
        "  - **Do not split**; keep the word as a single token.\n",
        "  - **Example**:\n",
        "    - Input: `\"eight\"`\n",
        "    - Output: `[\"eight\"]`\n",
        "\n",
        "- **If the token is pure digits** (e.g., numbers like `\"98005331\"`):\n",
        "  - **Split** into **individual characters**.\n",
        "  - **Example**:\n",
        "    - Input: `\"98005331\"`\n",
        "    - Output: `[\"9\", \"8\", \"0\", \"0\", \"5\", \"3\", \"3\", \"1\"]`\n",
        "\n",
        "- **If the token is a mix of letters and numbers** (e.g., `\"AB1234X\"`):\n",
        "  - **Split** into **individual characters** as well.\n",
        "  - **Example**:\n",
        "    - Input: `\"AB1234X\"`\n",
        "    - Output: `[\"A\", \"B\", \"1\", \"2\", \"3\", \"4\", \"X\"]`\n",
        "\n",
        "#### 3. When Flattening Entity Tokens (Before Final Alignment)\n",
        "\n",
        "- After all splitting:\n",
        "  - **Spelled-out numbers** (like `\"eight\"`) and **email parts** (like `\"hotmail\"`) are **kept whole**.\n",
        "  - Other tokens (numbers, single characters) appear **character-by-character**.\n",
        "\n",
        "- **Flattening Examples**:\n",
        "  - Tokens: `[\"eight\", \"5\", \"0\"]`\n",
        "    - Final output: `\"eight 5 0\"`\n",
        "  \n",
        "  - Tokens: `[\"hotmail\", \".\", \"com\"]`\n",
        "    - Final output: `\"hotmail . com\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import tempfile\n",
        "from pydub import AudioSegment\n",
        "from vosk import Model, KaldiRecognizer\n",
        "model_path = \"vosk-model-en-us-0.42-gigaspeech\" #model_new\"\n",
        "model = Model(model_path)\n",
        "def align_audio_with_text(audio_path, transcription):\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    recognizer = KaldiRecognizer(model, audio.frame_rate)\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_wav:\n",
        "        temp_wav_path = temp_wav.name\n",
        "        audio.export(temp_wav_path, format=\"wav\")\n",
        "    results = []\n",
        "    try:\n",
        "        with open(temp_wav_path, \"rb\") as wf:\n",
        "            wf.read(44)\n",
        "            recognizer.SetWords(True)\n",
        "            while True:\n",
        "                data = wf.read(4000)\n",
        "                if len(data) == 0:\n",
        "                    break\n",
        "                if recognizer.AcceptWaveform(data):\n",
        "                    results.append(json.loads(recognizer.Result()))\n",
        "            results.append(json.loads(recognizer.FinalResult()))\n",
        "    finally:\n",
        "        if os.path.exists(temp_wav_path):\n",
        "            os.remove(temp_wav_path)\n",
        "    words = []\n",
        "    for result in results:\n",
        "        if 'result' in result:\n",
        "            for word in result['result']:\n",
        "                words.append(word)\n",
        "    aligned_segments = []\n",
        "    for word in words:\n",
        "        aligned_segments.append({\n",
        "            \"start\": word[\"start\"],\n",
        "            \"end\": word[\"end\"],\n",
        "            \"word\": word[\"word\"]\n",
        "        })\n",
        "    return aligned_segments\n",
        "\n",
        "audio_dir = \"/content/drive/MyDrive/Share/Research/speechNER/finetune/Audio_Files_for_testing\"\n",
        "transcription_file = \"/content/drive/MyDrive/Share/Research/speechNER/Alignement_data/Text_with_ids_temp_preprocessed.jsonl\"\n",
        "output_file = \"/content/drive/MyDrive/Share/Research/speechNER/Alignement_data/tr_aligned_data_new.jsonl\"\n",
        "with open(transcription_file, 'r') as f:\n",
        "    transcriptions = [json.loads(line) for line in f]\n",
        "aligned_data = []\n",
        "for item in transcriptions:\n",
        "    audio_path = f\"{audio_dir}/id{item['id']}.wav\"\n",
        "    aligned_transcription = align_audio_with_text(audio_path, item['text'])\n",
        "    aligned_data.append({\n",
        "        \"id\": item['id'],\n",
        "        \"text\": item['text'],\n",
        "        \"align\": aligned_transcription\n",
        "    })\n",
        "with open(output_file, 'w') as f:\n",
        "    for item in aligned_data:\n",
        "        f.write(json.dumps(item) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Old vosk alignment function with greedy decoding (Archived)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 682,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "spelled_out_numbers = {\n",
        "    'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5',\n",
        "    'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10',\n",
        "    'eleven': '11', 'twelve': '12', 'thirteen': '13', 'fourteen': '14',\n",
        "    'fifteen': '15', 'sixteen': '16', 'seventeen': '17', 'eighteen': '18',\n",
        "    'nineteen': '19', 'twenty': '20', 'thirty': '30', 'forty': '40', 'fifty': '50',\n",
        "    'sixty': '60', 'seventy': '70', 'eighty': '80', 'ninety': '90',\n",
        "    'hundred': '100', 'thousand': '1000'\n",
        "}\n",
        "\n",
        "def clean_token(token):\n",
        "    \"\"\"Remove punctuation except '.' and lowercase.\"\"\"\n",
        "    allowed = '.'\n",
        "    punctuation_to_remove = ''.join(c for c in string.punctuation if c not in allowed)\n",
        "    return token.lower().translate(str.maketrans('', '', punctuation_to_remove))\n",
        "\n",
        "def process_entity_tokens(entity_tokens, char_tokens):\n",
        "    \"\"\"Prevent duplicates and extend entity tokens list.\"\"\"\n",
        "    for token in char_tokens:\n",
        "        if token not in entity_tokens:\n",
        "            entity_tokens.append(token)\n",
        "\n",
        "def align_transcript_with_vosk(vosk_words, transcript):\n",
        "    \"\"\"\n",
        "    Aligns a reference transcript with Vosk timestamps.\n",
        "    Handles [XXX_START]... [XXX_END] entities properly.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r'\\[.*?\\]|\\S+', transcript)  # Tokenize the transcript\n",
        "    aligned = []\n",
        "    vosk_idx = 0\n",
        "    current_entity = None\n",
        "    entity_tokens = []\n",
        "    entity_start_time = None\n",
        "    entity_end_time = None\n",
        "\n",
        "    entity_types_to_split = ['CREDIT_CARD', 'CAR_PLATE', 'BANK_ACCOUNT', 'NRIC', 'PHONE', 'PASSPORT_NUM']\n",
        "    \n",
        "    # Special case for emails: split on the dots (.) and @ but leave 'at' as-is\n",
        "    def split_email(token):\n",
        "        # Case 1: email with spaces (no @)\n",
        "        if '.' in token:\n",
        "            parts = re.split(r'([.])', token)\n",
        "            parts = [p for p in parts if p != '']\n",
        "            # print(parts)\n",
        "            return parts  # Remove any empty strings\n",
        "        # Case 2: email with @\n",
        "        elif '@' in token:\n",
        "            parts = re.split(r'([@.])', token)\n",
        "            parts = [p for p in parts if p != '']\n",
        "            # print(parts)\n",
        "            return parts\n",
        "        return [token]\n",
        "\n",
        "    i = 0  # Index to keep track of the current token in the list\n",
        "\n",
        "    while i < len(tokens):\n",
        "        token = tokens[i]\n",
        "\n",
        "        # print(f\"Current token: {token}\")\n",
        "\n",
        "        if token.endswith('_START]'):\n",
        "            # Start a new entity\n",
        "            current_entity = token.replace('[', '').replace(']', '').replace('_START', '')\n",
        "            entity_tokens = []\n",
        "            entity_start_time = None\n",
        "            entity_end_time = None\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        if token.endswith('_END]'):\n",
        "            # End the current entity\n",
        "            if current_entity:\n",
        "                # Flatten the entity and align with timestamps\n",
        "                flattened_entity = []\n",
        "                # print(f\"New entity tokens: {entity_tokens}\")\n",
        "                \n",
        "                for t in entity_tokens:\n",
        "                    # Clean the token\n",
        "                    clean_token_with_no_symbols = clean_token(t)\n",
        "                    \n",
        "                    # Check if the token is a spelled-out number\n",
        "                    if clean_token_with_no_symbols.lower() in spelled_out_numbers or current_entity == 'EMAIL':\n",
        "                        # If it's a spelled-out number, don't split it into characters\n",
        "                        flattened_entity.append(clean_token_with_no_symbols)\n",
        "                    else:\n",
        "                        # Otherwise, split the token into characters\n",
        "                        flattened_entity.extend(list(clean_token_with_no_symbols))\n",
        "                    \n",
        "                # Join the characters and align timestamps\n",
        "                aligned.append({\n",
        "                    \"word\": f\"[{current_entity}_START] {' '.join(flattened_entity)} [{current_entity}_END]\",\n",
        "                    \"start\": entity_start_time,\n",
        "                    \"end\": entity_end_time\n",
        "                })\n",
        "            current_entity = None\n",
        "            entity_tokens = []\n",
        "            entity_start_time = None\n",
        "            entity_end_time = None\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        clean_ref_word = clean_token(token)\n",
        "\n",
        "        if current_entity:\n",
        "            # Inside an entity, split the token into characters and modify the tokens list\n",
        "            if vosk_idx < len(vosk_words):\n",
        "                vosk_word = vosk_words[vosk_idx]['word']\n",
        "                if not entity_tokens:\n",
        "                    entity_start_time = vosk_words[vosk_idx]['start']\n",
        "                entity_end_time = vosk_words[vosk_idx]['end']\n",
        "\n",
        "                # Special handling for emails: split valid email format\n",
        "                if current_entity == 'EMAIL':\n",
        "                    char_tokens = split_email(token)  # Split email into parts\n",
        "                    tokens[i:i+1] = char_tokens  # Replace the current token with the split characters\n",
        "\n",
        "                    # Prevent duplicate tokens and extend entity tokens list\n",
        "                    process_entity_tokens(entity_tokens, char_tokens)\n",
        "                    print(f\"Entity tokens after email split: {entity_tokens}\")\n",
        "                # Inside the loop where you handle the token splitting:\n",
        "                elif current_entity in entity_types_to_split:\n",
        "                    clean_token_with_no_symbols = clean_token(token)  # Clean token\n",
        "\n",
        "                    # Check if the token is a spelled-out number\n",
        "                    if clean_token_with_no_symbols.lower() in spelled_out_numbers.keys():\n",
        "                        # If it's a spelled-out number, don't split it\n",
        "                        char_tokens = [clean_token_with_no_symbols]  # Keep the token as is\n",
        "                    else:\n",
        "                        # If it's not a spelled-out number, split it into characters\n",
        "                        char_tokens = list(clean_token_with_no_symbols)\n",
        "\n",
        "                    # Modify the tokens list in place by extending with the character tokens\n",
        "                    tokens[i:i+1] = char_tokens\n",
        "\n",
        "                    # Prevent duplicates and extend entity tokens list\n",
        "                    process_entity_tokens(entity_tokens, char_tokens)\n",
        "\n",
        "                    print(f\"Entity tokens after split: {entity_tokens}\")\n",
        "\n",
        "                vosk_idx += 1\n",
        "            else:\n",
        "                # No more Vosk words left (shouldn't happen usually)\n",
        "                entity_tokens.append(token)\n",
        "\n",
        "        else:\n",
        "            # Outside entity, normal matching\n",
        "            while vosk_idx < len(vosk_words):\n",
        "                clean_vosk_word = clean_token(vosk_words[vosk_idx]['word'])\n",
        "                aligned.append({\n",
        "                    \"word\": token,\n",
        "                    \"start\": vosk_words[vosk_idx]['start'],\n",
        "                    \"end\": vosk_words[vosk_idx]['end']\n",
        "                })\n",
        "                vosk_idx += 1\n",
        "                break\n",
        "\n",
        "        i += 1  # Move to the next token\n",
        "        # print(tokens)\n",
        "\n",
        "    return aligned"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
