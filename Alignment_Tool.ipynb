{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejNwrT-YYssJ"
      },
      "source": [
        "# Forced Alignment with Vosk\n",
        "\n",
        "Originally, we evaluated the tagger's F1-score by simply using indices, which may be too penalising. In order to properly evaluate the performance of our PII identification pipeline, we would need to perform *forced alignment*, which aligns token-level transcripts into their corresponding timestamps in the audio files.\n",
        "\n",
        "For this, we shall be using *Vosk*, a toolkit which offers forced-alignment models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change Directory to Root Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "\n",
        "os.chdir('..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 489,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/farhan/Desktop/Research'"
            ]
          },
          "execution_count": 489,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function for sorting the audio file names by ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 490,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_key(file: str) -> int:\n",
        "    try:\n",
        "        # 3 digit\n",
        "        key = int(file[2:5])\n",
        "    except ValueError:\n",
        "        # 1 digit\n",
        "        if file[3] == '.':\n",
        "            key = int(file[2])\n",
        "        else:\n",
        "            key = int(file[2:4])\n",
        "    return key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function to embed the entities within the transcripts (given a dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 817,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def insert_entity_tags_to_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Inserts entity boundary tags into the 'text' column based on 'entities',\n",
        "    and adds a new column 'tagged_text' with the result.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Must contain 'text' and 'entities' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Same DataFrame with an additional 'tagged_text' column.\n",
        "    \"\"\"\n",
        "    def insert_tags(row):\n",
        "        text = row[\"text\"]\n",
        "        entities = row[\"entities\"]\n",
        "\n",
        "        # Sort entities in reverse order of start index to avoid offset issues\n",
        "        entities_sorted = sorted(entities, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        for start, end, label in entities_sorted:\n",
        "            tag_start = f\"[{label}_START]\"\n",
        "            tag_end = f\"[{label}_END]\"\n",
        "            text = text[:end] + tag_end + text[end:]\n",
        "            text = text[:start] + tag_start + text[start:]\n",
        "        \n",
        "        return text\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"tagged_text\"] = df.apply(insert_tags, axis=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function to unify whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 818,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def unify_whitespace(s):\n",
        "    \"\"\"\n",
        "    - Unify multiple spaces into one space across the text.\n",
        "    - Ensure exactly one space after [XXX_START] and before [XXX_END].\n",
        "    - Ensure one space after [XXX_END] if missing.\n",
        "    - Ensure one space before [XXX_START] if missing.\n",
        "    \"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "\n",
        "    # Step 1: unify all whitespace\n",
        "    s = re.sub(r'\\s+', ' ', s.strip())\n",
        "\n",
        "    # Step 2: ensure space after [XXX_START] and before [XXX_END]\n",
        "    s = re.sub(r'(\\[\\w+_START\\])(\\S)', r'\\1 \\2', s)  # Add space after [START] if missing\n",
        "    s = re.sub(r'(\\S)(\\[\\w+_END\\])', r'\\1 \\2', s)    # Add space before [END] if missing\n",
        "\n",
        "    # Step 3: ensure space after [XXX_END] if missing\n",
        "    s = re.sub(r'(\\[\\w+_END\\])(\\S)', r'\\1 \\2', s)\n",
        "\n",
        "    # Step 4: ensure space before [XXX_START] if missing\n",
        "    s = re.sub(r'(\\S)(\\[\\w+_START\\])', r'\\1 \\2', s)  # Add space before [START] if missing\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function to run Vosk forced-alignment model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 493,
      "metadata": {},
      "outputs": [],
      "source": [
        "from vosk import Model, KaldiRecognizer\n",
        "import soundfile as sf\n",
        "import json\n",
        "\n",
        "def run_vosk(audio_path: str, vosk_model: Model) -> list:\n",
        "    # Load audio\n",
        "    audio_data, sample_rate = sf.read(audio_path)\n",
        "    \n",
        "    # Prepare recognizer\n",
        "    rec = KaldiRecognizer(vosk_model, sample_rate)\n",
        "    rec.SetWords(True)\n",
        "    \n",
        "    if audio_data.ndim > 1:\n",
        "        audio_data = audio_data.mean(axis=1)  # Stereo to mono\n",
        "\n",
        "    pcm_data = (audio_data * 32767).astype(\"int16\").tobytes()\n",
        "\n",
        "    rec.AcceptWaveform(pcm_data)\n",
        "    result = json.loads(rec.FinalResult())\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function for tokenizing and standardizing the reference/hypothesis generated by LLM correction module / Gold data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 856,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "digit_map = {\n",
        "    '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n",
        "    '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'\n",
        "}\n",
        "\n",
        "def tokenize_reference(text):\n",
        "    tokens = []\n",
        "    # Match tags (e.g., [EMAIL_START]) or words with allowed symbols\n",
        "    parts = re.findall(r'\\[.*?\\]|[\\w]+|[@._-]', text)\n",
        "\n",
        "    for part in parts:\n",
        "        if re.fullmatch(r'\\[.*?\\]', part):\n",
        "            tokens.append(part)\n",
        "\n",
        "        elif re.fullmatch(r'\\d{4}-\\d{4}-\\d{4}-\\d{4}', part):\n",
        "            # e.g. credit card numbers\n",
        "            for char in part:\n",
        "                if char in digit_map:\n",
        "                    tokens.append(digit_map[char])\n",
        "\n",
        "        elif re.fullmatch(r'\\d{3}-\\d{5}-\\d', part):\n",
        "            # e.g. bank account numbers\n",
        "            for char in part:\n",
        "                if char in digit_map:\n",
        "                    tokens.append(digit_map[char])\n",
        "\n",
        "        elif part in {'@', '.', '-', '_'}:\n",
        "            tokens.append(part)\n",
        "\n",
        "        else:\n",
        "            clean = part.lower().strip(string.punctuation)\n",
        "            if clean:\n",
        "                tokens.append(clean)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Edit-distance Heuristics\n",
        "\n",
        "1. Equal: Exact match is found. Append the reference token and include the `start` and `end` times.\n",
        "2. Deletion: Found in reference, but not in Vosk. Append the reference, but set `start` and `end` to `None`.\n",
        "3. Insertion: Found in Vosk, but not in reference. Skip vosk word.\n",
        "4. Replace: Word in vosk different to reference. \n",
        "\n",
        "Helper function to align reference Whisper-generated transcript to forced-alignment model (edit-distance based)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 740,
      "metadata": {},
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "import string\n",
        "\n",
        "def clean_token(token):\n",
        "    \"\"\"Lowercase and strip punctuation from a token (except tags).\"\"\"\n",
        "    if token.startswith('[') and token.endswith(']'):\n",
        "        return token.lower()\n",
        "    return token.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def is_start_tag(token):\n",
        "    return token.endswith('_START]')\n",
        "\n",
        "def is_end_tag(token):\n",
        "    return token.endswith('_END]')\n",
        "\n",
        "def align_transcript_with_vosk(vosk_words, transcript):\n",
        "    ref_tokens = tokenize_reference(transcript)\n",
        "    ref_tokens_clean = [clean_token(t) for t in ref_tokens]\n",
        "    vosk_tokens = [w['word'] for w in vosk_words]\n",
        "    vosk_tokens_clean = [clean_token(t) for t in vosk_tokens]\n",
        "\n",
        "    matcher = SequenceMatcher(None, ref_tokens_clean, vosk_tokens_clean)\n",
        "    aligned = []\n",
        "\n",
        "    current_entity = None\n",
        "    entity_buffer = []\n",
        "    entity_start_time = None\n",
        "    entity_end_time = None\n",
        "\n",
        "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
        "        if tag == \"equal\":\n",
        "            for r_i, v_i in zip(range(i1, i2), range(j1, j2)):\n",
        "                token = ref_tokens[r_i]\n",
        "                if is_start_tag(token):\n",
        "                    current_entity = token.replace('[', '').replace(']', '').replace('_START', '')\n",
        "                    entity_buffer = [token]\n",
        "                    entity_start_time = vosk_words[v_i]['start']\n",
        "                elif is_end_tag(token):\n",
        "                    entity_buffer.append(token)\n",
        "                    entity_end_time = vosk_words[v_i - 1]['end']\n",
        "                    aligned.append({\n",
        "                        'word': ' '.join(entity_buffer),\n",
        "                        'start': entity_start_time,\n",
        "                        'end': entity_end_time\n",
        "                    })\n",
        "                    current_entity = None\n",
        "                    entity_buffer = []\n",
        "                elif current_entity:\n",
        "                    entity_buffer.append(token)\n",
        "                else:\n",
        "                    aligned.append({\n",
        "                        'word': token,\n",
        "                        'start': vosk_words[v_i]['start'],\n",
        "                        'end': vosk_words[v_i]['end']\n",
        "                    })\n",
        "\n",
        "        elif tag == \"delete\":\n",
        "            for r_i in range(i1, i2):\n",
        "                aligned.append({\n",
        "                    'word': ref_tokens[r_i],\n",
        "                    'start': None,\n",
        "                    'end': None\n",
        "                })\n",
        "\n",
        "        elif tag == \"replace\":\n",
        "            # Assign start from first Vosk token and end from last Vosk token\n",
        "            vosk_start = vosk_words[j1]['start'] if j1 < len(vosk_words) else None\n",
        "            vosk_end = vosk_words[j2 - 1]['end'] if j2 - 1 < len(vosk_words) else None\n",
        "            for r_i in range(i1, i2):\n",
        "                aligned.append({\n",
        "                    'word': ref_tokens[r_i],\n",
        "                    'start': vosk_start if r_i == i1 else None,\n",
        "                    'end': vosk_end if r_i == i2 - 1 else None\n",
        "                })\n",
        "\n",
        "        elif tag == \"insert\":\n",
        "            continue  # Ignore inserted Vosk words not in reference\n",
        "\n",
        "    return aligned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract id from file name function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 753,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_id_number(filename):\n",
        "    match = re.search(r'id(\\d+)\\.wav', filename)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir('..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load batch 1 (150 samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 820,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The day before [DATE_START] yesterday, [DATE_E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>um my date of birth is uh second [DATE_START] ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>she handed over a crumpled piece of paper, the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aglio olio and err uh [CARDINAL_START] three t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[PERSON_START] Hong's [PERSON_END] email is [E...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  The day before [DATE_START] yesterday, [DATE_E...\n",
              "1  um my date of birth is uh second [DATE_START] ...\n",
              "2  she handed over a crumpled piece of paper, the...\n",
              "3  aglio olio and err uh [CARDINAL_START] three t...\n",
              "4  [PERSON_START] Hong's [PERSON_END] email is [E..."
            ]
          },
          "execution_count": 820,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "batch_one_ref = pd.read_json('data/true_data_150.jsonl', lines=True)\n",
        "batch_one_ref.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 821,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/Audio_Files_for_testing/id1.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/Audio_Files_for_testing/id2.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/Audio_Files_for_testing/id3.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/Audio_Files_for_testing/id4.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/Audio_Files_for_testing/id5.wav</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              file_name\n",
              "0  data/Audio_Files_for_testing/id1.wav\n",
              "1  data/Audio_Files_for_testing/id2.wav\n",
              "2  data/Audio_Files_for_testing/id3.wav\n",
              "3  data/Audio_Files_for_testing/id4.wav\n",
              "4  data/Audio_Files_for_testing/id5.wav"
            ]
          },
          "execution_count": 821,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "batch_one_files = sorted(os.listdir(\"data/Audio_Files_for_testing\"), key=retrieve_key)\n",
        "batch_one_files  = [f'data/Audio_Files_for_testing/{file}' for file in batch_one_files]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "batch_one_df = pd.DataFrame(data=batch_one_files, columns=['file_name'])\n",
        "batch_one_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load batch 2 (350 samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 822,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>entities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151</td>\n",
              "      <td>456 729103 8 is Kaifu Lee's DBS bank account, ...</td>\n",
              "      <td>[[0, 12, BANK_ACCOUNT]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>152</td>\n",
              "      <td>Jacob's OCBC bank account is 192-58462-3, and ...</td>\n",
              "      <td>[[29, 40, BANK_ACCOUNT]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>153</td>\n",
              "      <td>788 305194 2 is Zheng Qi's POSB bank account, ...</td>\n",
              "      <td>[[0, 12, BANK_ACCOUNT]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>154</td>\n",
              "      <td>Geetha's UOB bank account is 341-92741-9, and ...</td>\n",
              "      <td>[[29, 40, BANK_ACCOUNT]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>155</td>\n",
              "      <td>623 481057 6 is Ah Seng's Maybank account, and...</td>\n",
              "      <td>[[0, 12, BANK_ACCOUNT]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id                                               text  \\\n",
              "0  151  456 729103 8 is Kaifu Lee's DBS bank account, ...   \n",
              "1  152  Jacob's OCBC bank account is 192-58462-3, and ...   \n",
              "2  153  788 305194 2 is Zheng Qi's POSB bank account, ...   \n",
              "3  154  Geetha's UOB bank account is 341-92741-9, and ...   \n",
              "4  155  623 481057 6 is Ah Seng's Maybank account, and...   \n",
              "\n",
              "                   entities  \n",
              "0   [[0, 12, BANK_ACCOUNT]]  \n",
              "1  [[29, 40, BANK_ACCOUNT]]  \n",
              "2   [[0, 12, BANK_ACCOUNT]]  \n",
              "3  [[29, 40, BANK_ACCOUNT]]  \n",
              "4   [[0, 12, BANK_ACCOUNT]]  "
            ]
          },
          "execution_count": 822,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "batch_two_ref = pd.read_json('data/newtest_151_500_updated_TTS.jsonl', lines=True)\n",
        "batch_two_ref.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 823,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id151.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id152.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id153.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id154.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id155.wav</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    file_name\n",
              "0  data/newtest_151_500_updated_TTS/id151.wav\n",
              "1  data/newtest_151_500_updated_TTS/id152.wav\n",
              "2  data/newtest_151_500_updated_TTS/id153.wav\n",
              "3  data/newtest_151_500_updated_TTS/id154.wav\n",
              "4  data/newtest_151_500_updated_TTS/id155.wav"
            ]
          },
          "execution_count": 823,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "batch_two_files = sorted(os.listdir(\"data/newtest_151_500_updated_TTS\"), key=retrieve_key)\n",
        "batch_two_files  = [f'data/newtest_151_500_updated_TTS/{file}' for file in batch_two_files]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "batch_two_df = pd.DataFrame(data=batch_two_files, columns=['file_name'])\n",
        "batch_two_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, for batch 2, the entities enclosed are not in the reference transcripts. As we are given the indices, we can write a helper function to include them within the transcripts. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocess transcripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 824,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_two_ref = insert_entity_tags_to_df(batch_two_ref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 825,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>entities</th>\n",
              "      <th>tagged_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151</td>\n",
              "      <td>456 729103 8 is Kaifu Lee's DBS bank account, ...</td>\n",
              "      <td>[[0, 12, BANK_ACCOUNT]]</td>\n",
              "      <td>[BANK_ACCOUNT_START]456 729103 8[BANK_ACCOUNT_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>152</td>\n",
              "      <td>Jacob's OCBC bank account is 192-58462-3, and ...</td>\n",
              "      <td>[[29, 40, BANK_ACCOUNT]]</td>\n",
              "      <td>Jacob's OCBC bank account is [BANK_ACCOUNT_STA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>153</td>\n",
              "      <td>788 305194 2 is Zheng Qi's POSB bank account, ...</td>\n",
              "      <td>[[0, 12, BANK_ACCOUNT]]</td>\n",
              "      <td>[BANK_ACCOUNT_START]788 305194 2[BANK_ACCOUNT_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>154</td>\n",
              "      <td>Geetha's UOB bank account is 341-92741-9, and ...</td>\n",
              "      <td>[[29, 40, BANK_ACCOUNT]]</td>\n",
              "      <td>Geetha's UOB bank account is [BANK_ACCOUNT_STA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>155</td>\n",
              "      <td>623 481057 6 is Ah Seng's Maybank account, and...</td>\n",
              "      <td>[[0, 12, BANK_ACCOUNT]]</td>\n",
              "      <td>[BANK_ACCOUNT_START]623 481057 6[BANK_ACCOUNT_...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id                                               text  \\\n",
              "0  151  456 729103 8 is Kaifu Lee's DBS bank account, ...   \n",
              "1  152  Jacob's OCBC bank account is 192-58462-3, and ...   \n",
              "2  153  788 305194 2 is Zheng Qi's POSB bank account, ...   \n",
              "3  154  Geetha's UOB bank account is 341-92741-9, and ...   \n",
              "4  155  623 481057 6 is Ah Seng's Maybank account, and...   \n",
              "\n",
              "                   entities                                        tagged_text  \n",
              "0   [[0, 12, BANK_ACCOUNT]]  [BANK_ACCOUNT_START]456 729103 8[BANK_ACCOUNT_...  \n",
              "1  [[29, 40, BANK_ACCOUNT]]  Jacob's OCBC bank account is [BANK_ACCOUNT_STA...  \n",
              "2   [[0, 12, BANK_ACCOUNT]]  [BANK_ACCOUNT_START]788 305194 2[BANK_ACCOUNT_...  \n",
              "3  [[29, 40, BANK_ACCOUNT]]  Geetha's UOB bank account is [BANK_ACCOUNT_STA...  \n",
              "4   [[0, 12, BANK_ACCOUNT]]  [BANK_ACCOUNT_START]623 481057 6[BANK_ACCOUNT_...  "
            ]
          },
          "execution_count": 825,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_two_ref.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 826,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>entities</th>\n",
              "      <th>tagged_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>496</td>\n",
              "      <td>Patrick Loh boasting about his email patrick.l...</td>\n",
              "      <td>[[37, 60, EMAIL]]</td>\n",
              "      <td>Patrick Loh boasting about his email [EMAIL_ST...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>497</td>\n",
              "      <td>Jasmine Yeo got sian when someone spell her em...</td>\n",
              "      <td>[[50, 69, EMAIL]]</td>\n",
              "      <td>Jasmine Yeo got sian when someone spell her em...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>498</td>\n",
              "      <td>Bobby Tan write his email bobby.tan@gmail.com ...</td>\n",
              "      <td>[[26, 45, EMAIL]]</td>\n",
              "      <td>Bobby Tan write his email [EMAIL_START]bobby.t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>499</td>\n",
              "      <td>Kamala Singh telling the IT guy her email kama...</td>\n",
              "      <td>[[42, 60, EMAIL]]</td>\n",
              "      <td>Kamala Singh telling the IT guy her email [EMA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>500</td>\n",
              "      <td>Raymond Koh say his email raymond.k@singnet.co...</td>\n",
              "      <td>[[26, 50, EMAIL]]</td>\n",
              "      <td>Raymond Koh say his email [EMAIL_START]raymond...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                               text  \\\n",
              "346  496  Patrick Loh boasting about his email patrick.l...   \n",
              "347  497  Jasmine Yeo got sian when someone spell her em...   \n",
              "348  498  Bobby Tan write his email bobby.tan@gmail.com ...   \n",
              "349  499  Kamala Singh telling the IT guy her email kama...   \n",
              "350  500  Raymond Koh say his email raymond.k@singnet.co...   \n",
              "\n",
              "              entities                                        tagged_text  \n",
              "346  [[37, 60, EMAIL]]  Patrick Loh boasting about his email [EMAIL_ST...  \n",
              "347  [[50, 69, EMAIL]]  Jasmine Yeo got sian when someone spell her em...  \n",
              "348  [[26, 45, EMAIL]]  Bobby Tan write his email [EMAIL_START]bobby.t...  \n",
              "349  [[42, 60, EMAIL]]  Kamala Singh telling the IT guy her email [EMA...  \n",
              "350  [[26, 50, EMAIL]]  Raymond Koh say his email [EMAIL_START]raymond...  "
            ]
          },
          "execution_count": 826,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_two_ref.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 827,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[BANK_ACCOUNT_START]456 729103 8[BANK_ACCOUNT_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jacob's OCBC bank account is [BANK_ACCOUNT_STA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[BANK_ACCOUNT_START]788 305194 2[BANK_ACCOUNT_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Geetha's UOB bank account is [BANK_ACCOUNT_STA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[BANK_ACCOUNT_START]623 481057 6[BANK_ACCOUNT_...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  [BANK_ACCOUNT_START]456 729103 8[BANK_ACCOUNT_...\n",
              "1  Jacob's OCBC bank account is [BANK_ACCOUNT_STA...\n",
              "2  [BANK_ACCOUNT_START]788 305194 2[BANK_ACCOUNT_...\n",
              "3  Geetha's UOB bank account is [BANK_ACCOUNT_STA...\n",
              "4  [BANK_ACCOUNT_START]623 481057 6[BANK_ACCOUNT_..."
            ]
          },
          "execution_count": 827,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_two_ref = batch_two_ref.drop(columns=['entities', 'text', 'id'], axis=1)\n",
        "batch_two_ref.rename(columns={'tagged_text': 'text'}, inplace=True)\n",
        "batch_two_ref.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Combine the datasets [Run this when dataset not combined yet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 828,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_ref = pd.concat([batch_one_ref, batch_two_ref], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 829,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The day before [DATE_START] yesterday, [DATE_E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>um my date of birth is uh second [DATE_START] ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>she handed over a crumpled piece of paper, the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aglio olio and err uh [CARDINAL_START] three t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[PERSON_START] Hong's [PERSON_END] email is [E...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  The day before [DATE_START] yesterday, [DATE_E...\n",
              "1  um my date of birth is uh second [DATE_START] ...\n",
              "2  she handed over a crumpled piece of paper, the...\n",
              "3  aglio olio and err uh [CARDINAL_START] three t...\n",
              "4  [PERSON_START] Hong's [PERSON_END] email is [E..."
            ]
          },
          "execution_count": 829,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 830,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>Patrick Loh boasting about his email [EMAIL_ST...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>Jasmine Yeo got sian when someone spell her em...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>Bobby Tan write his email [EMAIL_START]bobby.t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>Kamala Singh telling the IT guy her email [EMA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>Raymond Koh say his email [EMAIL_START]raymond...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text\n",
              "496  Patrick Loh boasting about his email [EMAIL_ST...\n",
              "497  Jasmine Yeo got sian when someone spell her em...\n",
              "498  Bobby Tan write his email [EMAIL_START]bobby.t...\n",
              "499  Kamala Singh telling the IT guy her email [EMA...\n",
              "500  Raymond Koh say his email [EMAIL_START]raymond..."
            ]
          },
          "execution_count": 830,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 831,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_ref['text'] = test_set_ref['text'].apply(unify_whitespace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_ref.to_json('data/test_set_ref_combined.jsonl', lines=True, orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the combined processed dataset [When already combined]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 837,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_ref = pd.read_json('data/test_set_ref_all.jsonl', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 838,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The day before [DATE_START] yesterday, [DATE_E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>um my date of birth is uh second [DATE_START] ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>she handed over a crumpled piece of paper, the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aglio olio and err uh [CARDINAL_START] three t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[PERSON_START] Hong's [PERSON_END] email is [E...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  The day before [DATE_START] yesterday, [DATE_E...\n",
              "1  um my date of birth is uh second [DATE_START] ...\n",
              "2  she handed over a crumpled piece of paper, the...\n",
              "3  aglio olio and err uh [CARDINAL_START] three t...\n",
              "4  [PERSON_START] Hong's [PERSON_END] email is [E..."
            ]
          },
          "execution_count": 838,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 839,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>Patrick Loh boasting about his email [EMAIL_ST...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>Jasmine Yeo got sian when someone spell her em...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>Bobby Tan write his email [EMAIL_START] bobby....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>Kamala Singh telling the IT guy her email [EMA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>Raymond Koh say his email [EMAIL_START] raymon...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text\n",
              "495  Patrick Loh boasting about his email [EMAIL_ST...\n",
              "496  Jasmine Yeo got sian when someone spell her em...\n",
              "497  Bobby Tan write his email [EMAIL_START] bobby....\n",
              "498  Kamala Singh telling the IT guy her email [EMA...\n",
              "499  Raymond Koh say his email [EMAIL_START] raymon..."
            ]
          },
          "execution_count": 839,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the model (Vosk)\n",
        "\n",
        "Unfortunately, there are no current models that are tuned for Singaporean English (Singlish). As such, we shall use the `vosk-model-en-us-0.42-gigaspeech` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 498,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=8\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
            "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
            "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from models/vosk-model-en-us-0.42-gigaspeech/ivector/final.ie\n",
            "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
            "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from models/vosk-model-en-us-0.42-gigaspeech/graph/HCLG.fst\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from models/vosk-model-en-us-0.42-gigaspeech/graph/words.txt\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo models/vosk-model-en-us-0.42-gigaspeech/graph/phones/word_boundary.int\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from models/vosk-model-en-us-0.42-gigaspeech/rescore/G.fst\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from models/vosk-model-en-us-0.42-gigaspeech/rescore/G.carpa\n",
            "LOG (VoskAPI:ReadDataFiles():model.cc:318) Loading RNNLM model from models/vosk-model-en-us-0.42-gigaspeech/rnnlm/final.raw\n"
          ]
        }
      ],
      "source": [
        "from vosk import Model, KaldiRecognizer\n",
        "import soundfile as sf\n",
        "import json\n",
        "\n",
        "# Load model (replace path with your model directory)\n",
        "model_path = \"models/vosk-model-en-us-0.42-gigaspeech\"\n",
        "model = Model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Read Audio (With just one sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So here, several things are happening:\n",
        "\n",
        "1. We create a `KaldiRecognizer` instance and set `.SetWords()` to `True`, which means that we will get word-level timestamps.\n",
        "2. The `.AcceptWaveform()` method is used to process the waveform\n",
        "3. The `.FinalResult()` method is finally called to retrieve the word-level timestamps (transcribed from the Vosk Model - although with some innacurracies, as Vosk is not a full-fledged ASR model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test on one audio sample (Simple example with Name and Email PIIs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 707,
      "metadata": {},
      "outputs": [],
      "source": [
        "audio_path = \"data/Audio_Files_for_testing/id48.wav\"\n",
        "audio_data, sample_rate = sf.read(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 708,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = run_vosk(audio_path, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 709,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'result': [{'conf': 1.0, 'end': 0.9, 'start': 0.51, 'word': 'but'},\n",
              "  {'conf': 1.0, 'end': 1.83, 'start': 1.5, 'word': 'each'},\n",
              "  {'conf': 1.0, 'end': 2.28, 'start': 1.86, 'word': 'time'},\n",
              "  {'conf': 1.0, 'end': 2.79, 'start': 2.43, 'word': 'the'},\n",
              "  {'conf': 1.0, 'end': 3.06, 'start': 2.91, 'word': 'the'},\n",
              "  {'conf': 1.0, 'end': 3.48, 'start': 3.09, 'word': 'guest'},\n",
              "  {'conf': 1.0, 'end': 3.99, 'start': 3.48, 'word': 'inside'},\n",
              "  {'conf': 1.0, 'end': 4.38, 'start': 3.99, 'word': 'rate'},\n",
              "  {'conf': 1.0, 'end': 4.83, 'start': 4.59, 'word': 'they'},\n",
              "  {'conf': 1.0, 'end': 5.07, 'start': 4.86, 'word': 'if'},\n",
              "  {'conf': 0.642219, 'end': 5.25, 'start': 5.07, 'word': \"they're\"},\n",
              "  {'conf': 1.0, 'end': 5.49, 'start': 5.25, 'word': 'not'},\n",
              "  {'conf': 1.0, 'end': 5.700046, 'start': 5.52, 'word': 'doing'},\n",
              "  {'conf': 0.996174, 'end': 5.81997, 'start': 5.700046, 'word': 'the'},\n",
              "  {'conf': 0.897181, 'end': 6.21, 'start': 5.81997, 'word': 'running'},\n",
              "  {'conf': 0.60384, 'end': 6.57, 'start': 6.24, 'word': 'track'},\n",
              "  {'conf': 0.60384, 'end': 6.87, 'start': 6.57, 'word': 'thing'},\n",
              "  {'conf': 1.0, 'end': 7.29, 'start': 7.05, 'word': 'they'},\n",
              "  {'conf': 1.0, 'end': 7.59, 'start': 7.29, 'word': 'are'},\n",
              "  {'conf': 1.0, 'end': 8.43, 'start': 8.25, 'word': 'they'},\n",
              "  {'conf': 1.0, 'end': 8.64, 'start': 8.43, 'word': 'need'},\n",
              "  {'conf': 1.0, 'end': 8.76, 'start': 8.64, 'word': 'to'},\n",
              "  {'conf': 1.0, 'end': 9.047345, 'start': 8.76, 'word': 'wear'},\n",
              "  {'conf': 0.494983, 'end': 9.470869, 'start': 9.06, 'word': 'masks'},\n",
              "  {'conf': 0.505017, 'end': 9.69, 'start': 9.470869, 'word': 'then'},\n",
              "  {'conf': 1.0, 'end': 10.53, 'start': 10.23, 'word': 'each'},\n",
              "  {'conf': 1.0, 'end': 10.77, 'start': 10.53, 'word': 'time'},\n",
              "  {'conf': 1.0, 'end': 10.92, 'start': 10.8, 'word': 'we'},\n",
              "  {'conf': 1.0, 'end': 11.04, 'start': 10.92, 'word': 'are'},\n",
              "  {'conf': 1.0, 'end': 11.31, 'start': 11.04, 'word': 'able'},\n",
              "  {'conf': 1.0, 'end': 11.43, 'start': 11.31, 'word': 'to'},\n",
              "  {'conf': 1.0, 'end': 11.73, 'start': 11.43, 'word': 'allow'},\n",
              "  {'conf': 0.827304, 'end': 11.902243, 'start': 11.76, 'word': 'for'},\n",
              "  {'conf': 1.0, 'end': 12.33, 'start': 11.902243, 'word': 'fifteen'},\n",
              "  {'conf': 1.0, 'end': 12.84, 'start': 12.33, 'word': 'guests'},\n",
              "  {'conf': 1.0, 'end': 13.35, 'start': 12.87, 'word': 'inside'},\n",
              "  {'conf': 1.0, 'end': 14.43, 'start': 14.1, 'word': 'here'},\n",
              "  {'conf': 1.0, 'end': 14.58, 'start': 14.43, 'word': 'are'},\n",
              "  {'conf': 1.0, 'end': 14.76, 'start': 14.58, 'word': 'my'},\n",
              "  {'conf': 1.0, 'end': 15.12, 'start': 14.79, 'word': 'bank'},\n",
              "  {'conf': 1.0, 'end': 15.45, 'start': 15.12, 'word': 'account'},\n",
              "  {'conf': 1.0, 'end': 15.99, 'start': 15.45, 'word': 'details'},\n",
              "  {'conf': 1.0, 'end': 16.71, 'start': 16.23, 'word': 'seven'},\n",
              "  {'conf': 1.0, 'end': 17.46, 'start': 16.71, 'word': 'seventy'},\n",
              "  {'conf': 1.0, 'end': 18.09, 'start': 17.82, 'word': 'three'},\n",
              "  {'conf': 1.0, 'end': 18.57, 'start': 18.09, 'word': 'seven'},\n",
              "  {'conf': 1.0, 'end': 18.93, 'start': 18.66, 'word': 'eight'},\n",
              "  {'conf': 1.0, 'end': 19.17, 'start': 18.93, 'word': 'zero'},\n",
              "  {'conf': 1.0, 'end': 19.59, 'start': 19.17, 'word': 'seven'},\n",
              "  {'conf': 1.0, 'end': 20.1, 'start': 19.71, 'word': 'seven'}],\n",
              " 'text': \"but each time the the guest inside rate they if they're not doing the running track thing they are they need to wear masks then each time we are able to allow for fifteen guests inside here are my bank account details seven seventy three seven eight zero seven seven\"}"
            ]
          },
          "execution_count": 709,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 713,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 713,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sample['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 717,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['but', 'each', 'time', 'the', 'the', 'guest', 'inside', 'rate', 'they', 'if', \"they're\", 'not', 'doing', 'the', 'running', 'track', 'thing', 'they', 'are', 'they', 'need', 'to', 'wear', 'masks', 'then', 'each', 'time', 'we', 'are', 'able', 'to', 'allow', 'for', 'fifteen', 'guests', 'inside', 'here', 'are', 'my', 'bank', 'account', 'details', 'seven', 'seventy', 'three', 'seven', 'eight', 'zero', 'seven', 'seven']\n"
          ]
        }
      ],
      "source": [
        "vosk_output = []\n",
        "\n",
        "for item in sample['result']:\n",
        "    vosk_output.append(item['word'])\n",
        "\n",
        "print(vosk_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reference Transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 710,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'but uh each time the the guest inside right they if they are not doing the running track right they are they need to wear mask then uh each time we are able to allow for [CARDINAL_START] fifteen [CARDINAL_END] guests inside\"Here are my bank account details: [BANK_ACCOUNT_START] 773-3780-7-7 [BANK_ACCOUNT_END]'"
            ]
          },
          "execution_count": 710,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref['text'].iloc[47]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 712,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens: 58\n",
            "['but', 'uh', 'each', 'time', 'the', 'the', 'guest', 'inside', 'right', 'they', 'if', 'they', 'are', 'not', 'doing', 'the', 'running', 'track', 'right', 'they', 'are', 'they', 'need', 'to', 'wear', 'mask', 'then', 'uh', 'each', 'time', 'we', 'are', 'able', 'to', 'allow', 'for', '[CARDINAL_START]', 'fifteen', '[CARDINAL_END]', 'guests', 'inside', 'here', 'are', 'my', 'bank', 'account', 'details', '[BANK_ACCOUNT_START]', 'seven', 'seven', 'three', 'three', 'seven', 'eight', 'zero', 'seven', 'seven', '[BANK_ACCOUNT_END]']\n"
          ]
        }
      ],
      "source": [
        "tokenized_reference = tokenize_reference(test_set_ref['text'].iloc[47])\n",
        "\n",
        "print(\"Number of tokens:\", len(tokenized_reference))\n",
        "print(tokenized_reference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using `SequenceMatcher` by `difflib` to perform edit-distance based heuristics\n",
        "\n",
        "**Limitations of Greedy-alignment**\n",
        "\n",
        "As we observed, greedy-based approaches are not feasible due to the variability of the dataset plus the difference in output format between the Kaldi alignment model and the reference text. For example, some words in the reference text include fillers like \"uh\" whereas the Vosk model seem to omit it. Another example include the handling of contractions - with the Vosk model outputting contractions as-is (considered as one token) whereas the reference text separates the words (e.g., they're -> they are, considered as two tokens). Lastly, Kaldi is designed foremost as a forced-alignment model and not a speech-recognition model - which means that it often outputs mistranscriptions (e.g., seventy vs seven three). This can have severe impact when it comes to alignment quality. Therefore, it has since been decided that greedy-alignment is not sufficient for our use case.\n",
        "\n",
        "**Edit Distance Heuristics with `SequenceMatcher`**\n",
        "\n",
        "Edit-distance alignment approaches, such as those based on `SequenceMatcher`, offer a more robust alternative to greedy heuristics in aligning reference transcripts with forced alignment outputs. Unlike greedy methods that proceed sequentially and can easily desynchronize when encountering mismatches, edit-distance techniques globally evaluate the sequences and can better tolerate insertions, deletions, or substitutions. For example, in the reference transcript, the phrase \"they are\" may appear as two tokens, while Vosk outputs \"they're\" as a single token—this would break a greedy matcher, but an edit-distance algorithm would register this as a \"replace\" operation and continue aligning the rest correctly. Similarly, reference text may contain filler words like \"uh\" or structured tag blocks like `[EMAIL_START] foo bar [EMAIL_END]` that do not exist in the Vosk output. Edit-distance allows these to be treated as deletions while maintaining alignment integrity. Lastly, in numerical sequences, Vosk may transcribe \"seven three\" as \"seventy\", which would otherwise cause greedy approaches to misalign all following tokens. With edit-distance, such errors are localized and do not compromise the entire alignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 719,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "equal      ref[0:1] -> vosk[0:1]\n",
            "  REF : ['but']\n",
            "  VOSK: ['but']\n",
            "delete     ref[1:2] -> vosk[1:1]\n",
            "  REF : ['uh']\n",
            "  VOSK: []\n",
            "equal      ref[2:8] -> vosk[1:7]\n",
            "  REF : ['each', 'time', 'the', 'the', 'guest', 'inside']\n",
            "  VOSK: ['each', 'time', 'the', 'the', 'guest', 'inside']\n",
            "replace    ref[8:9] -> vosk[7:8]\n",
            "  REF : ['right']\n",
            "  VOSK: ['rate']\n",
            "equal      ref[9:11] -> vosk[8:10]\n",
            "  REF : ['they', 'if']\n",
            "  VOSK: ['they', 'if']\n",
            "replace    ref[11:13] -> vosk[10:11]\n",
            "  REF : ['they', 'are']\n",
            "  VOSK: [\"they're\"]\n",
            "equal      ref[13:18] -> vosk[11:16]\n",
            "  REF : ['not', 'doing', 'the', 'running', 'track']\n",
            "  VOSK: ['not', 'doing', 'the', 'running', 'track']\n",
            "replace    ref[18:19] -> vosk[16:17]\n",
            "  REF : ['right']\n",
            "  VOSK: ['thing']\n",
            "equal      ref[19:25] -> vosk[17:23]\n",
            "  REF : ['they', 'are', 'they', 'need', 'to', 'wear']\n",
            "  VOSK: ['they', 'are', 'they', 'need', 'to', 'wear']\n",
            "replace    ref[25:26] -> vosk[23:24]\n",
            "  REF : ['mask']\n",
            "  VOSK: ['masks']\n",
            "equal      ref[26:27] -> vosk[24:25]\n",
            "  REF : ['then']\n",
            "  VOSK: ['then']\n",
            "delete     ref[27:28] -> vosk[25:25]\n",
            "  REF : ['uh']\n",
            "  VOSK: []\n",
            "equal      ref[28:36] -> vosk[25:33]\n",
            "  REF : ['each', 'time', 'we', 'are', 'able', 'to', 'allow', 'for']\n",
            "  VOSK: ['each', 'time', 'we', 'are', 'able', 'to', 'allow', 'for']\n",
            "delete     ref[36:37] -> vosk[33:33]\n",
            "  REF : ['[CARDINAL_START]']\n",
            "  VOSK: []\n",
            "equal      ref[37:38] -> vosk[33:34]\n",
            "  REF : ['fifteen']\n",
            "  VOSK: ['fifteen']\n",
            "delete     ref[38:39] -> vosk[34:34]\n",
            "  REF : ['[CARDINAL_END]']\n",
            "  VOSK: []\n",
            "equal      ref[39:47] -> vosk[34:42]\n",
            "  REF : ['guests', 'inside', 'here', 'are', 'my', 'bank', 'account', 'details']\n",
            "  VOSK: ['guests', 'inside', 'here', 'are', 'my', 'bank', 'account', 'details']\n",
            "delete     ref[47:48] -> vosk[42:42]\n",
            "  REF : ['[BANK_ACCOUNT_START]']\n",
            "  VOSK: []\n",
            "equal      ref[48:49] -> vosk[42:43]\n",
            "  REF : ['seven']\n",
            "  VOSK: ['seven']\n",
            "replace    ref[49:51] -> vosk[43:44]\n",
            "  REF : ['seven', 'three']\n",
            "  VOSK: ['seventy']\n",
            "equal      ref[51:57] -> vosk[44:50]\n",
            "  REF : ['three', 'seven', 'eight', 'zero', 'seven', 'seven']\n",
            "  VOSK: ['three', 'seven', 'eight', 'zero', 'seven', 'seven']\n",
            "delete     ref[57:58] -> vosk[50:50]\n",
            "  REF : ['[BANK_ACCOUNT_END]']\n",
            "  VOSK: []\n"
          ]
        }
      ],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "ref = tokenize_reference(test_set_ref['text'].iloc[47])\n",
        "\n",
        "vosk = ['but', 'each', 'time', 'the', 'the', 'guest', 'inside', 'rate', 'they', 'if', \"they're\", 'not', 'doing', 'the', 'running', 'track', 'thing', 'they', 'are', 'they', 'need', 'to', 'wear', 'masks', 'then', 'each', 'time', 'we', 'are', 'able', 'to', 'allow', 'for', 'fifteen', 'guests', 'inside', 'here', 'are', 'my', 'bank', 'account', 'details', 'seven', 'seventy', 'three', 'seven', 'eight', 'zero', 'seven', 'seven']\n",
        "\n",
        "matcher = SequenceMatcher(None, ref, vosk)\n",
        "for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
        "    print(f\"{tag:10} ref[{i1}:{i2}] -> vosk[{j1}:{j2}]\")\n",
        "    print(f\"  REF : {ref[i1:i2]}\")\n",
        "    print(f\"  VOSK: {vosk[j1:j2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Align vosk output with reference (edit-distance based approach)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 741,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'word': 'but', 'start': 0.51, 'end': 0.9},\n",
              " {'word': 'uh', 'start': None, 'end': None},\n",
              " {'word': 'each', 'start': 1.5, 'end': 1.83},\n",
              " {'word': 'time', 'start': 1.86, 'end': 2.28},\n",
              " {'word': 'the', 'start': 2.43, 'end': 2.79},\n",
              " {'word': 'the', 'start': 2.91, 'end': 3.06},\n",
              " {'word': 'guest', 'start': 3.09, 'end': 3.48},\n",
              " {'word': 'inside', 'start': 3.48, 'end': 3.99},\n",
              " {'word': 'right', 'start': 3.99, 'end': 4.38},\n",
              " {'word': 'they', 'start': 4.59, 'end': 4.83},\n",
              " {'word': 'if', 'start': 4.86, 'end': 5.07},\n",
              " {'word': 'they', 'start': 5.07, 'end': None},\n",
              " {'word': 'are', 'start': None, 'end': 5.25},\n",
              " {'word': 'not', 'start': 5.25, 'end': 5.49},\n",
              " {'word': 'doing', 'start': 5.52, 'end': 5.700046},\n",
              " {'word': 'the', 'start': 5.700046, 'end': 5.81997},\n",
              " {'word': 'running', 'start': 5.81997, 'end': 6.21},\n",
              " {'word': 'track', 'start': 6.24, 'end': 6.57},\n",
              " {'word': 'right', 'start': 6.57, 'end': 6.87},\n",
              " {'word': 'they', 'start': 7.05, 'end': 7.29},\n",
              " {'word': 'are', 'start': 7.29, 'end': 7.59},\n",
              " {'word': 'they', 'start': 8.25, 'end': 8.43},\n",
              " {'word': 'need', 'start': 8.43, 'end': 8.64},\n",
              " {'word': 'to', 'start': 8.64, 'end': 8.76},\n",
              " {'word': 'wear', 'start': 8.76, 'end': 9.047345},\n",
              " {'word': 'mask', 'start': 9.06, 'end': 9.470869},\n",
              " {'word': 'then', 'start': 9.470869, 'end': 9.69},\n",
              " {'word': 'uh', 'start': None, 'end': None},\n",
              " {'word': 'each', 'start': 10.23, 'end': 10.53},\n",
              " {'word': 'time', 'start': 10.53, 'end': 10.77},\n",
              " {'word': 'we', 'start': 10.8, 'end': 10.92},\n",
              " {'word': 'are', 'start': 10.92, 'end': 11.04},\n",
              " {'word': 'able', 'start': 11.04, 'end': 11.31},\n",
              " {'word': 'to', 'start': 11.31, 'end': 11.43},\n",
              " {'word': 'allow', 'start': 11.43, 'end': 11.73},\n",
              " {'word': 'for', 'start': 11.76, 'end': 11.902243},\n",
              " {'word': '[CARDINAL_START]', 'start': None, 'end': None},\n",
              " {'word': 'fifteen', 'start': 11.902243, 'end': 12.33},\n",
              " {'word': '[CARDINAL_END]', 'start': None, 'end': None},\n",
              " {'word': 'guests', 'start': 12.33, 'end': 12.84},\n",
              " {'word': 'inside', 'start': 12.87, 'end': 13.35},\n",
              " {'word': 'here', 'start': 14.1, 'end': 14.43},\n",
              " {'word': 'are', 'start': 14.43, 'end': 14.58},\n",
              " {'word': 'my', 'start': 14.58, 'end': 14.76},\n",
              " {'word': 'bank', 'start': 14.79, 'end': 15.12},\n",
              " {'word': 'account', 'start': 15.12, 'end': 15.45},\n",
              " {'word': 'details', 'start': 15.45, 'end': 15.99},\n",
              " {'word': '[BANK_ACCOUNT_START]', 'start': None, 'end': None},\n",
              " {'word': 'seven', 'start': 16.23, 'end': 16.71},\n",
              " {'word': 'seven', 'start': 16.71, 'end': None},\n",
              " {'word': 'three', 'start': None, 'end': 17.46},\n",
              " {'word': 'three', 'start': 17.82, 'end': 18.09},\n",
              " {'word': 'seven', 'start': 18.09, 'end': 18.57},\n",
              " {'word': 'eight', 'start': 18.66, 'end': 18.93},\n",
              " {'word': 'zero', 'start': 18.93, 'end': 19.17},\n",
              " {'word': 'seven', 'start': 19.17, 'end': 19.59},\n",
              " {'word': 'seven', 'start': 19.71, 'end': 20.1},\n",
              " {'word': '[BANK_ACCOUNT_END]', 'start': None, 'end': None}]"
            ]
          },
          "execution_count": 741,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "align_transcript_with_vosk(sample['result'], test_set_ref['text'].iloc[47])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test on one audio sample (Numerical PIIs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 742,
      "metadata": {},
      "outputs": [],
      "source": [
        "audio_path = \"data/Audio_Files_for_testing/id52.wav\"\n",
        "audio_data, sample_rate = sf.read(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 743,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample2 = run_vosk(audio_path, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 744,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'result': [{'conf': 0.634362, 'end': 1.14, 'start': 0.66, 'word': 'ok'},\n",
              "  {'conf': 0.287375, 'end': 1.8, 'start': 1.5, 'word': 'ah'},\n",
              "  {'conf': 1.0, 'end': 2.55, 'start': 2.01, 'word': 'contact'},\n",
              "  {'conf': 1.0, 'end': 3.0, 'start': 2.55, 'word': 'number'},\n",
              "  {'conf': 1.0, 'end': 3.48, 'start': 3.12, 'word': 'just'},\n",
              "  {'conf': 1.0, 'end': 3.75, 'start': 3.48, 'word': 'put'},\n",
              "  {'conf': 1.0, 'end': 4.38, 'start': 3.93, 'word': 'nine'},\n",
              "  {'conf': 1.0, 'end': 4.71, 'start': 4.41, 'word': 'eight'},\n",
              "  {'conf': 1.0, 'end': 5.07, 'start': 4.71, 'word': 'four'},\n",
              "  {'conf': 1.0, 'end': 5.43, 'start': 5.07, 'word': 'zero'},\n",
              "  {'conf': 1.0, 'end': 5.85, 'start': 5.43, 'word': 'six'},\n",
              "  {'conf': 0.995706, 'end': 6.18, 'start': 5.85, 'word': 'four'},\n",
              "  {'conf': 1.0, 'end': 6.48, 'start': 6.18, 'word': 'one'},\n",
              "  {'conf': 1.0, 'end': 6.75, 'start': 6.51, 'word': 'three'}],\n",
              " 'text': 'ok ah contact number just put nine eight four zero six four one three'}"
            ]
          },
          "execution_count": 744,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 745,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'okay uh contact number just put [PHONE_START] Nine eight four zero six four one three [PHONE_END]'"
            ]
          },
          "execution_count": 745,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref['text'].iloc[51]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Force align"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "credit_card, car_plate, bank_account, nric, phone, passport_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 746,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'word': 'okay', 'start': 0.66, 'end': None},\n",
              " {'word': 'uh', 'start': None, 'end': 1.8},\n",
              " {'word': 'contact', 'start': 2.01, 'end': 2.55},\n",
              " {'word': 'number', 'start': 2.55, 'end': 3.0},\n",
              " {'word': 'just', 'start': 3.12, 'end': 3.48},\n",
              " {'word': 'put', 'start': 3.48, 'end': 3.75},\n",
              " {'word': '[PHONE_START]', 'start': None, 'end': None},\n",
              " {'word': 'nine', 'start': 3.93, 'end': 4.38},\n",
              " {'word': 'eight', 'start': 4.41, 'end': 4.71},\n",
              " {'word': 'four', 'start': 4.71, 'end': 5.07},\n",
              " {'word': 'zero', 'start': 5.07, 'end': 5.43},\n",
              " {'word': 'six', 'start': 5.43, 'end': 5.85},\n",
              " {'word': 'four', 'start': 5.85, 'end': 6.18},\n",
              " {'word': 'one', 'start': 6.18, 'end': 6.48},\n",
              " {'word': 'three', 'start': 6.51, 'end': 6.75},\n",
              " {'word': '[PHONE_END]', 'start': None, 'end': None}]"
            ]
          },
          "execution_count": 746,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "align_transcript_with_vosk(sample2['result'], test_set_ref['text'].iloc[51])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 747,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample3 = run_vosk(\"data/newtest_151_500_updated_TTS/id192.wav\", model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 748,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'result': [{'conf': 1.0, 'end': 0.48, 'start': 0.09, 'word': 'john'},\n",
              "  {'conf': 1.0, 'end': 0.78, 'start': 0.48, 'word': 'c'},\n",
              "  {'conf': 1.0, 'end': 1.02, 'start': 0.78, 'word': 'as'},\n",
              "  {'conf': 1.0, 'end': 1.86, 'start': 1.05, 'word': 'ocbc'},\n",
              "  {'conf': 1.0, 'end': 2.28, 'start': 1.86, 'word': 'account'},\n",
              "  {'conf': 1.0, 'end': 2.49, 'start': 2.31, 'word': 'is'},\n",
              "  {'conf': 1.0, 'end': 2.79, 'start': 2.49, 'word': 'six'},\n",
              "  {'conf': 1.0, 'end': 3.06, 'start': 2.79, 'word': 'five'},\n",
              "  {'conf': 1.0, 'end': 3.27, 'start': 3.06, 'word': 'eight'},\n",
              "  {'conf': 1.0, 'end': 3.69, 'start': 3.42, 'word': 'three'},\n",
              "  {'conf': 1.0, 'end': 4.08, 'start': 3.69, 'word': 'seven'},\n",
              "  {'conf': 0.948567, 'end': 4.29, 'start': 4.08, 'word': 'two'},\n",
              "  {'conf': 1.0, 'end': 4.56, 'start': 4.29, 'word': 'nine'},\n",
              "  {'conf': 0.90834, 'end': 4.86, 'start': 4.56, 'word': 'four'},\n",
              "  {'conf': 1.0, 'end': 5.19, 'start': 4.86, 'word': 'one'},\n",
              "  {'conf': 1.0, 'end': 5.61, 'start': 5.46, 'word': 'and'},\n",
              "  {'conf': 1.0, 'end': 5.88, 'start': 5.61, 'word': \"he's\"},\n",
              "  {'conf': 1.0, 'end': 6.0, 'start': 5.88, 'word': 'been'},\n",
              "  {'conf': 1.0, 'end': 6.51, 'start': 6.0, 'word': 'neglecting'},\n",
              "  {'conf': 1.0, 'end': 6.63, 'start': 6.51, 'word': 'it'},\n",
              "  {'conf': 1.0, 'end': 6.9, 'start': 6.63, 'word': 'since'},\n",
              "  {'conf': 1.0, 'end': 7.05, 'start': 6.9, 'word': 'his'},\n",
              "  {'conf': 1.0, 'end': 7.29, 'start': 7.05, 'word': 'move'},\n",
              "  {'conf': 1.0, 'end': 7.41, 'start': 7.29, 'word': 'to'},\n",
              "  {'conf': 0.532152, 'end': 8.07, 'start': 7.41, 'word': 'tampons'}],\n",
              " 'text': \"john c as ocbc account is six five eight three seven two nine four one and he's been neglecting it since his move to tampons\"}"
            ]
          },
          "execution_count": 748,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"John Seah's OCBC account is [BANK_ACCOUNT_START] 658-37294-1 [BANK_ACCOUNT_END] , and he's been neglecting it since his move to Tampines\""
            ]
          },
          "execution_count": 749,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set_ref['text'].iloc[47]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'word': 'john', 'start': 0.09, 'end': 0.48},\n",
              " {'word': 'seah', 'start': 0.48, 'end': None},\n",
              " {'word': 's', 'start': None, 'end': 1.02},\n",
              " {'word': 'ocbc', 'start': 1.05, 'end': 1.86},\n",
              " {'word': 'account', 'start': 1.86, 'end': 2.28},\n",
              " {'word': 'is', 'start': 2.31, 'end': 2.49},\n",
              " {'word': '[BANK_ACCOUNT_START]', 'start': None, 'end': None},\n",
              " {'word': 'six', 'start': 2.49, 'end': 2.79},\n",
              " {'word': 'five', 'start': 2.79, 'end': 3.06},\n",
              " {'word': 'eight', 'start': 3.06, 'end': 3.27},\n",
              " {'word': 'three', 'start': 3.42, 'end': 3.69},\n",
              " {'word': 'seven', 'start': 3.69, 'end': 4.08},\n",
              " {'word': 'two', 'start': 4.08, 'end': 4.29},\n",
              " {'word': 'nine', 'start': 4.29, 'end': 4.56},\n",
              " {'word': 'four', 'start': 4.56, 'end': 4.86},\n",
              " {'word': 'one', 'start': 4.86, 'end': 5.19},\n",
              " {'word': '[BANK_ACCOUNT_END]', 'start': None, 'end': None},\n",
              " {'word': 'and', 'start': 5.46, 'end': 5.61},\n",
              " {'word': 'he', 'start': 5.61, 'end': None},\n",
              " {'word': 's', 'start': None, 'end': 5.88},\n",
              " {'word': 'been', 'start': 5.88, 'end': 6.0},\n",
              " {'word': 'neglecting', 'start': 6.0, 'end': 6.51},\n",
              " {'word': 'it', 'start': 6.51, 'end': 6.63},\n",
              " {'word': 'since', 'start': 6.63, 'end': 6.9},\n",
              " {'word': 'his', 'start': 6.9, 'end': 7.05},\n",
              " {'word': 'move', 'start': 7.05, 'end': 7.29},\n",
              " {'word': 'to', 'start': 7.29, 'end': 7.41},\n",
              " {'word': 'tampines', 'start': 7.41, 'end': 8.07}]"
            ]
          },
          "execution_count": 750,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "align_transcript_with_vosk(sample3['result'], test_set_ref['text'].iloc[47])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Read Audio (All 500 samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 840,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "audio_paths = sorted(os.listdir(\"data/Audio_Files_for_testing\") + os.listdir(\"data/newtest_151_500_updated_TTS\"), key=retrieve_key)\n",
        "audio_paths_with_parent = [f'data/Audio_Files_for_testing/{file}' for file in audio_paths if file.endswith('.wav') and retrieve_key(file) < 151]\n",
        "audio_paths_with_parent += [f'data/newtest_151_500_updated_TTS/{file}' for file in audio_paths if file.endswith('.wav') and retrieve_key(file) >= 151]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 841,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['data/Audio_Files_for_testing/id1.wav',\n",
              " 'data/Audio_Files_for_testing/id2.wav',\n",
              " 'data/Audio_Files_for_testing/id3.wav',\n",
              " 'data/Audio_Files_for_testing/id4.wav',\n",
              " 'data/Audio_Files_for_testing/id5.wav',\n",
              " 'data/Audio_Files_for_testing/id6.wav',\n",
              " 'data/Audio_Files_for_testing/id7.wav',\n",
              " 'data/Audio_Files_for_testing/id8.wav',\n",
              " 'data/Audio_Files_for_testing/id9.wav',\n",
              " 'data/Audio_Files_for_testing/id10.wav',\n",
              " 'data/Audio_Files_for_testing/id11.wav',\n",
              " 'data/Audio_Files_for_testing/id12.wav',\n",
              " 'data/Audio_Files_for_testing/id13.wav',\n",
              " 'data/Audio_Files_for_testing/id14.wav',\n",
              " 'data/Audio_Files_for_testing/id15.wav',\n",
              " 'data/Audio_Files_for_testing/id16.wav',\n",
              " 'data/Audio_Files_for_testing/id17.wav',\n",
              " 'data/Audio_Files_for_testing/id18.wav',\n",
              " 'data/Audio_Files_for_testing/id19.wav',\n",
              " 'data/Audio_Files_for_testing/id20.wav',\n",
              " 'data/Audio_Files_for_testing/id21.wav',\n",
              " 'data/Audio_Files_for_testing/id22.wav',\n",
              " 'data/Audio_Files_for_testing/id23.wav',\n",
              " 'data/Audio_Files_for_testing/id24.wav',\n",
              " 'data/Audio_Files_for_testing/id25.wav',\n",
              " 'data/Audio_Files_for_testing/id26.wav',\n",
              " 'data/Audio_Files_for_testing/id27.wav',\n",
              " 'data/Audio_Files_for_testing/id28.wav',\n",
              " 'data/Audio_Files_for_testing/id29.wav',\n",
              " 'data/Audio_Files_for_testing/id30.wav',\n",
              " 'data/Audio_Files_for_testing/id31.wav',\n",
              " 'data/Audio_Files_for_testing/id32.wav',\n",
              " 'data/Audio_Files_for_testing/id33.wav',\n",
              " 'data/Audio_Files_for_testing/id34.wav',\n",
              " 'data/Audio_Files_for_testing/id35.wav',\n",
              " 'data/Audio_Files_for_testing/id36.wav',\n",
              " 'data/Audio_Files_for_testing/id37.wav',\n",
              " 'data/Audio_Files_for_testing/id38.wav',\n",
              " 'data/Audio_Files_for_testing/id39.wav',\n",
              " 'data/Audio_Files_for_testing/id40.wav',\n",
              " 'data/Audio_Files_for_testing/id41.wav',\n",
              " 'data/Audio_Files_for_testing/id42.wav',\n",
              " 'data/Audio_Files_for_testing/id43.wav',\n",
              " 'data/Audio_Files_for_testing/id44.wav',\n",
              " 'data/Audio_Files_for_testing/id45.wav',\n",
              " 'data/Audio_Files_for_testing/id46.wav',\n",
              " 'data/Audio_Files_for_testing/id47.wav',\n",
              " 'data/Audio_Files_for_testing/id48.wav',\n",
              " 'data/Audio_Files_for_testing/id49.wav',\n",
              " 'data/Audio_Files_for_testing/id50.wav',\n",
              " 'data/Audio_Files_for_testing/id51.wav',\n",
              " 'data/Audio_Files_for_testing/id52.wav',\n",
              " 'data/Audio_Files_for_testing/id53.wav',\n",
              " 'data/Audio_Files_for_testing/id54.wav',\n",
              " 'data/Audio_Files_for_testing/id55.wav',\n",
              " 'data/Audio_Files_for_testing/id56.wav',\n",
              " 'data/Audio_Files_for_testing/id57.wav',\n",
              " 'data/Audio_Files_for_testing/id58.wav',\n",
              " 'data/Audio_Files_for_testing/id59.wav',\n",
              " 'data/Audio_Files_for_testing/id60.wav',\n",
              " 'data/Audio_Files_for_testing/id61.wav',\n",
              " 'data/Audio_Files_for_testing/id62.wav',\n",
              " 'data/Audio_Files_for_testing/id63.wav',\n",
              " 'data/Audio_Files_for_testing/id64.wav',\n",
              " 'data/Audio_Files_for_testing/id65.wav',\n",
              " 'data/Audio_Files_for_testing/id66.wav',\n",
              " 'data/Audio_Files_for_testing/id67.wav',\n",
              " 'data/Audio_Files_for_testing/id68.wav',\n",
              " 'data/Audio_Files_for_testing/id69.wav',\n",
              " 'data/Audio_Files_for_testing/id70.wav',\n",
              " 'data/Audio_Files_for_testing/id71.wav',\n",
              " 'data/Audio_Files_for_testing/id72.wav',\n",
              " 'data/Audio_Files_for_testing/id73.wav',\n",
              " 'data/Audio_Files_for_testing/id74.wav',\n",
              " 'data/Audio_Files_for_testing/id75.wav',\n",
              " 'data/Audio_Files_for_testing/id76.wav',\n",
              " 'data/Audio_Files_for_testing/id77.wav',\n",
              " 'data/Audio_Files_for_testing/id78.wav',\n",
              " 'data/Audio_Files_for_testing/id79.wav',\n",
              " 'data/Audio_Files_for_testing/id80.wav',\n",
              " 'data/Audio_Files_for_testing/id81.wav',\n",
              " 'data/Audio_Files_for_testing/id82.wav',\n",
              " 'data/Audio_Files_for_testing/id83.wav',\n",
              " 'data/Audio_Files_for_testing/id84.wav',\n",
              " 'data/Audio_Files_for_testing/id85.wav',\n",
              " 'data/Audio_Files_for_testing/id86.wav',\n",
              " 'data/Audio_Files_for_testing/id87.wav',\n",
              " 'data/Audio_Files_for_testing/id88.wav',\n",
              " 'data/Audio_Files_for_testing/id89.wav',\n",
              " 'data/Audio_Files_for_testing/id90.wav',\n",
              " 'data/Audio_Files_for_testing/id91.wav',\n",
              " 'data/Audio_Files_for_testing/id92.wav',\n",
              " 'data/Audio_Files_for_testing/id93.wav',\n",
              " 'data/Audio_Files_for_testing/id94.wav',\n",
              " 'data/Audio_Files_for_testing/id95.wav',\n",
              " 'data/Audio_Files_for_testing/id96.wav',\n",
              " 'data/Audio_Files_for_testing/id97.wav',\n",
              " 'data/Audio_Files_for_testing/id98.wav',\n",
              " 'data/Audio_Files_for_testing/id99.wav',\n",
              " 'data/Audio_Files_for_testing/id100.wav',\n",
              " 'data/Audio_Files_for_testing/id101.wav',\n",
              " 'data/Audio_Files_for_testing/id102.wav',\n",
              " 'data/Audio_Files_for_testing/id103.wav',\n",
              " 'data/Audio_Files_for_testing/id104.wav',\n",
              " 'data/Audio_Files_for_testing/id105.wav',\n",
              " 'data/Audio_Files_for_testing/id106.wav',\n",
              " 'data/Audio_Files_for_testing/id107.wav',\n",
              " 'data/Audio_Files_for_testing/id108.wav',\n",
              " 'data/Audio_Files_for_testing/id109.wav',\n",
              " 'data/Audio_Files_for_testing/id110.wav',\n",
              " 'data/Audio_Files_for_testing/id111.wav',\n",
              " 'data/Audio_Files_for_testing/id112.wav',\n",
              " 'data/Audio_Files_for_testing/id113.wav',\n",
              " 'data/Audio_Files_for_testing/id114.wav',\n",
              " 'data/Audio_Files_for_testing/id115.wav',\n",
              " 'data/Audio_Files_for_testing/id116.wav',\n",
              " 'data/Audio_Files_for_testing/id117.wav',\n",
              " 'data/Audio_Files_for_testing/id118.wav',\n",
              " 'data/Audio_Files_for_testing/id119.wav',\n",
              " 'data/Audio_Files_for_testing/id120.wav',\n",
              " 'data/Audio_Files_for_testing/id121.wav',\n",
              " 'data/Audio_Files_for_testing/id122.wav',\n",
              " 'data/Audio_Files_for_testing/id123.wav',\n",
              " 'data/Audio_Files_for_testing/id124.wav',\n",
              " 'data/Audio_Files_for_testing/id125.wav',\n",
              " 'data/Audio_Files_for_testing/id126.wav',\n",
              " 'data/Audio_Files_for_testing/id127.wav',\n",
              " 'data/Audio_Files_for_testing/id128.wav',\n",
              " 'data/Audio_Files_for_testing/id129.wav',\n",
              " 'data/Audio_Files_for_testing/id130.wav',\n",
              " 'data/Audio_Files_for_testing/id131.wav',\n",
              " 'data/Audio_Files_for_testing/id132.wav',\n",
              " 'data/Audio_Files_for_testing/id133.wav',\n",
              " 'data/Audio_Files_for_testing/id134.wav',\n",
              " 'data/Audio_Files_for_testing/id135.wav',\n",
              " 'data/Audio_Files_for_testing/id136.wav',\n",
              " 'data/Audio_Files_for_testing/id137.wav',\n",
              " 'data/Audio_Files_for_testing/id138.wav',\n",
              " 'data/Audio_Files_for_testing/id139.wav',\n",
              " 'data/Audio_Files_for_testing/id140.wav',\n",
              " 'data/Audio_Files_for_testing/id141.wav',\n",
              " 'data/Audio_Files_for_testing/id142.wav',\n",
              " 'data/Audio_Files_for_testing/id143.wav',\n",
              " 'data/Audio_Files_for_testing/id144.wav',\n",
              " 'data/Audio_Files_for_testing/id145.wav',\n",
              " 'data/Audio_Files_for_testing/id146.wav',\n",
              " 'data/Audio_Files_for_testing/id147.wav',\n",
              " 'data/Audio_Files_for_testing/id148.wav',\n",
              " 'data/Audio_Files_for_testing/id149.wav',\n",
              " 'data/Audio_Files_for_testing/id150.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id151.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id152.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id153.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id154.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id155.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id156.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id157.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id158.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id159.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id160.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id161.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id162.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id163.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id164.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id165.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id166.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id167.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id168.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id169.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id170.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id171.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id172.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id173.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id174.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id175.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id176.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id177.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id178.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id179.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id180.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id181.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id182.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id183.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id184.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id185.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id186.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id187.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id188.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id189.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id190.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id191.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id192.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id193.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id194.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id195.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id196.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id197.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id198.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id199.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id200.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id201.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id202.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id203.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id204.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id205.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id206.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id207.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id208.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id209.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id210.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id211.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id212.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id213.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id214.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id215.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id216.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id217.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id218.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id219.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id220.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id221.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id222.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id223.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id224.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id225.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id226.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id227.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id228.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id229.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id230.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id231.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id232.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id233.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id234.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id235.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id236.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id237.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id238.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id239.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id240.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id241.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id242.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id243.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id244.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id245.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id246.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id247.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id248.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id249.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id250.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id251.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id252.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id253.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id254.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id255.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id256.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id257.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id258.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id259.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id260.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id261.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id262.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id263.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id264.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id265.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id266.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id267.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id268.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id269.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id270.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id271.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id272.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id273.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id274.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id275.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id276.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id277.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id278.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id279.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id280.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id281.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id282.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id283.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id284.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id285.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id286.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id287.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id288.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id289.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id290.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id291.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id292.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id293.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id294.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id295.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id296.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id297.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id298.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id299.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id300.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id301.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id302.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id303.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id304.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id305.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id306.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id307.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id308.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id309.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id310.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id311.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id312.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id313.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id314.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id315.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id316.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id317.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id318.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id319.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id320.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id321.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id322.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id323.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id324.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id325.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id326.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id327.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id328.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id329.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id330.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id331.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id332.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id333.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id334.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id335.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id336.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id337.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id338.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id339.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id340.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id341.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id342.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id343.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id344.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id345.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id346.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id347.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id348.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id349.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id350.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id351.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id352.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id353.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id354.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id355.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id356.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id357.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id358.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id359.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id360.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id361.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id362.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id363.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id364.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id365.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id366.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id367.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id368.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id369.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id370.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id371.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id372.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id373.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id374.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id375.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id376.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id377.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id378.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id379.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id380.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id381.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id382.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id383.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id384.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id385.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id386.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id387.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id388.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id389.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id390.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id391.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id392.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id393.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id394.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id395.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id396.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id397.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id398.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id399.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id400.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id401.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id402.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id403.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id404.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id405.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id406.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id407.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id408.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id409.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id410.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id411.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id412.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id413.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id414.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id415.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id416.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id417.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id418.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id419.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id420.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id421.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id422.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id423.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id424.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id425.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id426.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id427.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id428.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id429.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id430.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id431.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id432.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id433.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id434.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id435.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id436.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id437.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id438.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id439.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id440.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id441.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id442.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id443.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id444.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id445.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id446.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id447.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id448.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id449.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id450.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id451.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id452.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id453.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id454.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id455.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id456.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id457.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id458.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id459.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id460.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id461.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id462.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id463.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id464.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id465.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id466.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id467.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id468.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id469.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id470.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id471.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id472.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id473.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id474.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id475.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id476.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id477.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id478.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id479.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id480.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id481.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id482.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id483.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id484.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id485.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id486.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id487.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id488.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id489.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id490.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id491.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id492.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id493.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id494.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id495.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id496.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id497.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id498.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id499.wav',\n",
              " 'data/newtest_151_500_updated_TTS/id500.wav']"
            ]
          },
          "execution_count": 841,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "audio_paths_with_parent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 842,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "execution_count": 842,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(audio_paths_with_parent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 859,
      "metadata": {},
      "outputs": [],
      "source": [
        "aligned_transcripts = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 860,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running forced alignment algorithm:  48%|████▊     | 242/500 [06:09<02:29,  1.73files/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing data/newtest_151_500_updated_TTS/id243.wav: 'result'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running forced alignment algorithm: 100%|██████████| 500/500 [11:44<00:00,  1.41s/files]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for audio in tqdm(audio_paths_with_parent, desc=\"Running forced alignment algorithm\", unit=\"files\", total=len(audio_paths_with_parent)):\n",
        "    try:\n",
        "        sample = run_vosk(audio, model)\n",
        "        vosk_output = sample['result']\n",
        "        # Ref text just get the number e.g., id48\n",
        "        idx = int(extract_id_number(audio))\n",
        "        ref_text = test_set_ref['text'].iloc[idx - 1]\n",
        "        ref_text_aligned = align_transcript_with_vosk(vosk_output, ref_text)\n",
        "        aligned_transcripts.append({\n",
        "            'file_name': audio,\n",
        "            'vosk_output': vosk_output,\n",
        "            'ref_text': ref_text,\n",
        "            'aligned_transcript': ref_text_aligned\n",
        "        })\n",
        "        # print(ref_text_aligned)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio}: {e}\")\n",
        "        aligned_transcripts.append({\n",
        "            'file_name': audio,\n",
        "            'vosk_output': None,\n",
        "            'ref_text': None,\n",
        "            'aligned_transcript': None\n",
        "        })\n",
        "        continue        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 861,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>vosk_output</th>\n",
              "      <th>ref_text</th>\n",
              "      <th>aligned_transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/Audio_Files_for_testing/id1.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.69, 'start': 0.51, 'wo...</td>\n",
              "      <td>The day before [DATE_START] yesterday, [DATE_E...</td>\n",
              "      <td>[{'word': 'the', 'start': 0.51, 'end': 0.69}, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/Audio_Files_for_testing/id2.wav</td>\n",
              "      <td>[{'conf': 0.666652, 'end': 1.14, 'start': 0.69...</td>\n",
              "      <td>um my date of birth is uh second [DATE_START] ...</td>\n",
              "      <td>[{'word': 'um', 'start': 0.69, 'end': 1.14}, {...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/Audio_Files_for_testing/id3.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.84, 'start': 0.48, 'wo...</td>\n",
              "      <td>she handed over a crumpled piece of paper, the...</td>\n",
              "      <td>[{'word': 'she', 'start': 0.48, 'end': 0.84}, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/Audio_Files_for_testing/id4.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 1.5, 'start': 1.11, 'wor...</td>\n",
              "      <td>aglio olio and err uh [CARDINAL_START] three t...</td>\n",
              "      <td>[{'word': 'aglio', 'start': None, 'end': None}...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/Audio_Files_for_testing/id5.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 1.65, 'start': 1.14, 'wo...</td>\n",
              "      <td>[PERSON_START] Hong's [PERSON_END] email is [E...</td>\n",
              "      <td>[{'word': '[PERSON_START]', 'start': 1.14, 'en...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              file_name  \\\n",
              "0  data/Audio_Files_for_testing/id1.wav   \n",
              "1  data/Audio_Files_for_testing/id2.wav   \n",
              "2  data/Audio_Files_for_testing/id3.wav   \n",
              "3  data/Audio_Files_for_testing/id4.wav   \n",
              "4  data/Audio_Files_for_testing/id5.wav   \n",
              "\n",
              "                                         vosk_output  \\\n",
              "0  [{'conf': 1.0, 'end': 0.69, 'start': 0.51, 'wo...   \n",
              "1  [{'conf': 0.666652, 'end': 1.14, 'start': 0.69...   \n",
              "2  [{'conf': 1.0, 'end': 0.84, 'start': 0.48, 'wo...   \n",
              "3  [{'conf': 1.0, 'end': 1.5, 'start': 1.11, 'wor...   \n",
              "4  [{'conf': 1.0, 'end': 1.65, 'start': 1.14, 'wo...   \n",
              "\n",
              "                                            ref_text  \\\n",
              "0  The day before [DATE_START] yesterday, [DATE_E...   \n",
              "1  um my date of birth is uh second [DATE_START] ...   \n",
              "2  she handed over a crumpled piece of paper, the...   \n",
              "3  aglio olio and err uh [CARDINAL_START] three t...   \n",
              "4  [PERSON_START] Hong's [PERSON_END] email is [E...   \n",
              "\n",
              "                                  aligned_transcript  \n",
              "0  [{'word': 'the', 'start': 0.51, 'end': 0.69}, ...  \n",
              "1  [{'word': 'um', 'start': 0.69, 'end': 1.14}, {...  \n",
              "2  [{'word': 'she', 'start': 0.48, 'end': 0.84}, ...  \n",
              "3  [{'word': 'aglio', 'start': None, 'end': None}...  \n",
              "4  [{'word': '[PERSON_START]', 'start': 1.14, 'en...  "
            ]
          },
          "execution_count": 861,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df = pd.DataFrame(aligned_transcripts)\n",
        "processed_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 862,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>vosk_output</th>\n",
              "      <th>ref_text</th>\n",
              "      <th>aligned_transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id496.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.45, 'start': 0.03, 'wo...</td>\n",
              "      <td>Patrick Loh boasting about his email [EMAIL_ST...</td>\n",
              "      <td>[{'word': 'patrick', 'start': 0.03, 'end': 0.4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id497.wav</td>\n",
              "      <td>[{'conf': 0.990332, 'end': 0.45, 'start': 0.03...</td>\n",
              "      <td>Jasmine Yeo got sian when someone spell her em...</td>\n",
              "      <td>[{'word': 'jasmine', 'start': 0.03, 'end': 0.4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id498.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.45, 'start': 0.03, 'wo...</td>\n",
              "      <td>Bobby Tan write his email [EMAIL_START] bobby....</td>\n",
              "      <td>[{'word': 'bobby', 'start': 0.03, 'end': 0.45}...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id499.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.48, 'start': 0.06, 'wo...</td>\n",
              "      <td>Kamala Singh telling the IT guy her email [EMA...</td>\n",
              "      <td>[{'word': 'kamala', 'start': 0.06, 'end': 0.48...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>data/newtest_151_500_updated_TTS/id500.wav</td>\n",
              "      <td>[{'conf': 1.0, 'end': 0.48, 'start': 0.09, 'wo...</td>\n",
              "      <td>Raymond Koh say his email [EMAIL_START] raymon...</td>\n",
              "      <td>[{'word': 'raymond', 'start': 0.09, 'end': 0.4...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      file_name  \\\n",
              "495  data/newtest_151_500_updated_TTS/id496.wav   \n",
              "496  data/newtest_151_500_updated_TTS/id497.wav   \n",
              "497  data/newtest_151_500_updated_TTS/id498.wav   \n",
              "498  data/newtest_151_500_updated_TTS/id499.wav   \n",
              "499  data/newtest_151_500_updated_TTS/id500.wav   \n",
              "\n",
              "                                           vosk_output  \\\n",
              "495  [{'conf': 1.0, 'end': 0.45, 'start': 0.03, 'wo...   \n",
              "496  [{'conf': 0.990332, 'end': 0.45, 'start': 0.03...   \n",
              "497  [{'conf': 1.0, 'end': 0.45, 'start': 0.03, 'wo...   \n",
              "498  [{'conf': 1.0, 'end': 0.48, 'start': 0.06, 'wo...   \n",
              "499  [{'conf': 1.0, 'end': 0.48, 'start': 0.09, 'wo...   \n",
              "\n",
              "                                              ref_text  \\\n",
              "495  Patrick Loh boasting about his email [EMAIL_ST...   \n",
              "496  Jasmine Yeo got sian when someone spell her em...   \n",
              "497  Bobby Tan write his email [EMAIL_START] bobby....   \n",
              "498  Kamala Singh telling the IT guy her email [EMA...   \n",
              "499  Raymond Koh say his email [EMAIL_START] raymon...   \n",
              "\n",
              "                                    aligned_transcript  \n",
              "495  [{'word': 'patrick', 'start': 0.03, 'end': 0.4...  \n",
              "496  [{'word': 'jasmine', 'start': 0.03, 'end': 0.4...  \n",
              "497  [{'word': 'bobby', 'start': 0.03, 'end': 0.45}...  \n",
              "498  [{'word': 'kamala', 'start': 0.06, 'end': 0.48...  \n",
              "499  [{'word': 'raymond', 'start': 0.09, 'end': 0.4...  "
            ]
          },
          "execution_count": 862,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 866,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'conf': 0.84101, 'end': 0.36, 'start': 0.03, 'word': 'mark'},\n",
              " {'conf': 1.0, 'end': 0.81, 'start': 0.36, 'word': 'lee'},\n",
              " {'conf': 1.0, 'end': 1.14, 'start': 0.84, 'word': 'gave'},\n",
              " {'conf': 1.0, 'end': 1.47, 'start': 1.14, 'word': 'his'},\n",
              " {'conf': 1.0, 'end': 2.52, 'start': 1.89, 'word': 'icy'},\n",
              " {'conf': 0.855037, 'end': 3.51, 'start': 3.0, 'word': 'eso'},\n",
              " {'conf': 1.0, 'end': 3.75, 'start': 3.51, 'word': 'eight'},\n",
              " {'conf': 1.0, 'end': 4.23, 'start': 3.75, 'word': 'nine'},\n",
              " {'conf': 1.0, 'end': 4.5, 'start': 4.23, 'word': 'one'},\n",
              " {'conf': 1.0, 'end': 4.98, 'start': 4.5, 'word': 'seven'},\n",
              " {'conf': 1.0, 'end': 5.22, 'start': 4.98, 'word': 'one'},\n",
              " {'conf': 1.0, 'end': 5.7, 'start': 5.22, 'word': 'nine'},\n",
              " {'conf': 1.0, 'end': 6.27, 'start': 5.88, 'word': 'd'},\n",
              " {'conf': 1.0, 'end': 7.08, 'start': 6.9, 'word': 'to'},\n",
              " {'conf': 1.0, 'end': 7.2, 'start': 7.08, 'word': 'the'},\n",
              " {'conf': 1.0, 'end': 7.53, 'start': 7.2, 'word': 'bank'},\n",
              " {'conf': 1.0, 'end': 7.92, 'start': 7.53, 'word': 'teller'},\n",
              " {'conf': 1.0, 'end': 8.1, 'start': 7.92, 'word': 'for'},\n",
              " {'conf': 1.0, 'end': 8.22, 'start': 8.1, 'word': 'the'},\n",
              " {'conf': 1.0, 'end': 8.4, 'start': 8.22, 'word': 'new'},\n",
              " {'conf': 1.0, 'end': 9.06, 'start': 8.4, 'word': 'account'}]"
            ]
          },
          "execution_count": 866,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df['vosk_output'].iloc[332]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 867,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Mark Lee gave his I C [NRIC_START] S0891719D [NRIC_END] to the bank teller for the new account.'"
            ]
          },
          "execution_count": 867,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df['ref_text'].iloc[332]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 868,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'word': 'mark', 'start': 0.03, 'end': 0.36},\n",
              " {'word': 'lee', 'start': 0.36, 'end': 0.81},\n",
              " {'word': 'gave', 'start': 0.84, 'end': 1.14},\n",
              " {'word': 'his', 'start': 1.14, 'end': 1.47},\n",
              " {'word': 'i', 'start': 1.89, 'end': None},\n",
              " {'word': 'c', 'start': None, 'end': None},\n",
              " {'word': '[NRIC_START]', 'start': None, 'end': None},\n",
              " {'word': 's0891719d', 'start': None, 'end': None},\n",
              " {'word': '[NRIC_END]', 'start': None, 'end': 6.27},\n",
              " {'word': 'to', 'start': 6.9, 'end': 7.08},\n",
              " {'word': 'the', 'start': 7.08, 'end': 7.2},\n",
              " {'word': 'bank', 'start': 7.2, 'end': 7.53},\n",
              " {'word': 'teller', 'start': 7.53, 'end': 7.92},\n",
              " {'word': 'for', 'start': 7.92, 'end': 8.1},\n",
              " {'word': 'the', 'start': 8.1, 'end': 8.22},\n",
              " {'word': 'new', 'start': 8.22, 'end': 8.4},\n",
              " {'word': 'account', 'start': 8.4, 'end': 9.06},\n",
              " {'word': '.', 'start': None, 'end': None}]"
            ]
          },
          "execution_count": 868,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df['aligned_transcript'].iloc[332]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 869,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['mark',\n",
              " 'lee',\n",
              " 'gave',\n",
              " 'his',\n",
              " 'i',\n",
              " 'c',\n",
              " '[NRIC_START]',\n",
              " 's0891719d',\n",
              " '[NRIC_END]',\n",
              " 'to',\n",
              " 'the',\n",
              " 'bank',\n",
              " 'teller',\n",
              " 'for',\n",
              " 'the',\n",
              " 'new',\n",
              " 'account',\n",
              " '.']"
            ]
          },
          "execution_count": 869,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize_reference(test_set_ref['text'].iloc[332])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "current issue:\n",
        "\n",
        "- works very well for numbers\n",
        "- need to convert the reference piis to text form (if number), and treat them as separate tokens (e.g., S1234567B -> S one two three four five six seven B; applies to NRIC, CAR_PLATE, BANK_ACCOUNT)\n",
        "- strings grouped together logically needs to be treated separately (e.g., sg -> s g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [Archived]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heuristics Description (Greedy-based Alignment):\n",
        "\n",
        "Forced-alignment heuristics are necessary because:\n",
        "- The **Vosk model** may tokenize words differently compared to the reference transcript, especially for structured data like emails, phone numbers, and other PIIs (Personally Identifiable Information).\n",
        "- **PII structures vary greatly** (e.g., \"rendy.tan@hotmail.com\" vs \"rendy . tan at hotmail dot com\"), and simple word-to-word alignment would fail.\n",
        "- To achieve robust alignment and accurate timestamp mapping, **manual control** over token splitting and flattening is required based on the entity type.\n",
        "\n",
        "These heuristics ensure that:\n",
        "- Common free-text is aligned naturally,\n",
        "- Structured PII is broken down appropriately for correct timestamp boundary matching.\n",
        "\n",
        "#### 1. Outside of Entity Boundaries (General Case)\n",
        "\n",
        "- Tokens are aligned **as-is** with Vosk words.\n",
        "- No special splitting is done.\n",
        "- Regular cleaning (punctuation removal except for `\".\"`) is applied when matching.\n",
        "- **Example**:\n",
        "  - Input Transcript: `\"reach me at\"`\n",
        "  - Tokens: `[\"reach\", \"me\", \"at\"]`\n",
        "  - Aligned directly without splitting.\n",
        "\n",
        "#### 2. Inside Entity Boundaries (e.g., [EMAIL_START], [PHONE_START], etc.)\n",
        "\n",
        "- Special handling is done based on the entity type.\n",
        "\n",
        "##### (A) EMAIL Entity (`current_entity == 'EMAIL'`)\n",
        "\n",
        "- Split tokens based on `\".\"` and `\"@\"` separators.\n",
        "- Words like `\"at\"` are **left intact**.\n",
        "- **Example**:\n",
        "  - Input: `\"rendy.tan@hotmail.com\"`\n",
        "  - Split into: `[\"rendy\", \".\", \"tan\", \"@\", \"hotmail\", \".\", \"com\"]`\n",
        "\n",
        "##### (B) Other Entity Types (`CREDIT_CARD`, `CAR_PLATE`, `BANK_ACCOUNT`, `NRIC`, `PHONE`, `PASSPORT_NUM`)\n",
        "\n",
        "- **If the token is a spelled-out number** (checked against a dictionary):\n",
        "  - **Do not split**; keep the word as a single token.\n",
        "  - **Example**:\n",
        "    - Input: `\"eight\"`\n",
        "    - Output: `[\"eight\"]`\n",
        "\n",
        "- **If the token is pure digits** (e.g., numbers like `\"98005331\"`):\n",
        "  - **Split** into **individual characters**.\n",
        "  - **Example**:\n",
        "    - Input: `\"98005331\"`\n",
        "    - Output: `[\"9\", \"8\", \"0\", \"0\", \"5\", \"3\", \"3\", \"1\"]`\n",
        "\n",
        "- **If the token is a mix of letters and numbers** (e.g., `\"AB1234X\"`):\n",
        "  - **Split** into **individual characters** as well.\n",
        "  - **Example**:\n",
        "    - Input: `\"AB1234X\"`\n",
        "    - Output: `[\"A\", \"B\", \"1\", \"2\", \"3\", \"4\", \"X\"]`\n",
        "\n",
        "#### 3. When Flattening Entity Tokens (Before Final Alignment)\n",
        "\n",
        "- After all splitting:\n",
        "  - **Spelled-out numbers** (like `\"eight\"`) and **email parts** (like `\"hotmail\"`) are **kept whole**.\n",
        "  - Other tokens (numbers, single characters) appear **character-by-character**.\n",
        "\n",
        "- **Flattening Examples**:\n",
        "  - Tokens: `[\"eight\", \"5\", \"0\"]`\n",
        "    - Final output: `\"eight 5 0\"`\n",
        "  \n",
        "  - Tokens: `[\"hotmail\", \".\", \"com\"]`\n",
        "    - Final output: `\"hotmail . com\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import tempfile\n",
        "from pydub import AudioSegment\n",
        "from vosk import Model, KaldiRecognizer\n",
        "model_path = \"vosk-model-en-us-0.42-gigaspeech\" #model_new\"\n",
        "model = Model(model_path)\n",
        "def align_audio_with_text(audio_path, transcription):\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    recognizer = KaldiRecognizer(model, audio.frame_rate)\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_wav:\n",
        "        temp_wav_path = temp_wav.name\n",
        "        audio.export(temp_wav_path, format=\"wav\")\n",
        "    results = []\n",
        "    try:\n",
        "        with open(temp_wav_path, \"rb\") as wf:\n",
        "            wf.read(44)\n",
        "            recognizer.SetWords(True)\n",
        "            while True:\n",
        "                data = wf.read(4000)\n",
        "                if len(data) == 0:\n",
        "                    break\n",
        "                if recognizer.AcceptWaveform(data):\n",
        "                    results.append(json.loads(recognizer.Result()))\n",
        "            results.append(json.loads(recognizer.FinalResult()))\n",
        "    finally:\n",
        "        if os.path.exists(temp_wav_path):\n",
        "            os.remove(temp_wav_path)\n",
        "    words = []\n",
        "    for result in results:\n",
        "        if 'result' in result:\n",
        "            for word in result['result']:\n",
        "                words.append(word)\n",
        "    aligned_segments = []\n",
        "    for word in words:\n",
        "        aligned_segments.append({\n",
        "            \"start\": word[\"start\"],\n",
        "            \"end\": word[\"end\"],\n",
        "            \"word\": word[\"word\"]\n",
        "        })\n",
        "    return aligned_segments\n",
        "\n",
        "audio_dir = \"/content/drive/MyDrive/Share/Research/speechNER/finetune/Audio_Files_for_testing\"\n",
        "transcription_file = \"/content/drive/MyDrive/Share/Research/speechNER/Alignement_data/Text_with_ids_temp_preprocessed.jsonl\"\n",
        "output_file = \"/content/drive/MyDrive/Share/Research/speechNER/Alignement_data/tr_aligned_data_new.jsonl\"\n",
        "with open(transcription_file, 'r') as f:\n",
        "    transcriptions = [json.loads(line) for line in f]\n",
        "aligned_data = []\n",
        "for item in transcriptions:\n",
        "    audio_path = f\"{audio_dir}/id{item['id']}.wav\"\n",
        "    aligned_transcription = align_audio_with_text(audio_path, item['text'])\n",
        "    aligned_data.append({\n",
        "        \"id\": item['id'],\n",
        "        \"text\": item['text'],\n",
        "        \"align\": aligned_transcription\n",
        "    })\n",
        "with open(output_file, 'w') as f:\n",
        "    for item in aligned_data:\n",
        "        f.write(json.dumps(item) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Old vosk alignment function with greedy decoding (Archived)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 682,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "spelled_out_numbers = {\n",
        "    'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5',\n",
        "    'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10',\n",
        "    'eleven': '11', 'twelve': '12', 'thirteen': '13', 'fourteen': '14',\n",
        "    'fifteen': '15', 'sixteen': '16', 'seventeen': '17', 'eighteen': '18',\n",
        "    'nineteen': '19', 'twenty': '20', 'thirty': '30', 'forty': '40', 'fifty': '50',\n",
        "    'sixty': '60', 'seventy': '70', 'eighty': '80', 'ninety': '90',\n",
        "    'hundred': '100', 'thousand': '1000'\n",
        "}\n",
        "\n",
        "def clean_token(token):\n",
        "    \"\"\"Remove punctuation except '.' and lowercase.\"\"\"\n",
        "    allowed = '.'\n",
        "    punctuation_to_remove = ''.join(c for c in string.punctuation if c not in allowed)\n",
        "    return token.lower().translate(str.maketrans('', '', punctuation_to_remove))\n",
        "\n",
        "def process_entity_tokens(entity_tokens, char_tokens):\n",
        "    \"\"\"Prevent duplicates and extend entity tokens list.\"\"\"\n",
        "    for token in char_tokens:\n",
        "        if token not in entity_tokens:\n",
        "            entity_tokens.append(token)\n",
        "\n",
        "def align_transcript_with_vosk(vosk_words, transcript):\n",
        "    \"\"\"\n",
        "    Aligns a reference transcript with Vosk timestamps.\n",
        "    Handles [XXX_START]... [XXX_END] entities properly.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r'\\[.*?\\]|\\S+', transcript)  # Tokenize the transcript\n",
        "    aligned = []\n",
        "    vosk_idx = 0\n",
        "    current_entity = None\n",
        "    entity_tokens = []\n",
        "    entity_start_time = None\n",
        "    entity_end_time = None\n",
        "\n",
        "    entity_types_to_split = ['CREDIT_CARD', 'CAR_PLATE', 'BANK_ACCOUNT', 'NRIC', 'PHONE', 'PASSPORT_NUM']\n",
        "    \n",
        "    # Special case for emails: split on the dots (.) and @ but leave 'at' as-is\n",
        "    def split_email(token):\n",
        "        # Case 1: email with spaces (no @)\n",
        "        if '.' in token:\n",
        "            parts = re.split(r'([.])', token)\n",
        "            parts = [p for p in parts if p != '']\n",
        "            # print(parts)\n",
        "            return parts  # Remove any empty strings\n",
        "        # Case 2: email with @\n",
        "        elif '@' in token:\n",
        "            parts = re.split(r'([@.])', token)\n",
        "            parts = [p for p in parts if p != '']\n",
        "            # print(parts)\n",
        "            return parts\n",
        "        return [token]\n",
        "\n",
        "    i = 0  # Index to keep track of the current token in the list\n",
        "\n",
        "    while i < len(tokens):\n",
        "        token = tokens[i]\n",
        "\n",
        "        # print(f\"Current token: {token}\")\n",
        "\n",
        "        if token.endswith('_START]'):\n",
        "            # Start a new entity\n",
        "            current_entity = token.replace('[', '').replace(']', '').replace('_START', '')\n",
        "            entity_tokens = []\n",
        "            entity_start_time = None\n",
        "            entity_end_time = None\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        if token.endswith('_END]'):\n",
        "            # End the current entity\n",
        "            if current_entity:\n",
        "                # Flatten the entity and align with timestamps\n",
        "                flattened_entity = []\n",
        "                # print(f\"New entity tokens: {entity_tokens}\")\n",
        "                \n",
        "                for t in entity_tokens:\n",
        "                    # Clean the token\n",
        "                    clean_token_with_no_symbols = clean_token(t)\n",
        "                    \n",
        "                    # Check if the token is a spelled-out number\n",
        "                    if clean_token_with_no_symbols.lower() in spelled_out_numbers or current_entity == 'EMAIL':\n",
        "                        # If it's a spelled-out number, don't split it into characters\n",
        "                        flattened_entity.append(clean_token_with_no_symbols)\n",
        "                    else:\n",
        "                        # Otherwise, split the token into characters\n",
        "                        flattened_entity.extend(list(clean_token_with_no_symbols))\n",
        "                    \n",
        "                # Join the characters and align timestamps\n",
        "                aligned.append({\n",
        "                    \"word\": f\"[{current_entity}_START] {' '.join(flattened_entity)} [{current_entity}_END]\",\n",
        "                    \"start\": entity_start_time,\n",
        "                    \"end\": entity_end_time\n",
        "                })\n",
        "            current_entity = None\n",
        "            entity_tokens = []\n",
        "            entity_start_time = None\n",
        "            entity_end_time = None\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        clean_ref_word = clean_token(token)\n",
        "\n",
        "        if current_entity:\n",
        "            # Inside an entity, split the token into characters and modify the tokens list\n",
        "            if vosk_idx < len(vosk_words):\n",
        "                vosk_word = vosk_words[vosk_idx]['word']\n",
        "                if not entity_tokens:\n",
        "                    entity_start_time = vosk_words[vosk_idx]['start']\n",
        "                entity_end_time = vosk_words[vosk_idx]['end']\n",
        "\n",
        "                # Special handling for emails: split valid email format\n",
        "                if current_entity == 'EMAIL':\n",
        "                    char_tokens = split_email(token)  # Split email into parts\n",
        "                    tokens[i:i+1] = char_tokens  # Replace the current token with the split characters\n",
        "\n",
        "                    # Prevent duplicate tokens and extend entity tokens list\n",
        "                    process_entity_tokens(entity_tokens, char_tokens)\n",
        "                    print(f\"Entity tokens after email split: {entity_tokens}\")\n",
        "                # Inside the loop where you handle the token splitting:\n",
        "                elif current_entity in entity_types_to_split:\n",
        "                    clean_token_with_no_symbols = clean_token(token)  # Clean token\n",
        "\n",
        "                    # Check if the token is a spelled-out number\n",
        "                    if clean_token_with_no_symbols.lower() in spelled_out_numbers.keys():\n",
        "                        # If it's a spelled-out number, don't split it\n",
        "                        char_tokens = [clean_token_with_no_symbols]  # Keep the token as is\n",
        "                    else:\n",
        "                        # If it's not a spelled-out number, split it into characters\n",
        "                        char_tokens = list(clean_token_with_no_symbols)\n",
        "\n",
        "                    # Modify the tokens list in place by extending with the character tokens\n",
        "                    tokens[i:i+1] = char_tokens\n",
        "\n",
        "                    # Prevent duplicates and extend entity tokens list\n",
        "                    process_entity_tokens(entity_tokens, char_tokens)\n",
        "\n",
        "                    print(f\"Entity tokens after split: {entity_tokens}\")\n",
        "\n",
        "                vosk_idx += 1\n",
        "            else:\n",
        "                # No more Vosk words left (shouldn't happen usually)\n",
        "                entity_tokens.append(token)\n",
        "\n",
        "        else:\n",
        "            # Outside entity, normal matching\n",
        "            while vosk_idx < len(vosk_words):\n",
        "                clean_vosk_word = clean_token(vosk_words[vosk_idx]['word'])\n",
        "                aligned.append({\n",
        "                    \"word\": token,\n",
        "                    \"start\": vosk_words[vosk_idx]['start'],\n",
        "                    \"end\": vosk_words[vosk_idx]['end']\n",
        "                })\n",
        "                vosk_idx += 1\n",
        "                break\n",
        "\n",
        "        i += 1  # Move to the next token\n",
        "        # print(tokens)\n",
        "\n",
        "    return aligned"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
