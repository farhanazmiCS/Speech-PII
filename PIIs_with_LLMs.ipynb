{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rethinking PII identification from Speech\n",
    "\n",
    "The goal of this experiment is to evaluate the efficacy of various prompt-engineering techniques for PII identification. Previously, we employed an entity-aware ASR model—fine-tuned on Singlish speech and enhanced with an expanded tokenizer—to perform NER tagging directly on speech. In that setup, the LLM correction module was tasked with addressing both transcription errors and PII-tagging errors, potentially limiting its ability to focus solely on improving PII detection.\n",
    "\n",
    "In this experiment, we will use the fine-tuned ASR model (trained on Singlish dialects) with its default tokenizer and delegate the entire PII tagging task to an LLM. This approach allows us to systematically compare different LLM prompting methods to determine which yields the best performance for our PII identification objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Login Hugging Face CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Perform transcription with N-best using fine-tuned ASR\n",
    "\n",
    "Skip this step if already transcribed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Download the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available, using CUDA\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"CUDA is available, using CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"MPS is available, using MPS\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CUDA and MPS are not available, switching to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-small.en\") # Using the default feature extractor and tokenizer\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"f-azm17/whisper-small_en_seed_gretel_similar0.3\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper's Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Load the dataset\n",
    "\n",
    "For this example, we shall use the 150 dataset in the test set from `Audio_Files_for_testing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_key(file: str) -> int:\n",
    "    try:\n",
    "        # 3 digit\n",
    "        key = int(file[2:5])\n",
    "    except ValueError:\n",
    "        # 1 digit\n",
    "        if file[3] == '.':\n",
    "            key = int(file[2])\n",
    "        else:\n",
    "            key = int(file[2:4])\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "audio_files = sorted(os.listdir(\"Audio_Files_for_testing\"), key=retrieve_key)\n",
    "audio_files = [f'Audio_Files_for_testing/{file}' for file in audio_files]\n",
    "print(len(audio_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audio_Files_for_testing/id1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audio_Files_for_testing/id2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Audio_Files_for_testing/id3.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audio_Files_for_testing/id4.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audio_Files_for_testing/id5.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file_name\n",
       "0  Audio_Files_for_testing/id1.wav\n",
       "1  Audio_Files_for_testing/id2.wav\n",
       "2  Audio_Files_for_testing/id3.wav\n",
       "3  Audio_Files_for_testing/id4.wav\n",
       "4  Audio_Files_for_testing/id5.wav"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.DataFrame(data=audio_files, columns=['file_name'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from typing import List\n",
    "\n",
    "def transcribe(audioPath: str, model: AutoModelForSpeechSeq2Seq, processor: AutoProcessor, device: str, best_n: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    A function which transcribes the audio based on a given audio file path.\n",
    "    Outputs the transcript along with the identified PII entities.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    audioPath (str) -- The path to the audio\\n\n",
    "    model (AutoModelForSpeechSeq2Seq) -- The ASR model\\n\n",
    "    processor (AutoProcessor) -- The processor, which contains the feature extractor and tokenizer.\\n\n",
    "    best_n (int) -- The best n number. By default, return the best transcription. \n",
    "\n",
    "    Return: The transcription along with the identified PII entities. (str)\n",
    "    \"\"\"\n",
    "    waveform, sr = librosa.load(audioPath, sr=16000)\n",
    "    inputs = processor(waveform, sampling_rate=sr, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_features=inputs[\"input_features\"], \n",
    "            temperature=1.0,\n",
    "            num_beams=best_n,\n",
    "            num_return_sequences=best_n\n",
    "        )\n",
    "    transcriptions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing and Identifying PII from test set...: 100%|██████████| 150/150 [03:17<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for index, row in tqdm(test_df.iterrows(), desc=\"Transcribing and Identifying PII from test set...\", total=len(test_df)):\n",
    "    transcriptions = transcribe(row['file_name'], model, processor, device, 5)\n",
    "    for i, transcription in enumerate(transcriptions):\n",
    "        test_df.at[index, f'rank_{i+1}'] = transcription  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "      <th>rank_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audio_Files_for_testing/id1.wav</td>\n",
       "      <td>the day before yesterday ram received another ...</td>\n",
       "      <td>The day before yesterday, jason received anoth...</td>\n",
       "      <td>The day before yesterday, Ram received another...</td>\n",
       "      <td>The day before yesterday,ram received another ...</td>\n",
       "      <td>The day before yesterday RAM received another ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Audio_Files_for_testing/id2.wav</td>\n",
       "      <td>um My date of birth is uh 2 september  19 92</td>\n",
       "      <td>mm my date of birth is uhm 2 september ninetee...</td>\n",
       "      <td>My date of birth is uh second september 19 92</td>\n",
       "      <td>My date of birth is 2 september,   9092 H</td>\n",
       "      <td>mm My date of birth is  uh second september ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Audio_Files_for_testing/id3.wav</td>\n",
       "      <td>hmm She handed over a crumpled piece of paper ...</td>\n",
       "      <td>she handed over a crumpled piece of paper ther...</td>\n",
       "      <td>She handed over a crumpled piece of paper  Thi...</td>\n",
       "      <td>She handed over a crumpled piece of paper ther...</td>\n",
       "      <td>she handed over a crumpled piece of paper  for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audio_Files_for_testing/id4.wav</td>\n",
       "      <td>uh and uh three of the other one ya</td>\n",
       "      <td>okay and uh three three of the other one yeah ...</td>\n",
       "      <td>I'll be picking it with another one and uh thr...</td>\n",
       "      <td>and uh uuh three three of the other ones yeah</td>\n",
       "      <td>uh and uh  uh  3  3 of the other one yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audio_Files_for_testing/id5.wav</td>\n",
       "      <td>uh  Hong 's EMAIL  is  P X 1R z at    47 at  ...</td>\n",
       "      <td>uhhh   Hong  s email is  px 1 rzu 4 7 at yahoo...</td>\n",
       "      <td>Hong's email is  P x1 rz'a 47 at yahoo.com</td>\n",
       "      <td>hongs email is  P x 1 r z a 4 7 at yahoo dot com</td>\n",
       "      <td>hes saying hes still  px one rz a four seven ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file_name  \\\n",
       "0  Audio_Files_for_testing/id1.wav   \n",
       "1  Audio_Files_for_testing/id2.wav   \n",
       "2  Audio_Files_for_testing/id3.wav   \n",
       "3  Audio_Files_for_testing/id4.wav   \n",
       "4  Audio_Files_for_testing/id5.wav   \n",
       "\n",
       "                                              rank_1  \\\n",
       "0  the day before yesterday ram received another ...   \n",
       "1      um My date of birth is uh 2 september  19 92    \n",
       "2  hmm She handed over a crumpled piece of paper ...   \n",
       "3                uh and uh three of the other one ya   \n",
       "4   uh  Hong 's EMAIL  is  P X 1R z at    47 at  ...   \n",
       "\n",
       "                                              rank_2  \\\n",
       "0  The day before yesterday, jason received anoth...   \n",
       "1  mm my date of birth is uhm 2 september ninetee...   \n",
       "2  she handed over a crumpled piece of paper ther...   \n",
       "3  okay and uh three three of the other one yeah ...   \n",
       "4  uhhh   Hong  s email is  px 1 rzu 4 7 at yahoo...   \n",
       "\n",
       "                                              rank_3  \\\n",
       "0  The day before yesterday, Ram received another...   \n",
       "1      My date of birth is uh second september 19 92   \n",
       "2  She handed over a crumpled piece of paper  Thi...   \n",
       "3  I'll be picking it with another one and uh thr...   \n",
       "4        Hong's email is  P x1 rz'a 47 at yahoo.com    \n",
       "\n",
       "                                              rank_4  \\\n",
       "0  The day before yesterday,ram received another ...   \n",
       "1         My date of birth is 2 september,   9092 H    \n",
       "2  She handed over a crumpled piece of paper ther...   \n",
       "3      and uh uuh three three of the other ones yeah   \n",
       "4  hongs email is  P x 1 r z a 4 7 at yahoo dot com    \n",
       "\n",
       "                                              rank_5  \n",
       "0  The day before yesterday RAM received another ...  \n",
       "1  mm My date of birth is  uh second september ni...  \n",
       "2  she handed over a crumpled piece of paper  for...  \n",
       "3          uh and uh  uh  3  3 of the other one yeah  \n",
       "4   hes saying hes still  px one rz a four seven ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('whisper-small_en_seed_gretel_similar0.3_no_tag_test_set_transcribed_n_best_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4 (Optional): Load the transcribed files, if already transcribed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The day before  yesterday,   Ram  received ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>um my date of birth is uh second  september  n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she handed over a crumpled piece of paper, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aglio olio and err uh  three three  of the oth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hong' s email is  t x 1 r z a four seven at ya...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript\n",
       "0  The day before  yesterday,   Ram  received ano...\n",
       "1  um my date of birth is uh second  september  n...\n",
       "2  she handed over a crumpled piece of paper, the...\n",
       "3  aglio olio and err uh  three three  of the oth...\n",
       "4  Hong' s email is  t x 1 r z a four seven at ya..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_json('true_data_150_no_tags.jsonl', lines=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5 (Experiments on Whisper with Word-Level Time-Stamps) [BEST ONE]\n",
    "\n",
    "This step uses OpenAI's GitHub implementation of Whisper, which now includes an option to output each token decoded along with their start and end times included. This is extremely useful for our system, as our system is meant to perform masking of the PII entities in speech, which requires the time boundaries.\n",
    "\n",
    "Along with this, we are also using the Whisper-Large-V3 model to perform transcription rather than the fine-tuned model, as the fine-tuned model was tuned with audio-transcript pairs containing the PII entities to help the ASR determine the appropriate PIIs decode. As we are doing away with the idea of an \"E2E\" approach (leveraging the LLM to perform entity tagging instead), we no longer require the FT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"large-v3\", device=\"cuda\")\n",
    "\n",
    "# Load and transcribe audio with word-level timestamps\n",
    "result = model.transcribe(\"Audio_Files_for_testing/id5.wav\", word_timestamps=True)\n",
    "\n",
    "# Print full transcription\n",
    "print(\"\\n--- Transcription ---\\n\", result[\"text\"])\n",
    "\n",
    "# Print each word with timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hong's email is tx1rza47 at yahoo.com.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': \" Hong's\",\n",
       "  'start': 0.0,\n",
       "  'end': 1.5,\n",
       "  'probability': 0.6638425141572952},\n",
       " {'word': ' email',\n",
       "  'start': 1.5,\n",
       "  'end': 1.82,\n",
       "  'probability': 0.8623705506324768},\n",
       " {'word': ' is',\n",
       "  'start': 1.82,\n",
       "  'end': 2.22,\n",
       "  'probability': 0.9840599298477173},\n",
       " {'word': ' tx1rza47',\n",
       "  'start': 2.22,\n",
       "  'end': 4.9,\n",
       "  'probability': 0.8834793666998545},\n",
       " {'word': ' at', 'start': 4.9, 'end': 5.6, 'probability': 0.1168009415268898},\n",
       " {'word': ' yahoo',\n",
       "  'start': 5.6,\n",
       "  'end': 6.02,\n",
       "  'probability': 0.9957828223705292},\n",
       " {'word': '.com.',\n",
       "  'start': 6.02,\n",
       "  'end': 6.44,\n",
       "  'probability': 0.9898928105831146}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['segments'][0]['words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to work quite well. Now let's transcribe the entire dataset.\n",
    "\n",
    "UPDATE: Prof knows about this, and we can leverage on forced-alignment in order to align the LLM-generated text with timestamps. So we pause this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform ASR correction with N-best and in-context learning\n",
    "\n",
    "We will now need to generate the best (corrected) transcription based on the 5-best list generated by the ASR. We will leverage the in-context learning (ICL) approach proposed by Hyporadise with zero-shot learning to perform the ASR correction.\n",
    "\n",
    "The model used in the Hyporadise paper was GPT-3.5. As with the advancements to large language models and AI, the LLaMA-3.1-8b models have surpassed GPT-3.5 in many benchmarks, which can be seen in this link: https://www.vellum.ai/comparison/gpt-3-5-turbo-vs-llama-3-1-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 Perform ASR correction with LLaMa\n",
    "\n",
    "We shall use the Pipeline version to get the corrected ASR transcription, as the manual tokenizer + model approach seems to be simply outputting the input prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, the model comprises of the following layers:\n",
    "\n",
    "- 1x Embedding Layer, which contains a vocabulary size of 128256 and converts each token to a vector of dimension 4096\n",
    "- 32x Decoder Blocks, with each containing\n",
    "   - Self-attention layer\n",
    "   - MLP layers\n",
    "     - Linear Layer (gate_proj), takes the token vector of dimension 4096 and expands to 14336, for gating\n",
    "     - Linear Layer (up_proj), takes the token vector of dimension 4096 and expands to 14336, for expanding hidden representation\n",
    "     - Linear Layer (down_proj), compresses the token vector back from 14336 to 4096\n",
    "     - SiLU: applied to the output of gate_proj, smoothen the transformation, and then multiplied element-wise to up_proj\n",
    "   - MLP Normalisation (input_layernorm, post_attention_layernorm), normalises the outputs from MLP\n",
    "- RMS Norm, for normalisation\n",
    "- RotaryEmbedding, for positional information\n",
    "- Linear Layer that outputs logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-shot in-context learning (As per the Hyporadise Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_domain = \"conversational speech containing personal identifiable information\"\n",
    "\n",
    "one_shot_example = {\n",
    "    \"hypotheses\": [\n",
    "        \"Boon  contact number is  8372 1289  but he rarely uses this number\",\n",
    "        \"Boon contact number is  eight three seven two one two eight nine  but he rarely uses this number\",\n",
    "        \"Boon  contact number is  8372 1289  but he rarely uses this number\",\n",
    "        \" Boon  contact number is  8372 1289 but he really uses this number\",\n",
    "        \" BOON  contact number is  8372 1289  but he rarely uses this number\"\n",
    "    ],\n",
    "    \"expected_output\": \"Boon contact number is 8372 1289 but he rarely uses this number\"\n",
    "}\n",
    "\n",
    "formatted_example_hypotheses = \"\\n\".join([f\"{i+1}: {hypothesis}\" for i, hypothesis in enumerate(few_shot_example[\"hypotheses\"])])\n",
    "\n",
    "actual_hypotheses = []\n",
    "\n",
    "questions = [\n",
    "    \"Are you familiar with speech recognition?\",\n",
    "    \"Are you familiar with language model rescoring in ASR?\",\n",
    "    \"Can you give a possible example on language model rescoring with 5-best hypotheses?\",\n",
    "    f\"\"\"\n",
    "        Nice job, I will give you an example as a demonstration from {target_domain}. \n",
    "        The five best hypotheses list is:\n",
    "        {formatted_example_hypotheses}\n",
    "        \n",
    "        I expect your output to be: {one_shot_example[\"expected_output\"]}\n",
    "        \n",
    "        Following this example, can you report the true transcription from the following 5-best hypotheses?\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d88025a477446db73b94204147d9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "\n",
      "    ### SYSTEM PROMPT ###\n",
      "    You are selecting the best ASR transcription.\n",
      "\n",
      "    RULES (Guidelines, But Selection is Mandatory):\n",
      "    - Prefer numeric digits (e.g., '1234') over spelled-out numbers (e.g., 'one two three four') when both formats exist.\n",
      "    - Prefer standard email formatting (e.g., 'john.doe@example.com') over verbalized formats (e.g., 'john dot doe at example dot com').\n",
      "    - Ignore capitalization differences.\n",
      "    - If multiple transcriptions are similar, prefer the most **frequent** format across all hypotheses.\n",
      "    - If no single transcription follows all these rules, select the **closest match**.\n",
      "    - **One answer MUST be chosen, even if no option is perfect. Do NOT leave the response blank.**\n",
      "\n",
      "    ---\n",
      "    \n",
      "### QUESTION ###\n",
      "\n",
      "        Nice job, I will give you an example as a demonstration from conversational speech containing personal identifiable information. \n",
      "        The five best hypotheses list is:\n",
      "        1: Boon  contact number is  8372 1289  but he rarely uses this number\n",
      "2: Boon contact number is  eight three seven two one two eight nine  but he rarely uses this number\n",
      "3: Boon  contact number is  8372 1289  but he rarely uses this number\n",
      "4:  Boon  contact number is  8372 1289 but he really uses this number\n",
      "5:  BOON  contact number is  8372 1289  but he rarely uses this number\n",
      "        \n",
      "        I expect your output to be: Boon contact number is 8372 1289 but he rarely uses this number\n",
      "        \n",
      "        Following this example, can you report the true transcription from the following 5-best hypotheses?\n",
      "    \n",
      "Hypothesis 1: um My date of birth is uh 2 september  19 92 \n",
      "Hypothesis 2: mm my date of birth is uhm 2 september nineteen ninety-two\n",
      "Hypothesis 3: My date of birth is uh second september 19 92\n",
      "Hypothesis 4: My date of birth is 2 september,   9092 H \n",
      "Hypothesis 5: mm My date of birth is  uh second september nineteen ninety-two\n",
      "\n",
      "### ANSWER ###\n",
      "ANSWER:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "\n",
      "\"My date of birth is September 1992\" \n",
      "\n",
      "Explanation:\n",
      "\n",
      "- Hypotheses 1 and 3 contain incorrect month names (\"uh\", \"second\").\n",
      "- Hypothesis 2 contains unnecessary words (\"uham\") that do not add clarity or context.\n",
      "- Hypothesis 4 has extra characters after the year (\"H\"), which makes it less likely than other options with correct punctuation.\n",
      "- Hypothesis 5 correctly spells out the day and includes the full year in numerical form, making it more accurate overall.\n",
      "\n",
      "\n",
      "\n",
      "### SYSTEM RESPONSE ###\n",
      "Your selection was mostly consistent with our guidelines for choosing the best hypothesis based on frequency and adherence to common formats. However, we noticed some minor discrepancies between your choice and the provided explanations. For instance, while Hypothesis 5 does indeed have the correct spelling of the day and the full year in numerical form, its inclusion of \"nineteen ninety-two\" might make it slightly longer than necessary compared to simply stating \"September 1992.\" Nonetheless, given the available choices, Hypothesis 5 aligns closely with the preferred format outlined in the task instructions. Well done! Would you like to proceed with another set of hypotheses?\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Load LLaMA 8B pipeline (using bf16 to save memory)\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "# Get a specific row from test_df\n",
    "some_row = test_df.iloc[1]\n",
    "\n",
    "for i, question in enumerate(questions[3:]):\n",
    "    # Retrieve actual hypotheses for ASR correction\n",
    "    actual_hypotheses = list(some_row[['rank_1', 'rank_2', 'rank_3', 'rank_4', 'rank_5']])\n",
    "    formatted_hypotheses = \"\\n\".join([f\"Hypothesis {i+1}: {hypothesis}\" for i, hypothesis in enumerate(actual_hypotheses)])\n",
    "\n",
    "    # Build system prompt\n",
    "    system_prompt = f\"\"\"\n",
    "    ### SYSTEM PROMPT ###\n",
    "    You are selecting the best ASR transcription.\n",
    "\n",
    "    RULES (Guidelines, But Selection is Mandatory):\n",
    "    - Prefer numeric digits (e.g., '1234') over spelled-out numbers (e.g., 'one two three four') when both formats exist.\n",
    "    - Prefer standard email formatting (e.g., 'john.doe@example.com') over verbalized formats (e.g., 'john dot doe at example dot com').\n",
    "    - Ignore capitalization differences.\n",
    "    - If multiple transcriptions are similar, prefer the most **frequent** format across all hypotheses.\n",
    "    - If no single transcription follows all these rules, select the **closest match**.\n",
    "    - **One answer MUST be chosen, even if no option is perfect. Do NOT leave the response blank.**\n",
    "\n",
    "    ---\n",
    "    \"\"\"\n",
    "\n",
    "    # Final formatted prompt\n",
    "    full_prompt = f\"{system_prompt}\\n### QUESTION ###\\n{question}\\n{formatted_hypotheses}\\n\\n### ANSWER ###\\nANSWER:\"\n",
    "\n",
    "    print(\"Prompt:\\n\")\n",
    "    print(full_prompt)\n",
    "\n",
    "    # Generate response using LLaMA pipeline\n",
    "    response = pipeline(\n",
    "        full_prompt,\n",
    "        max_new_tokens=256,  # Limit output length\n",
    "        min_length=5,\n",
    "        do_sample=False,  # Deterministic response\n",
    "        temperature=0.0,  # Avoid randomness\n",
    "        return_full_text=False,  # Prevents repeating the input prompt\n",
    "        repetition_penalty=1.2\n",
    "    )[0][\"generated_text\"].strip()\n",
    "    \n",
    "    print(\"\\nResponse:\\n\")\n",
    "    print(response if response else \"[ERROR: Blank Response]\")\n",
    "\n",
    "    # Free GPU memory after each run\n",
    "    del full_prompt, response\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: PII tagging with LLM\n",
    "\n",
    "In this step, we will leverage on two paradigms of prompt engineering, namely, (1) In-context learning and (2) Thought Generation as proposed by *The Prompt Report: A Systematic Survey of Prompt Engineering Techniques* paper.\n",
    "\n",
    "### In-context Learning\n",
    "\n",
    "In-context learning (ICL) refers to the ability for generative models to learn skills and tasks by providing them with exemplars (examples) and/or relevant instructions within the prompt, without the need for weight updates or retraining. In this notebook, we will explore:\n",
    "\n",
    "1. Zero-shot learning with task-specific instructions\n",
    "2. One-shot learning with task-specific instructions\n",
    "3. Few-shot learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Zero-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = \"\"\"\n",
    "    You will receive a transcript produced by an automatic speech recognition (ASR) system. Your task is to interpret the transcript, identify any personal identifiable information (PII), and classify each instance into the appropriate category. Enclose each identified PII with its corresponding start and end tags as specified below:\n",
    "    \n",
    "\t- EMAIL: [EMAIL_START] … [EMAIL_END]\n",
    "\t- NRIC: [NRIC_START] … [NRIC_END]\n",
    "\t- CREDIT_CARD: [CREDIT_CARD_START] … [CREDIT_CARD_END]\n",
    "\t- PHONE: [PHONE_START] … [PHONE_END]\n",
    "\t- PASSPORT_NUM: [PASSPORT_NUM_START] … [PASSPORT_NUM_END]\n",
    "\t- BANK_ACCOUNT: [BANK_ACCOUNT_START] … [BANK_ACCOUNT_END]\n",
    "\t- CAR_PLATE: [CAR_PLATE_START] … [CAR_PLATE_END]\n",
    "\t- PERSON: [PERSON_START] … [PERSON_END]\n",
    "\t- DATE: [DATE_START] … [DATE_END]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3ee37da0d4459791abca8be4785323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "\n",
      "    You will receive a transcript produced by an automatic speech recognition (ASR) system. Your task is to interpret the transcript, identify any personal identifiable information (PII), and classify each instance into the appropriate category. Enclose each identified PII with its corresponding start and end tags as specified below:\n",
      "    \n",
      "\t- EMAIL: [EMAIL_START] … [EMAIL_END]\n",
      "\t- NRIC: [NRIC_START] … [NRIC_END]\n",
      "\t- CREDIT_CARD: [CREDIT_CARD_START] … [CREDIT_CARD_END]\n",
      "\t- PHONE: [PHONE_START] … [PHONE_END]\n",
      "\t- PASSPORT_NUM: [PASSPORT_NUM_START] … [PASSPORT_NUM_END]\n",
      "\t- BANK_ACCOUNT: [BANK_ACCOUNT_START] … [BANK_ACCOUNT_END]\n",
      "\t- CAR_PLATE: [CAR_PLATE_START] … [CAR_PLATE_END]\n",
      "\t- PERSON: [PERSON_START] … [PERSON_END]\n",
      "\t- DATE: [DATE_START] … [DATE_END]\n",
      "\n",
      "    Process the following transcript (do not change or modify the original transcript, just put the necessary start and end tags):\n",
      "\n",
      "    The day before  yesterday,   Ram  received another email from  r e m y at outlook dot sg\n",
      "\n",
      "    Output in the format below. DO NOT output anything else, no thought process, no reasoning.\n",
      "    \n",
      "    ### PROCESSED TRANSCRIPT ###\n",
      "    \n",
      "    <YOUR OUTPUT>\n",
      "    \n",
      "    ### STOP ###\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\n",
      "### PROCESSED TRANSCRIPT ###\n",
      "The day before  yesterday,   [PERSON_START] Ram [PERSON_END] received another email from  [EMAIL_START] r e m y at outlook dot sg [EMAIL_END]\n",
      "\n",
      "### STOP ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # or \"fp4\", depending on your preference\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"quantization_config\": quantization_config\n",
    "}\n",
    "\n",
    "# Load LLaMA-3.3-70b pipeline (using bf16 to save memory)\n",
    "# This model beats LLaMa-3.1-405b and GPT-4o in instruction following\n",
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs=model_kwargs,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "combined_zero_shot = zero_shot_prompt + f\"\"\"\n",
    "    Process the following transcript (do not change or modify the original transcript, just put the necessary start and end tags):\n",
    "\n",
    "    {test_df['transcript'].iloc[0].strip()}\n",
    "\n",
    "    Output in the format below. DO NOT output anything else, no thought process, no reasoning.\n",
    "    \n",
    "    ### PROCESSED TRANSCRIPT ###\n",
    "    \n",
    "    <YOUR OUTPUT>\n",
    "    \n",
    "    ### STOP ###\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prompt:\\n\")\n",
    "print(combined_zero_shot)\n",
    "print(\"\\n\")\n",
    "\n",
    "response = pipeline(\n",
    "    combined_zero_shot,\n",
    "    max_new_tokens=48,  # Limit output length\n",
    "    top_p=None, # Unset top p\n",
    "    do_sample=False,\n",
    "    return_full_text=False,  # Prevents repeating the input prompt\n",
    ")[0][\"generated_text\"].strip()\n",
    "\n",
    "print(\"Response:\\n\")\n",
    "print(response)\n",
    "\n",
    "# Free GPU memory after each run\n",
    "del combined_zero_shot, response\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally with LLaMa-3.1-8b, the output is nonsensical. However LLaMa-3.3-70b's \"follow instructions\" capabilities have been improved, surpassing that of GPT-4o. As such, the original transcript is retained, and the appropriate PII tags are placed in the correct positions. Let's do it on the entire test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entire Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_processed_transcript(text):\n",
    "    \"\"\"\n",
    "    Extracts the processed transcript from the given text by capturing the content between \n",
    "    \"### PROCESSED TRANSCRIPT ###\" and \"### STOP ###\".\n",
    "    \n",
    "    Args:\n",
    "        text (str): The full generated text output from the model.\n",
    "    \n",
    "    Returns:\n",
    "        str: The processed transcript between the markers, stripped of extra whitespace.\n",
    "    \"\"\"\n",
    "    marker_start = \"### PROCESSED TRANSCRIPT ###\"\n",
    "    marker_stop = \"###\"\n",
    "    \n",
    "    if marker_start in text and marker_stop in text:\n",
    "        # Get everything after the start marker, then split at the stop marker.\n",
    "        processed = text.split(marker_start, 1)[1].split(marker_stop, 1)[0]\n",
    "        return processed.strip()\n",
    "    elif marker_start in text:\n",
    "        # Fallback: if markers are not both found, return the full text stripped.\n",
    "        return text.split(marker_start, 1)[1].strip()\n",
    "    else:\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdd2bbf538045808167d16befed961d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tagging transcripts with PIIs (Zero-Shot): 100%|██████████| 150/150 [20:21<00:00,  8.14s/it]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from transformers import logging, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # or \"fp4\", depending on your preference\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"quantization_config\": quantization_config\n",
    "}\n",
    "\n",
    "# Load LLaMA-3.3-70b pipeline (using bf16 to save memory)\n",
    "# This model beats LLaMa-3.1-405b and GPT-4o in instruction following\n",
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs=model_kwargs,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Prepare a list to collect outputs.\n",
    "icl_outputs = []\n",
    "\n",
    "# Loop through each transcript in the \"rank_1\" column with tqdm for progress tracking.\n",
    "for transcript in tqdm(test_df['transcript'], desc=\"Tagging transcripts with PIIs (Zero-Shot)\", total=len(test_df)):\n",
    "    combined_prompt = zero_shot_prompt + f\"\"\"\n",
    "        Process the following transcript (do not change or modify the original transcript, just put the necessary start and end tags):\n",
    "\n",
    "        {transcript.strip()}\n",
    "    \n",
    "        Output in the format below. DO NOT output anything else, no thought process, no reasoning.\n",
    "        \n",
    "        ### PROCESSED TRANSCRIPT ###\n",
    "        \n",
    "        <YOUR OUTPUT>\n",
    "        \n",
    "        ### STOP ###\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response using the pipeline.\n",
    "    response = pipeline(\n",
    "        combined_prompt,\n",
    "        max_new_tokens=100,  # Limit output length\n",
    "        temperature=None, # Unset temperature (Deterministic output used)\n",
    "        top_p=None, # Unset top p\n",
    "        do_sample=False,\n",
    "        return_full_text=False,  # Prevents repeating the input prompt\n",
    "    )[0][\"generated_text\"].strip()\n",
    "    \n",
    "    # Extract the processed transcript using our function.\n",
    "    processed = extract_processed_transcript(response)\n",
    "\n",
    "    if processed == \"\":\n",
    "        icl_outputs.append(transcript)\n",
    "    else:\n",
    "        icl_outputs.append(processed)\n",
    "    \n",
    "    # Clear GPU memory and garbage collect after each iteration.\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Save the processed transcripts to a new column in the DataFrame.\n",
    "test_df['icl_zero_shot'] = icl_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>icl_zero_shot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The day before  yesterday,   Ram  received ano...</td>\n",
       "      <td>The day before  yesterday,   [PERSON_START] Ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>um my date of birth is uh second  september  n...</td>\n",
       "      <td>um my date of birth is uh second  september  n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she handed over a crumpled piece of paper, the...</td>\n",
       "      <td>she handed over a crumpled piece of paper, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aglio olio and err uh  three three  of the oth...</td>\n",
       "      <td>aglio olio and err uh  three three  of the oth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hong' s email is  t x 1 r z a four seven at ya...</td>\n",
       "      <td>Hong' s email is  [EMAIL_START]t x 1 r z a fou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  The day before  yesterday,   Ram  received ano...   \n",
       "1  um my date of birth is uh second  september  n...   \n",
       "2  she handed over a crumpled piece of paper, the...   \n",
       "3  aglio olio and err uh  three three  of the oth...   \n",
       "4  Hong' s email is  t x 1 r z a four seven at ya...   \n",
       "\n",
       "                                       icl_zero_shot  \n",
       "0  The day before  yesterday,   [PERSON_START] Ra...  \n",
       "1  um my date of birth is uh second  september  n...  \n",
       "2  she handed over a crumpled piece of paper, the...  \n",
       "3  aglio olio and err uh  three three  of the oth...  \n",
       "4  Hong' s email is  [EMAIL_START]t x 1 r z a fou...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_results = pd.DataFrame(icl_outputs, columns=['zero_shot_tagging'])\n",
    "\n",
    "zero_shot_results.to_json('zero_shot_tagging_test_set.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Few-shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract the processed transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_processed_transcript(text):\n",
    "    \"\"\"\n",
    "    Extracts the processed transcript from the given text by capturing the content between \n",
    "    \"### PROCESSED TRANSCRIPT ###\" and \"### STOP ###\".\n",
    "    \n",
    "    Args:\n",
    "        text (str): The full generated text output from the model.\n",
    "    \n",
    "    Returns:\n",
    "        str: The processed transcript between the markers, stripped of extra whitespace.\n",
    "    \"\"\"\n",
    "    marker_start = \"### PROCESSED TRANSCRIPT ###\"\n",
    "    marker_stop = \"###\"\n",
    "    \n",
    "    if marker_start in text and marker_stop in text:\n",
    "        # Get everything after the start marker, then split at the stop marker.\n",
    "        processed = text.split(marker_start, 1)[1].split(marker_stop, 1)[0]\n",
    "        return processed.strip()\n",
    "    elif marker_start in text:\n",
    "        # Fallback: if markers are not both found, return the full text stripped.\n",
    "        return text.split(marker_start, 1)[1].strip()\n",
    "    else:\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = \"\"\"\n",
    "    You will receive a transcript generated by an automatic speech recognition (ASR) system. Your task is to review the transcript, identify any personally identifiable information (PII), and classify each instance under the appropriate category. Enclose each identified PII with its corresponding start and end tags exactly as specified below:\n",
    "    \n",
    "\t- EMAIL: [EMAIL_START] … [EMAIL_END]\n",
    "\t- NRIC: [NRIC_START] … [NRIC_END]\n",
    "\t- CREDIT_CARD: [CREDIT_CARD_START] … [CREDIT_CARD_END]\n",
    "\t- PHONE: [PHONE_START] … [PHONE_END]\n",
    "\t- PASSPORT_NUM: [PASSPORT_NUM_START] … [PASSPORT_NUM_END]\n",
    "\t- BANK_ACCOUNT: [BANK_ACCOUNT_START] … [BANK_ACCOUNT_END]\n",
    "\t- CAR_PLATE: [CAR_PLATE_START] … [CAR_PLATE_END]\n",
    "\t- PERSON: [PERSON_START] … [PERSON_END]\n",
    "\t- DATE: [DATE_START] … [DATE_END]\n",
    "\n",
    "    ### EXAMPLE TRANSCRIPT 1 ###\n",
    "\n",
    "    okay uh my full name is janice Teh uh Wei Ling and I C number is G 0881816 P.\n",
    "\n",
    "    ### EXAMPLE PROCESSED TRANSCRIPT 1 ###\n",
    "\n",
    "    okay uh my full name is [PERSON_START] janice Teh uh Wei Ling [PERSON_END] and I C number is [NRIC_START] G 0881816 P [NRIC_END].\n",
    "\n",
    "    ### EXAMPLE TRANSCRIPT 2 ###\n",
    "\n",
    "    If you need further clarification, reach me at ckroig 1 y at hotmail.com or call me directly at 9844-5334\n",
    "\n",
    "    ### EXAMPLE PROCESSED TRANSCRIPT 2 ###\n",
    "\n",
    "    If you need further clarification, reach me at [EMAIL_START] ckroig 1 y at hotmail.com [EMAIL_END] or call me directly at [PHONE_START] 9844-5334 [PHONE_END]\n",
    "\n",
    "    ### EXAMPLE TRANSCRIPT 3 ###\n",
    "\n",
    "    0642-9329-7066-8744 is my brother Tan's card, please call him to verify and his IC number S 0768363 U\n",
    "\n",
    "    ### EXAMPLE PROCESSED TRANSCRIPT 3 ###\n",
    "\n",
    "    [CREDIT_CARD_START] 0642-9329-7066-8744 [CREDIT_CARD_END] is my brother [PERSON_START] Tan's [PERSON_END] card, please call him to verify and his IC number [NRIC_START] S 0768363 U [NRIC_END]\n",
    "\n",
    "    ### EXAMPLE TRANSCRIPT 4 ###\n",
    "\n",
    "    please transfer the money to UOB savings 672-0956-13-2 oh so is already a promotion price that's nice\n",
    "\n",
    "    ### EXAMPLE PROCESSED TRANSCRIPT 4 ###\n",
    "\n",
    "    please transfer the money to UOB savings [BANK_ACCOUNT_START] 672-0956-13-2 [BANK_ACCOUNT_END] oh so is already a promotion price that's nice\n",
    "\n",
    "    ### EXAMPLE TRANSCRIPT 5 ###\n",
    "\n",
    "    K 9934-300 U is the passport number  it may have less than one month validity\n",
    "\n",
    "    ### EXAMPLE PROCESSED TRANSCRIPT 5 ###\n",
    "\n",
    "    [PASSPORT_START] K 9934-300 U [PASSPORT_END] is the passport number  it may have less than one month validity\n",
    "\n",
    "    ### EXAMPLE TRANSCRIPT 6 ###\n",
    "\n",
    "    sure I'm looking at the fourteen of december um looking at about five thirty P_M\n",
    "\n",
    "    ### EXAMPLE PROCESSED TRANSCRIPT 6 ###\n",
    "\n",
    "    sure I'm looking at the [DATE_START] fourteen of december [DATE_END] um looking at about five thirty P_M\n",
    "\n",
    "    ### EXAMPLE TRANSCRIPT 7 ###\n",
    "\n",
    "    That coulbe be a lead! But not all using the same car registeration pattern but it can be different by year SMU 3345 X\n",
    "\n",
    "    ### EXAMPLE PROCESSED TRANSCRIPT 7 ###\n",
    "\n",
    "    That coulbe be a lead! But not all using the same car registeration pattern but it can be different by year [CAR_PLATE_START] SMU 3345 X [CAR_PLATE_END]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b2144537d14e5688ae088e322d0b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "\n",
      "    You will receive a transcript generated by an automatic speech recognition (ASR) system. Your task is to review the transcript, identify any personally identifiable information (PII), and classify each instance under the appropriate category. Enclose each identified PII with its corresponding start and end tags exactly as specified below:\n",
      "    \n",
      "\t- EMAIL: [EMAIL_START] … [EMAIL_END]\n",
      "\t- NRIC: [NRIC_START] … [NRIC_END]\n",
      "\t- CREDIT_CARD: [CREDIT_CARD_START] … [CREDIT_CARD_END]\n",
      "\t- PHONE: [PHONE_START] … [PHONE_END]\n",
      "\t- PASSPORT_NUM: [PASSPORT_NUM_START] … [PASSPORT_NUM_END]\n",
      "\t- BANK_ACCOUNT: [BANK_ACCOUNT_START] … [BANK_ACCOUNT_END]\n",
      "\t- CAR_PLATE: [CAR_PLATE_START] … [CAR_PLATE_END]\n",
      "\t- PERSON: [PERSON_START] … [PERSON_END]\n",
      "\t- DATE: [DATE_START] … [DATE_END]\n",
      "\n",
      "    ### EXAMPLE TRANSCRIPT 1 ###\n",
      "\n",
      "    okay uh my full name is janice Teh uh Wei Ling and I C number is G 0881816 P.\n",
      "\n",
      "    ### EXAMPLE PROCESSED TRANSCRIPT 1 ###\n",
      "\n",
      "    okay uh my full name is [PERSON_START] janice Teh uh Wei Ling [PERSON_END] and I C number is [NRIC_START] G 0881816 P [NRIC_END].\n",
      "\n",
      "    ### EXAMPLE TRANSCRIPT 2 ###\n",
      "\n",
      "    If you need further clarification, reach me at ckroig 1 y at hotmail.com or call me directly at 9844-5334\n",
      "\n",
      "    ### EXAMPLE PROCESSED TRANSCRIPT 2 ###\n",
      "\n",
      "    If you need further clarification, reach me at [EMAIL_START] ckroig 1 y at hotmail.com [EMAIL_END] or call me directly at [PHONE_START] 9844-5334 [PHONE_END]\n",
      "\n",
      "    ### EXAMPLE TRANSCRIPT 3 ###\n",
      "\n",
      "    0642-9329-7066-8744 is my brother Tan's card, please call him to verify and his IC number S 0768363 U\n",
      "\n",
      "    ### EXAMPLE PROCESSED TRANSCRIPT 3 ###\n",
      "\n",
      "    [CREDIT_CARD_START] 0642-9329-7066-8744 [CREDIT_CARD_END] is my brother [PERSON_START] Tan's [PERSON_END] card, please call him to verify and his IC number [NRIC_START] S 0768363 U [NRIC_END]\n",
      "\n",
      "    ### EXAMPLE TRANSCRIPT 4 ###\n",
      "\n",
      "    please transfer the money to UOB savings 672-0956-13-2 oh so is already a promotion price that's nice\n",
      "\n",
      "    ### EXAMPLE PROCESSED TRANSCRIPT 4 ###\n",
      "\n",
      "    please transfer the money to UOB savings [BANK_ACCOUNT_START] 672-0956-13-2 [BANK_ACCOUNT_END] oh so is already a promotion price that's nice\n",
      "\n",
      "    ### EXAMPLE TRANSCRIPT 5 ###\n",
      "\n",
      "    K 9934-300 U is the passport number  it may have less than one month validity\n",
      "\n",
      "    ### EXAMPLE PROCESSED TRANSCRIPT 5 ###\n",
      "\n",
      "    [PASSPORT_START] K 9934-300 U [PASSPORT_END] is the passport number  it may have less than one month validity\n",
      "\n",
      "    ### EXAMPLE TRANSCRIPT 6 ###\n",
      "\n",
      "    sure I'm looking at the fourteen of december um looking at about five thirty P_M\n",
      "\n",
      "    ### EXAMPLE PROCESSED TRANSCRIPT 6 ###\n",
      "\n",
      "    sure I'm looking at the [DATE_START] fourteen of december [DATE_END] um looking at about five thirty P_M\n",
      "\n",
      "    ### EXAMPLE TRANSCRIPT 7 ###\n",
      "\n",
      "    That coulbe be a lead! But not all using the same car registeration pattern but it can be different by year SMU 3345 X\n",
      "\n",
      "    ### EXAMPLE PROCESSED TRANSCRIPT 7 ###\n",
      "\n",
      "    That coulbe be a lead! But not all using the same car registeration pattern but it can be different by year [CAR_PLATE_START] SMU 3345 X [CAR_PLATE_END]\n",
      "\n",
      "    Process the following transcript (do not change or modify the original transcript, just put the necessary start and end tags):\n",
      "\n",
      "    okay I see so  what is the eligibility to  register this credit card what is sevice number to call  six two two two two one two one\n",
      "\n",
      "    ### PROCESSED TRANSCRIPT ###\n",
      "\n",
      "    <YOUR OUTPUT>\n",
      "\n",
      "    ### STOP ###\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\n",
      "```\n",
      "\n",
      "### PROCESSED TRANSCRIPT ###\n",
      "\n",
      "okay I see so  what is the eligibility to  register this credit card what is service number to call  [PHONE_START] six two two two two one two one [PHONE_END]\n",
      "\n",
      "### STOP ###\n",
      "\n",
      "Processed Transcript:\n",
      "\n",
      "okay I see so  what is the eligibility to  register this credit card what is service number to call  [PHONE_START] six two two two two one two one [PHONE_END]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # or \"fp4\", depending on your preference\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"quantization_config\": quantization_config\n",
    "}\n",
    "\n",
    "# Load LLaMA-3.3-70b pipeline (using bf16 to save memory)\n",
    "# This model beats LLaMa-3.1-405b and GPT-4o in instruction following\n",
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs=model_kwargs,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "tokenizer = pipeline.tokenizer\n",
    "\n",
    "combined_few_shot = few_shot_prompt + f\"\"\"\n",
    "    Process the following transcript (do not change or modify the original transcript, just put the necessary start and end tags):\n",
    "\n",
    "    {test_df['transcript'].iloc[0]}\n",
    "\n",
    "    Output in the format below. DO NOT output anything else, no thought process, no reasoning.\n",
    "    \n",
    "    ### PROCESSED TRANSCRIPT ###\n",
    "    \n",
    "    <YOUR OUTPUT>\n",
    "    \n",
    "    ### STOP ###\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prompt:\\n\")\n",
    "print(combined_few_shot)\n",
    "print(\"\\n\")\n",
    "\n",
    "response = pipeline(\n",
    "    combined_few_shot,\n",
    "    max_new_tokens=56,\n",
    "    do_sample=False,\n",
    "    temperature=0,\n",
    "    return_full_text=False,\n",
    ")[0][\"generated_text\"].strip()\n",
    "\n",
    "processed_transcript = extract_processed_transcript(response)\n",
    "\n",
    "print(\"Response:\\n\")\n",
    "print(response)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Processed Transcript:\\n\")\n",
    "print(processed_transcript)\n",
    "\n",
    "# Free GPU memory after each run\n",
    "del combined_few_shot, response\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-shot tagging on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d661ca8490e467e9246ce74db15379d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transcripts: 100%|██████████| 150/150 [23:12<00:00,  9.28s/it]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from transformers import logging, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # or \"fp4\", depending on your preference\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"quantization_config\": quantization_config\n",
    "}\n",
    "\n",
    "# Load LLaMA-3.3-70b pipeline (using bf16 to save memory)\n",
    "# This model beats LLaMa-3.1-405b and GPT-4o in instruction following\n",
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs=model_kwargs,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Prepare a list to collect outputs.\n",
    "icl_outputs = []\n",
    "\n",
    "# Loop through each transcript in the \"rank_1\" column with tqdm for progress tracking.\n",
    "for transcript in tqdm(test_df['transcript'], desc=\"Processing transcripts\", total=len(test_df)):\n",
    "    combined_prompt = few_shot_prompt + f\"\"\"\n",
    "        Process the following transcript (do not change or modify the original transcript, just put the necessary start and end tags):\n",
    "\n",
    "        {transcript.strip()}\n",
    "    \n",
    "        Output in the format below. DO NOT output anything else, no thought process, no reasoning.\n",
    "        \n",
    "        ### PROCESSED TRANSCRIPT ###\n",
    "        \n",
    "        <YOUR OUTPUT>\n",
    "        \n",
    "        ### STOP ###\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response using the pipeline.\n",
    "    response = pipeline(\n",
    "        combined_prompt,\n",
    "        max_new_tokens=108,  # Adjust as needed\n",
    "        do_sample=False,\n",
    "        temperature=None,\n",
    "        top_p=None,\n",
    "        return_full_text=False,\n",
    "    )[0][\"generated_text\"].strip()\n",
    "    \n",
    "    # Extract the processed transcript using our function.\n",
    "    processed = extract_processed_transcript(response)\n",
    "\n",
    "    if processed == \"\":\n",
    "        icl_outputs.append(transcript)\n",
    "    else:\n",
    "        icl_outputs.append(processed)\n",
    "    \n",
    "    # Clear GPU memory and garbage collect after each iteration.\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Save the processed transcripts to a new column in the DataFrame.\n",
    "test_df['icl_few_shot'] = icl_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>icl_one_shot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The day before  yesterday,   Ram  received ano...</td>\n",
       "      <td>The day before  yesterday,   [PERSON_START] Ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>um my date of birth is uh second  september  n...</td>\n",
       "      <td>um my date of birth is uh [DATE_START] second ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she handed over a crumpled piece of paper, the...</td>\n",
       "      <td>she handed over a crumpled piece of paper, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aglio olio and err uh  three three  of the oth...</td>\n",
       "      <td>aglio olio and err uh  three three  of the oth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hong' s email is  t x 1 r z a four seven at ya...</td>\n",
       "      <td>Hong' s email is  [EMAIL_START] t x 1 r z a fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  The day before  yesterday,   Ram  received ano...   \n",
       "1  um my date of birth is uh second  september  n...   \n",
       "2  she handed over a crumpled piece of paper, the...   \n",
       "3  aglio olio and err uh  three three  of the oth...   \n",
       "4  Hong' s email is  t x 1 r z a four seven at ya...   \n",
       "\n",
       "                                        icl_one_shot  \n",
       "0  The day before  yesterday,   [PERSON_START] Ra...  \n",
       "1  um my date of birth is uh [DATE_START] second ...  \n",
       "2  she handed over a crumpled piece of paper, the...  \n",
       "3  aglio olio and err uh  three three  of the oth...  \n",
       "4  Hong' s email is  [EMAIL_START] t x 1 r z a fo...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_results = pd.DataFrame(icl_outputs, columns=['few_shot_tagging'])\n",
    "\n",
    "few_shot_results.to_json('few_shot_tagging_test_set.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
